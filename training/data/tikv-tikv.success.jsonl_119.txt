fn get_pool_size(&self, priority_level: CommandPri) -> usize {
        if priority_level == CommandPri::High {
            self.high_worker_pool.get_pool_size()
        } else {
            self.worker_pool.get_pool_size()
        }
    }
fn test_scale_scheduler_pool() {
    let snapshot_fp = "scheduler_start_execute";
    let mut cluster = new_server_cluster(0, 1);
    cluster.run();
    let origin_pool_size = cluster.cfg.storage.scheduler_worker_pool_size;

    let engine = cluster
        .sim
        .read()
        .unwrap()
        .storages
        .get(&1)
        .unwrap()
        .clone();
    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())
        .config(cluster.cfg.tikv.storage.clone())
        .build()
        .unwrap();

    let cfg = new_tikv_config(1);
    let kv_engine = storage.get_engine().kv_engine().unwrap();
    let (_tx, rx) = std::sync::mpsc::channel();
    let flow_controller = Arc::new(FlowController::Singleton(EngineFlowController::new(
        &cfg.storage.flow_control,
        kv_engine.clone(),
        rx,
    )));

    let cfg_controller = ConfigController::new(cfg);
    let (scheduler, _receiver) = dummy_scheduler();
    cfg_controller.register(
        Module::Storage,
        Box::new(StorageConfigManger::new(
            kv_engine,
            scheduler,
            flow_controller,
            storage.get_scheduler(),
        )),
    );
    let scheduler = storage.get_scheduler();

    let region = cluster.get_region(b"k1");
    let mut ctx = Context::default();
    ctx.set_region_id(region.id);
    ctx.set_region_epoch(region.get_region_epoch().clone());
    ctx.set_peer(cluster.leader_of_region(region.id).unwrap());
    let do_prewrite = |key: &[u8], val: &[u8]| {
        // prewrite
        let (prewrite_tx, prewrite_rx) = channel();
        storage
            .sched_txn_command(
                commands::Prewrite::new(
                    vec![Mutation::make_put(Key::from_raw(key), val.to_vec())],
                    key.to_vec(),
                    10.into(),
                    100,
                    false,
                    2,
                    TimeStamp::default(),
                    TimeStamp::default(),
                    None,
                    false,
                    AssertionLevel::Off,
                    ctx.clone(),
                ),
                Box::new(move |res: storage::Result<_>| {
                    let _ = prewrite_tx.send(res);
                }),
            )
            .unwrap();
        prewrite_rx.recv_timeout(Duration::from_secs(2))
    };

    let scale_pool = |size: usize| {
        cfg_controller
            .update_config("storage.scheduler-worker-pool-size", &format!("{}", size))
            .unwrap();
        assert_eq!(
            scheduler.get_sched_pool().get_pool_size(CommandPri::Normal),
            size
        );
    };

    scale_pool(1);
    fail::cfg(snapshot_fp, "1*pause").unwrap();
    // propose one prewrite to block the only worker
    do_prewrite(b"k1", b"v1").unwrap_err();

    scale_pool(2);

    // do prewrite again, as we scale another worker, this request should success
    do_prewrite(b"k2", b"v2").unwrap().unwrap();

    // restore to original config.
    scale_pool(origin_pool_size);
    fail::remove(snapshot_fp);
}