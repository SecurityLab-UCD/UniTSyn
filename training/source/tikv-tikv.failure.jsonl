{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_prewrite_without_value", "test": "fn test_prewrite_without_value() {\n    let cluster = new_server_cluster(0, 2);\n    cluster.pd_client.disable_default_operator();\n    let mut suite = TestSuiteBuilder::new().cluster(cluster).build();\n    let rid = suite.cluster.get_region(&[]).id;\n    let ctx = suite.get_context(rid);\n    let client = suite.get_tikv_client(rid).clone();\n    let large_value = vec![b'x'; 2 * txn_types::SHORT_VALUE_MAX_LEN];\n\n    // Perform a pessimistic prewrite with a large value.\n    let mut muts = vec![Mutation::default()];\n    muts[0].set_op(Op::Put);\n    muts[0].key = b\"key\".to_vec();\n    muts[0].value = large_value.clone();\n    try_kv_prewrite_pessimistic(&client, ctx.clone(), muts, b\"key\".to_vec(), 10);\n\n    let req = suite.new_changedata_request(rid);\n    let (mut req_tx, _, receive_event) = new_event_feed(suite.get_region_cdc_client(rid));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n\n    // The prewrite can be retrieved from incremental scan.\n    let event = receive_event(false);\n    assert_eq!(\n        event.get_events()[0].get_entries().entries[0].value,\n        large_value\n    );\n\n    // check_txn_status will put the lock again, but without value.\n    must_check_txn_status(&client, ctx.clone(), b\"key\", 10, 12, 12);\n    must_kv_commit(&client, ctx, vec![b\"key\".to_vec()], 10, 14, 14);\n    // The lock without value shouldn't be retrieved.\n    let event = receive_event(false);\n    assert_eq!(event.get_events()[0].get_entries().entries[0].commit_ts, 14);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/sst.rs::file_path", "test": "fn file_path() -> Result<()> {\n    let tempdir = tempdir();\n    let sst_path = tempdir\n        .path()\n        .join(\"test-data.sst\")\n        .to_string_lossy()\n        .to_string();\n    let sst_builder = <KvTestEngine as SstExt>::SstWriterBuilder::new();\n    let mut sst_writer = sst_builder.build(&sst_path)?;\n\n    sst_writer.put(b\"k1\", b\"v1\")?;\n    let info = sst_writer.finish()?;\n    assert_eq!(info.file_path().to_str(), Some(sst_path.as_str()));\n\n    Ok(())\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/failpoints/test_bucket.rs::test_refresh_bucket", "test": "fn test_refresh_bucket() {\n    let mut cluster = Cluster::default();\n    let store_id = cluster.node(0).id();\n    let raft_engine = cluster.node(0).running_state().unwrap().raft_engine.clone();\n    let router = &mut cluster.routers[0];\n\n    let region_2 = 2;\n    let region = router.region_detail(region_2);\n    let peer = region.get_peers()[0].clone();\n    router.wait_applied_to_current_term(region_2, Duration::from_secs(3));\n\n    // Region 2 [\"\", \"\"]\n    //   -> Region 2    [\"\", \"k22\"]\n    //      Region 1000 [\"k22\", \"\"] peer(1, 10)\n    let region_state = raft_engine\n        .get_region_state(region_2, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n\n    // to simulate the delay of set_apply_scheduler\n    fail::cfg(\"delay_set_apply_scheduler\", \"sleep(1000)\").unwrap();\n    split_region_and_refresh_bucket(\n        router,\n        region,\n        peer,\n        1000,\n        new_peer(store_id, 10),\n        b\"k22\",\n        false,\n    );\n\n    for _i in 1..100 {\n        std::thread::sleep(Duration::from_millis(50));\n        let meta = router\n            .must_query_debug_info(1000, Duration::from_secs(1))\n            .unwrap();\n        if !meta.bucket_keys.is_empty() {\n            assert_eq!(meta.bucket_keys.len(), 4); // include region start/end keys\n            assert_eq!(meta.bucket_keys[1], b\"1\".to_vec());\n            assert_eq!(meta.bucket_keys[2], b\"2\".to_vec());\n            return;\n        }\n    }\n    panic!(\"timeout for updating buckets\"); // timeout\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/failpoints/test_split.rs::test_restart_resume", "test": "fn test_restart_resume() {\n    let mut cluster = Cluster::default();\n    let raft_engine = cluster.node(0).running_state().unwrap().raft_engine.clone();\n    let router = &mut cluster.routers[0];\n\n    let region_id = 2;\n    let region = router.region_detail(region_id);\n    let peer = region.get_peers()[0].clone();\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n\n    let fp = \"async_write_before_cb\";\n    fail::cfg(fp, \"return\").unwrap();\n\n    let split_region_id = 1000;\n    let mut new_peer = peer.clone();\n    new_peer.set_id(1001);\n    split_region(\n        router,\n        region,\n        peer,\n        split_region_id,\n        new_peer,\n        None,\n        None,\n        b\"k11\",\n        b\"k11\",\n        true,\n    );\n\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"k22\", b\"value\");\n    let header = Box::new(router.new_request_for(region_id).take_header());\n    let (msg, mut sub) = PeerMsg::simple_write(header, put.encode());\n    router.send(region_id, msg).unwrap();\n    // Send a command to ensure split init is triggered.\n    block_on(sub.wait_proposed());\n\n    let region_state = raft_engine\n        .get_region_state(split_region_id, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n    let path = cluster\n        .node(0)\n        .tablet_registry()\n        .tablet_path(split_region_id, RAFT_INIT_LOG_INDEX);\n    assert!(!path.exists(), \"{} should not exist\", path.display());\n    drop(raft_engine);\n\n    cluster.restart(0);\n    // If split is resumed, the tablet should be installed.\n    assert!(\n        path.exists(),\n        \"{} should exist after restart\",\n        path.display()\n    );\n\n    // Both region should be recovered correctly.\n    let cases = vec![\n        (split_region_id, b\"k01\", b\"v01\"),\n        (region_id, b\"k21\", b\"v21\"),\n    ];\n    let router = &mut cluster.routers[0];\n    let new_epoch = router\n        .new_request_for(split_region_id)\n        .take_header()\n        .take_region_epoch();\n    // Split will be resumed for region 2, not removing the fp will make write block\n    // forever.\n    fail::remove(fp);\n    let timer = Instant::now();\n    for (region_id, key, val) in cases {\n        let mut put = SimpleWriteEncoder::with_capacity(64);\n        put.put(CF_DEFAULT, key, val);\n        let mut header = Box::new(router.new_request_for(region_id).take_header());\n        while timer.elapsed() < Duration::from_secs(3) {\n            // We need to wait till source peer replay split.\n            if *header.get_region_epoch() != new_epoch {\n                thread::sleep(Duration::from_millis(100));\n                header = Box::new(router.new_request_for(region_id).take_header());\n                continue;\n            }\n            break;\n        }\n        assert_eq!(*header.get_region_epoch(), new_epoch, \"{:?}\", header);\n        let (msg, sub) = PeerMsg::simple_write(header, put.encode());\n        router.send(region_id, msg).unwrap();\n        // Send a command to ensure split init is triggered.\n        let resp = block_on(sub.result()).unwrap();\n        assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_conf_change.rs::test_remove_by_conf_change", "test": "fn test_remove_by_conf_change() {\n    let cluster = Cluster::with_node_count(2, None);\n    let (region_id, peer_id, offset_id) = (2, 10, 1);\n    let mut req = add_learner(&cluster, offset_id, region_id, peer_id);\n\n    // write one kv to make flow control replicated.\n    let (key, val) = (b\"key\", b\"value\");\n    write_kv(&cluster, region_id, key, val);\n\n    let new_conf_ver = req.get_header().get_region_epoch().get_conf_ver() + 1;\n    req.mut_header()\n        .mut_region_epoch()\n        .set_conf_ver(new_conf_ver);\n    req.mut_admin_request()\n        .mut_change_peer()\n        .set_change_type(ConfChangeType::RemoveNode);\n    let (admin_msg, admin_sub) = PeerMsg::admin_command(req.clone());\n    // write one kv after removal\n    let (key, val) = (b\"key1\", b\"value\");\n    let header = Box::new(cluster.routers[0].new_request_for(region_id).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, key, val);\n    let (msg, sub) = PeerMsg::simple_write(header, put.encode());\n    // Send them at the same time so they will be all sent to learner.\n    cluster.routers[0].send(region_id, admin_msg).unwrap();\n    cluster.routers[0].send(region_id, msg).unwrap();\n    let resp = block_on(admin_sub.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    let resp = block_on(sub.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n\n    // Dispatch messages so the learner will receive conf remove and write at the\n    // same time.\n    cluster.dispatch(region_id, vec![]);\n    cluster.routers[1].wait_flush(region_id, Duration::from_millis(300));\n    // Wait for apply.\n    std::thread::sleep(Duration::from_millis(100));\n    let raft_engine = &cluster.node(1).running_state().unwrap().raft_engine;\n    let region_state = raft_engine\n        .get_region_state(region_id, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state.get_state(), PeerState::Tombstone);\n    assert_eq!(raft_engine.get_raft_state(region_id).unwrap(), None);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_conf_change.rs::test_unknown_peer", "test": "fn test_unknown_peer() {\n    let cluster = Cluster::with_node_count(1, None);\n\n    let router = &cluster.routers[0];\n    let header = router.new_request_for(2).take_header();\n\n    // Create a fake message to see whether it's responded.\n    let from_peer = new_peer(10, 10);\n    let mut msg = Box::<RaftMessage>::default();\n    msg.set_region_id(2);\n    msg.set_to_peer(header.get_peer().clone());\n    msg.set_region_epoch(header.get_region_epoch().clone());\n    msg.set_from_peer(from_peer.clone());\n    let raft_message = msg.mut_message();\n    raft_message.set_msg_type(raft::prelude::MessageType::MsgHeartbeat);\n    raft_message.set_from(10);\n    raft_message.set_term(10);\n\n    router.send_raft_message(msg).unwrap();\n    router.wait_flush(2, Duration::from_secs(3));\n    // If peer cache is updated correctly, it should be able to respond.\n    let msg = cluster.receiver(0).try_recv().unwrap();\n    assert_eq!(*msg.get_to_peer(), from_peer);\n    assert_eq!(msg.get_from_peer(), header.get_peer());\n    assert_eq!(\n        msg.get_message().get_msg_type(),\n        MessageType::MsgHeartbeatResponse\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_life.rs::test_destroy_by_larger_id", "test": "fn test_destroy_by_larger_id() {\n    let mut cluster = Cluster::default();\n    let router = &cluster.routers[0];\n    let test_region_id = 4;\n    let test_peer_id = 6;\n    let init_term = 5;\n    let mut msg = Box::<RaftMessage>::default();\n    msg.set_region_id(test_region_id);\n    msg.set_to_peer(new_peer(1, test_peer_id));\n    msg.mut_region_epoch().set_conf_ver(1);\n    msg.set_from_peer(new_peer(2, 8));\n    let raft_message = msg.mut_message();\n    raft_message.set_msg_type(MessageType::MsgHeartbeat);\n    raft_message.set_from(6);\n    raft_message.set_term(init_term);\n    // Create the peer.\n    router.send_raft_message(msg.clone()).unwrap();\n    // There must be heartbeat response.\n    let hb = cluster\n        .receiver(0)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap();\n    assert_eq!(\n        hb.get_message().get_msg_type(),\n        MessageType::MsgHeartbeatResponse\n    );\n\n    let timeout = Duration::from_secs(3);\n    let meta = router\n        .must_query_debug_info(test_region_id, timeout)\n        .unwrap();\n    assert_eq!(meta.raft_status.id, test_peer_id);\n\n    // Smaller ID should be ignored.\n    let mut smaller_id_msg = msg;\n    smaller_id_msg.set_to_peer(new_peer(1, test_peer_id - 1));\n    smaller_id_msg.mut_message().set_term(init_term + 1);\n    router.send_raft_message(smaller_id_msg.clone()).unwrap();\n    let meta = router\n        .must_query_debug_info(test_region_id, timeout)\n        .unwrap();\n    assert_eq!(meta.raft_status.id, test_peer_id);\n    assert_eq!(meta.raft_status.hard_state.term, init_term);\n    cluster\n        .receiver(0)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap_err();\n\n    // Smaller ID tombstone message should trigger report.\n    let mut smaller_id_tombstone_msg = smaller_id_msg.clone();\n    smaller_id_tombstone_msg.set_is_tombstone(true);\n    router.send_raft_message(smaller_id_tombstone_msg).unwrap();\n    let report = cluster\n        .receiver(0)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap();\n    assert_valid_report(&report, test_region_id, test_peer_id - 1);\n\n    // Larger ID should trigger destroy.\n    let mut larger_id_msg = smaller_id_msg;\n    larger_id_msg.set_to_peer(new_peer(1, test_peer_id + 1));\n    router.send_raft_message(larger_id_msg).unwrap();\n    assert_peer_not_exist(test_region_id, test_peer_id, router);\n    let meta = router\n        .must_query_debug_info(test_region_id, timeout)\n        .unwrap();\n    assert_eq!(meta.raft_status.id, test_peer_id + 1);\n    assert_eq!(meta.raft_status.hard_state.term, init_term + 1);\n\n    // New peer should survive restart.\n    cluster.restart(0);\n    let router = &cluster.routers[0];\n    let meta = router\n        .must_query_debug_info(test_region_id, timeout)\n        .unwrap();\n    assert_eq!(meta.raft_status.id, test_peer_id + 1);\n    assert_eq!(meta.raft_status.hard_state.term, init_term + 1);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_life.rs::test_gc_peer_response", "test": "fn test_gc_peer_response() {\n    let cluster = Cluster::with_node_count(2, None);\n    let region_id = 2;\n    let mut req = cluster.routers[0].new_request_for(region_id);\n    let admin_req = req.mut_admin_request();\n    admin_req.set_cmd_type(AdminCmdType::ChangePeer);\n    admin_req\n        .mut_change_peer()\n        .set_change_type(ConfChangeType::AddLearnerNode);\n    let store_id = cluster.node(1).id();\n    let new_peer = new_learner_peer(store_id, 10);\n    admin_req.mut_change_peer().set_peer(new_peer.clone());\n    let resp = cluster.routers[0].admin_command(2, req.clone()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    let raft_engine = &cluster.node(0).running_state().unwrap().raft_engine;\n    let region_state = raft_engine\n        .get_region_state(region_id, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert!(region_state.get_removed_records().is_empty());\n\n    let new_conf_ver = req.get_header().get_region_epoch().get_conf_ver() + 1;\n    req.mut_header()\n        .mut_region_epoch()\n        .set_conf_ver(new_conf_ver);\n    req.mut_admin_request()\n        .mut_change_peer()\n        .set_change_type(ConfChangeType::RemoveNode);\n    let resp = cluster.routers[0]\n        .admin_command(region_id, req.clone())\n        .unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    cluster.routers[0].wait_flush(region_id, Duration::from_millis(300));\n    // Drain all existing messages.\n    while cluster.receiver(0).try_recv().is_ok() {}\n\n    let mut msg = Box::<RaftMessage>::default();\n    msg.set_region_id(region_id);\n    msg.set_to_peer(req.get_header().get_peer().clone());\n    msg.set_from_peer(new_peer);\n    let receiver = &cluster.receiver(0);\n    for ty in &[MessageType::MsgRequestVote, MessageType::MsgRequestPreVote] {\n        msg.mut_message().set_msg_type(*ty);\n        cluster.routers[0].send_raft_message(msg.clone()).unwrap();\n        let tombstone_msg = match receiver.recv_timeout(Duration::from_millis(300)) {\n            Ok(msg) => msg,\n            Err(e) => panic!(\"failed to receive tombstone message {:?}: {:?}\", ty, e),\n        };\n        assert_tombstone_msg(&tombstone_msg, region_id, 10);\n    }\n    // Non-vote message should not trigger tombstone.\n    msg.mut_message().set_msg_type(MessageType::MsgHeartbeat);\n    cluster.routers[0].send_raft_message(msg).unwrap();\n    cluster\n        .receiver(0)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap_err();\n\n    // GcTick should also trigger tombstone.\n    cluster.routers[0]\n        .send(region_id, PeerMsg::Tick(PeerTick::GcPeer))\n        .unwrap();\n    let tombstone_msg = cluster\n        .receiver(0)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap();\n    assert_tombstone_msg(&tombstone_msg, region_id, 10);\n\n    // First message to create the peer and destroy.\n    cluster.routers[1]\n        .send_raft_message(Box::new(tombstone_msg.clone()))\n        .unwrap();\n    cluster.routers[1].wait_flush(region_id, Duration::from_millis(300));\n    cluster\n        .receiver(1)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap_err();\n    // Send message should trigger tombstone report.\n    cluster.routers[1]\n        .send_raft_message(Box::new(tombstone_msg))\n        .unwrap();\n    let report = cluster\n        .receiver(1)\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap();\n    assert_valid_report(&report, region_id, 10);\n    cluster.routers[0]\n        .send_raft_message(Box::new(report))\n        .unwrap();\n    let raft_engine = &cluster.node(0).running_state().unwrap().raft_engine;\n    let region_state = raft_engine\n        .get_region_state(region_id, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state.get_removed_records().len(), 1);\n    // Tick should flush records gc.\n    cluster.routers[0]\n        .send(region_id, PeerMsg::Tick(PeerTick::GcPeer))\n        .unwrap();\n    // Trigger a write to make sure records gc is finished.\n    let header = Box::new(cluster.routers[0].new_request_for(region_id).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key\", b\"value\");\n    let (msg, sub) = PeerMsg::simple_write(header, put.encode());\n    cluster.routers[0].send(region_id, msg).unwrap();\n    block_on(sub.result()).unwrap();\n    cluster.routers[0].wait_flush(region_id, Duration::from_millis(300));\n    let region_state = raft_engine\n        .get_region_state(region_id, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert!(region_state.get_removed_records().is_empty());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_merge.rs::test_merge", "test": "fn test_merge() {\n    let mut cluster = Cluster::default();\n    let store_id = cluster.node(0).id();\n    let raft_engine = cluster.node(0).running_state().unwrap().raft_engine.clone();\n    let router = &mut cluster.routers[0];\n\n    let do_split =\n        |r: &mut TestRouter, region: Region, peer: &Peer, v: u64| -> (Region, Region, Peer) {\n            let rid = region.get_id();\n            let old_region_state = raft_engine\n                .get_region_state(rid, u64::MAX)\n                .unwrap()\n                .unwrap();\n            let new_peer = new_peer(store_id, peer.get_id() + 1);\n            let (lhs, rhs) = split_region(\n                r,\n                region,\n                peer.clone(),\n                rid + 1,\n                new_peer.clone(),\n                Some(format!(\"k{}{}\", rid, v).as_bytes()),\n                Some(format!(\"k{}{}\", rid + 1, v).as_bytes()),\n                format!(\"k{}\", rid + 1).as_bytes(),\n                format!(\"k{}\", rid + 1).as_bytes(),\n                false,\n            );\n            let region_state = raft_engine\n                .get_region_state(rid, u64::MAX)\n                .unwrap()\n                .unwrap();\n            assert!(region_state.get_tablet_index() > old_region_state.get_tablet_index());\n            assert_eq!(\n                region_state.get_region().get_region_epoch().get_version(),\n                old_region_state\n                    .get_region()\n                    .get_region_epoch()\n                    .get_version()\n                    + 1,\n            );\n            let region_state = raft_engine\n                .get_region_state(rid + 1, u64::MAX)\n                .unwrap()\n                .unwrap();\n            assert_eq!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n            (lhs, rhs, new_peer)\n        };\n\n    let region_1 = router.region_detail(2);\n    let peer_1 = region_1.get_peers()[0].clone();\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n\n    // Split into 6.\n    let (region_1, region_2, peer_2) = do_split(router, region_1, &peer_1, 1);\n    let (region_2, region_3, peer_3) = do_split(router, region_2, &peer_2, 2);\n    let (region_3, region_4, peer_4) = do_split(router, region_3, &peer_3, 3);\n    let (region_4, region_5, peer_5) = do_split(router, region_4, &peer_4, 4);\n    let (region_5, region_6, peer_6) = do_split(router, region_5, &peer_5, 5);\n    drop(raft_engine);\n    // The last region version is smaller.\n    for (i, v) in [1, 2, 3, 4, 5, 5].iter().enumerate() {\n        let rid = region_1.get_id() + i as u64;\n        let snapshot = router.stale_snapshot(rid);\n        let key = format!(\"k{rid}{v}\");\n        assert!(\n            snapshot.get_value(key.as_bytes()).unwrap().is_some(),\n            \"{} {:?}\",\n            rid,\n            key\n        );\n    }\n\n    let region_2 = merge_region(&cluster, 0, region_1.clone(), peer_1, region_2, true);\n    {\n        let snapshot = cluster.routers[0].stale_snapshot(region_2.get_id());\n        let key = format!(\"k{}1\", region_1.get_id());\n        assert!(snapshot.get_value(key.as_bytes()).unwrap().is_some());\n    }\n    let region_5 = merge_region(&cluster, 0, region_6.clone(), peer_6, region_5, true);\n    {\n        let snapshot = cluster.routers[0].stale_snapshot(region_5.get_id());\n        let key = format!(\"k{}5\", region_6.get_id());\n        assert!(snapshot.get_value(key.as_bytes()).unwrap().is_some());\n    }\n    let region_3 = merge_region(&cluster, 0, region_2, peer_2, region_3, true);\n    let region_4 = merge_region(&cluster, 0, region_3, peer_3, region_4, true);\n    let region_5 = merge_region(&cluster, 0, region_4, peer_4, region_5, true);\n\n    cluster.restart(0);\n    let snapshot = cluster.routers[0].stale_snapshot(region_5.get_id());\n    for (i, v) in [1, 2, 3, 4, 5, 5].iter().enumerate() {\n        let rid = region_1.get_id() + i as u64;\n        let key = format!(\"k{rid}{v}\");\n        assert!(\n            snapshot.get_value(key.as_bytes()).unwrap().is_some(),\n            \"{} {:?}\",\n            rid,\n            key\n        );\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_pd_heartbeat.rs::test_store_heartbeat", "test": "fn test_store_heartbeat() {\n    let region_id = 2;\n    let cluster = Cluster::with_node_count(1, None);\n    let store_id = cluster.node(0).id();\n    let router = &cluster.routers[0];\n    // load data to split bucket.\n    let header = Box::new(router.new_request_for(region_id).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key\", b\"value\");\n    let data = put.encode();\n    let write_bytes = data.data_size();\n    let (msg, sub) = PeerMsg::simple_write(header, data);\n    router.send(region_id, msg).unwrap();\n    let _resp = block_on(sub.result()).unwrap();\n\n    // report store heartbeat to pd.\n    std::thread::sleep(std::time::Duration::from_millis(50));\n    router\n        .store_router()\n        .send_control(StoreMsg::Tick(StoreTick::PdStoreHeartbeat))\n        .unwrap();\n    std::thread::sleep(std::time::Duration::from_millis(50));\n\n    let stats = block_on(cluster.node(0).pd_client().get_store_stats_async(store_id)).unwrap();\n    if stats.get_start_time() > 0 {\n        assert_ne!(stats.get_capacity(), 0);\n        assert_ne!(stats.get_used_size(), 0);\n        assert_eq!(stats.get_keys_written(), 1);\n        assert!(stats.get_bytes_written() > write_bytes.try_into().unwrap());\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_read.rs::test_snap_with_invalid_parameter", "test": "fn test_snap_with_invalid_parameter() {\n    let cluster = Cluster::default();\n    let router = &cluster.routers[0];\n    std::thread::sleep(std::time::Duration::from_millis(200));\n    let region_id = 2;\n    let mut req = router.new_request_for(region_id);\n    let mut request_inner = Request::default();\n    request_inner.set_cmd_type(CmdType::Snap);\n    req.mut_requests().push(request_inner);\n\n    // store_id is incorrect;\n    let mut invalid_req = req.clone();\n    invalid_req.mut_header().set_peer(new_peer(2, 3));\n    let res = router.query(region_id, invalid_req).unwrap();\n    let error_resp = res.response().unwrap();\n    assert!(error_resp.get_header().has_error());\n\n    // run again, with incorrect peer_id\n    let mut invalid_req = req.clone();\n    invalid_req.mut_header().set_peer(new_peer(1, 4));\n    let res = router.query(region_id, invalid_req).unwrap();\n    let error_resp = res.response().unwrap();\n    assert!(error_resp.get_header().has_error());\n\n    // run with stale term\n    let mut invalid_req = req.clone();\n    invalid_req.mut_header().set_term(1);\n    let res = router.query(region_id, invalid_req).unwrap();\n    let error_resp = res.response().unwrap();\n    assert!(error_resp.get_header().has_error());\n\n    // run with stale read\n    let mut invalid_req = req.clone();\n    invalid_req\n        .mut_header()\n        .set_flags(WriteBatchFlags::STALE_READ.bits());\n    let res = router.query(region_id, invalid_req).unwrap();\n    let error_resp = res.response().unwrap();\n    assert!(error_resp.get_header().has_error());\n\n    // run again with invalid region_epoch\n    let mut invalid_req = req.clone();\n    let invalid_ver = req.get_header().get_region_epoch().get_version() + 1;\n    invalid_req\n        .mut_header()\n        .mut_region_epoch()\n        .set_version(invalid_ver);\n    let res = router.query(region_id, invalid_req).unwrap();\n    let error_resp = res.response().unwrap();\n    assert!(error_resp.get_header().has_error());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_split.rs::test_split", "test": "fn test_split() {\n    let mut cluster = Cluster::default();\n    let store_id = cluster.node(0).id();\n    let raft_engine = cluster.node(0).running_state().unwrap().raft_engine.clone();\n    let router = &mut cluster.routers[0];\n\n    let region_2 = 2;\n    let region = router.region_detail(region_2);\n    let peer = region.get_peers()[0].clone();\n    router.wait_applied_to_current_term(region_2, Duration::from_secs(3));\n\n    // Region 2 [\"\", \"\"]\n    //   -> Region 2    [\"\", \"k22\"]\n    //      Region 1000 [\"k22\", \"\"] peer(1, 10)\n    let region_state = raft_engine\n        .get_region_state(region_2, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n    let (left, mut right) = split_region(\n        router,\n        region,\n        peer.clone(),\n        1000,\n        new_peer(store_id, 10),\n        Some(b\"k11\"),\n        Some(b\"k33\"),\n        b\"k22\",\n        b\"k22\",\n        false,\n    );\n    let region_state = raft_engine\n        .get_region_state(region_2, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_ne!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n    assert_eq!(\n        region_state.get_region().get_region_epoch().get_version(),\n        INIT_EPOCH_VER + 1\n    );\n    let region_state0 = raft_engine\n        .get_region_state(region_2, region_state.get_tablet_index())\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state, region_state0);\n    let flushed_index = raft_engine\n        .get_flushed_index(region_2, CF_RAFT)\n        .unwrap()\n        .unwrap();\n    assert!(\n        flushed_index >= region_state.get_tablet_index(),\n        \"{flushed_index} >= {}\",\n        region_state.get_tablet_index()\n    );\n\n    // Region 2 [\"\", \"k22\"]\n    //   -> Region 2    [\"\", \"k11\"]\n    //      Region 1001 [\"k11\", \"k22\"] peer(1, 11)\n    let _ = split_region(\n        router,\n        left,\n        peer,\n        1001,\n        new_peer(store_id, 11),\n        Some(b\"k00\"),\n        Some(b\"k11\"),\n        b\"k11\",\n        b\"k11\",\n        false,\n    );\n    let region_state = raft_engine\n        .get_region_state(region_2, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_ne!(\n        region_state.get_tablet_index(),\n        region_state0.get_tablet_index()\n    );\n    assert_eq!(\n        region_state.get_region().get_region_epoch().get_version(),\n        INIT_EPOCH_VER + 2\n    );\n    let region_state1 = raft_engine\n        .get_region_state(region_2, region_state.get_tablet_index())\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state, region_state1);\n    let flushed_index = raft_engine\n        .get_flushed_index(region_2, CF_RAFT)\n        .unwrap()\n        .unwrap();\n    assert!(\n        flushed_index >= region_state.get_tablet_index(),\n        \"{flushed_index} >= {}\",\n        region_state.get_tablet_index()\n    );\n\n    // Region 1000 [\"k22\", \"\"] peer(1, 10)\n    //   -> Region 1000 [\"k22\", \"k33\"] peer(1, 10)\n    //      Region 1002 [\"k33\", \"\"]    peer(1, 12)\n    let region_1000 = 1000;\n    let region_state = raft_engine\n        .get_region_state(region_1000, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n    right = split_region(\n        router,\n        right,\n        new_peer(store_id, 10),\n        1002,\n        new_peer(store_id, 12),\n        Some(b\"k22\"),\n        Some(b\"k33\"),\n        b\"k33\",\n        b\"k33\",\n        false,\n    )\n    .1;\n    let region_state = raft_engine\n        .get_region_state(region_1000, u64::MAX)\n        .unwrap()\n        .unwrap();\n    assert_ne!(region_state.get_tablet_index(), RAFT_INIT_LOG_INDEX);\n    assert_eq!(\n        region_state.get_region().get_region_epoch().get_version(),\n        INIT_EPOCH_VER + 2\n    );\n    let region_state2 = raft_engine\n        .get_region_state(region_1000, region_state.get_tablet_index())\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_state, region_state2);\n    let flushed_index = raft_engine\n        .get_flushed_index(region_1000, CF_RAFT)\n        .unwrap()\n        .unwrap();\n    assert!(\n        flushed_index >= region_state.get_tablet_index(),\n        \"{flushed_index} >= {}\",\n        region_state.get_tablet_index()\n    );\n\n    // 1002 -> 1002, 1003\n    let split_key = Key::from_raw(b\"k44\").append_ts(TimeStamp::zero());\n    let actual_split_key = split_key.clone().truncate_ts().unwrap();\n    split_region(\n        router,\n        right,\n        new_peer(store_id, 12),\n        1003,\n        new_peer(store_id, 13),\n        Some(b\"k33\"),\n        Some(b\"k55\"),\n        split_key.as_encoded(),\n        actual_split_key.as_encoded(),\n        false,\n    );\n\n    // Split should survive restart.\n    drop(raft_engine);\n    cluster.restart(0);\n    let region_and_key = vec![\n        (2, b\"k00\"),\n        (1000, b\"k22\"),\n        (1001, b\"k11\"),\n        (1002, b\"k33\"),\n        (1003, b\"k55\"),\n    ];\n    for (region_id, key) in region_and_key {\n        let snapshot = cluster.routers[0].stale_snapshot(region_id);\n        assert!(\n            snapshot.get_value(key).unwrap().is_some(),\n            \"{} {:?}\",\n            region_id,\n            key\n        );\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_transfer_leader.rs::test_transfer_leader", "test": "fn test_transfer_leader() {\n    let mut cluster = Cluster::with_node_count(3, None);\n    let region_id = 2;\n    let router0 = &cluster.routers[0];\n\n    let mut req = router0.new_request_for(region_id);\n    let admin_req = req.mut_admin_request();\n    admin_req.set_cmd_type(AdminCmdType::ChangePeer);\n    admin_req\n        .mut_change_peer()\n        .set_change_type(ConfChangeType::AddNode);\n    let store_id = cluster.node(1).id();\n    let peer1 = new_peer(store_id, 10);\n    admin_req.mut_change_peer().set_peer(peer1.clone());\n    let req_clone = req.clone();\n    let resp = router0.admin_command(region_id, req_clone).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    let epoch = req.get_header().get_region_epoch();\n    let new_conf_ver = epoch.get_conf_ver() + 1;\n    let leader_peer = req.get_header().get_peer().clone();\n    let meta = router0\n        .must_query_debug_info(region_id, Duration::from_secs(3))\n        .unwrap();\n    assert_eq!(meta.region_state.epoch.version, epoch.get_version());\n    assert_eq!(meta.region_state.epoch.conf_ver, new_conf_ver);\n    assert_eq!(meta.region_state.peers, vec![leader_peer, peer1.clone()]);\n    let peer0_id = meta.raft_status.id;\n\n    // So heartbeat will create a learner.\n    cluster.dispatch(region_id, vec![]);\n    let router1 = &cluster.routers[1];\n    let meta = router1\n        .must_query_debug_info(region_id, Duration::from_secs(3))\n        .unwrap();\n    assert_eq!(peer0_id, meta.raft_status.soft_state.leader_id);\n    assert_eq!(meta.raft_status.id, peer1.id, \"{:?}\", meta);\n    assert_eq!(meta.region_state.epoch.version, epoch.get_version());\n    assert_eq!(meta.region_state.epoch.conf_ver, new_conf_ver);\n    cluster.dispatch(region_id, vec![]);\n\n    // Ensure follower has latest entries before transfer leader.\n    put_data(region_id, &mut cluster, 0, 1, b\"key1\");\n\n    // Perform transfer leader\n    must_transfer_leader(&cluster, region_id, 0, 1, peer1);\n\n    // Before transfer back to peer0, put some data again.\n    put_data(region_id, &mut cluster, 1, 0, b\"key2\");\n\n    // Perform transfer leader\n    let store_id = cluster.node(0).id();\n    must_transfer_leader(&cluster, region_id, 1, 0, new_peer(store_id, peer0_id));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/resolved_ts/tests/failpoints/mod.rs::test_report_min_resolved_ts", "test": "fn test_report_min_resolved_ts() {\n    fail::cfg(\"mock_tick_interval\", \"return(0)\").unwrap();\n    fail::cfg(\"mock_collect_tick_interval\", \"return(0)\").unwrap();\n    fail::cfg(\"mock_min_resolved_ts_interval\", \"return(0)\").unwrap();\n    let mut suite = TestSuite::new(1);\n    // default config is 1s\n    assert_eq!(\n        suite\n            .cluster\n            .cfg\n            .tikv\n            .raft_store\n            .report_min_resolved_ts_interval,\n        ReadableDuration::secs(1)\n    );\n    let region = suite.cluster.get_region(&[]);\n    let ts1 = suite.cluster.pd_client.get_min_resolved_ts();\n\n    // Prewrite\n    let (k, v) = (b\"k1\", b\"v\");\n    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.key = k.to_vec();\n    mutation.value = v.to_vec();\n    suite.must_kv_prewrite(region.id, vec![mutation], k.to_vec(), start_ts, false);\n\n    // Commit\n    let commit_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    suite.must_kv_commit(region.id, vec![k.to_vec()], start_ts, commit_ts);\n\n    sleep_ms(100);\n    let ts3 = suite.cluster.pd_client.get_min_resolved_ts();\n    let unapplied_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    assert!(ts3 > ts1);\n    assert!(TimeStamp::new(ts3) > commit_ts);\n    assert!(TimeStamp::new(ts3) < unapplied_ts);\n    fail::remove(\"mock_tick_interval\");\n    fail::remove(\"mock_collect_tick_interval\");\n    fail::remove(\"mock_min_resolved_ts_interval\");\n    suite.stop();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_async_fetch.rs::test_node_async_fetch", "test": "fn test_node_async_fetch() {\n    let count = 3;\n    let mut cluster = new_node_cluster(0, count);\n\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(100000);\n    cluster.cfg.raft_store.raft_log_gc_threshold = 50;\n    cluster.cfg.raft_store.raft_log_gc_size_limit = Some(ReadableSize::mb(20));\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(100);\n    cluster.cfg.raft_store.raft_log_reserve_max_ticks = 2;\n    cluster.cfg.raft_store.raft_entry_cache_life_time = ReadableDuration::millis(100);\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let mut before_states = HashMap::default();\n\n    for (&id, engines) in &cluster.engines {\n        must_get_equal(&engines.kv, b\"k1\", b\"v1\");\n        let mut state: RaftApplyState = engines\n            .kv\n            .get_msg_cf(CF_RAFT, &keys::apply_state_key(1))\n            .unwrap()\n            .unwrap_or_default();\n        let state = state.take_truncated_state();\n        // compact should not start\n        assert_eq!(RAFT_INIT_LOG_INDEX, state.get_index());\n        assert_eq!(RAFT_INIT_LOG_TERM, state.get_term());\n        before_states.insert(id, state);\n    }\n\n    cluster.stop_node(1);\n\n    for i in 1..60u32 {\n        let k = i.to_string().into_bytes();\n        let v = k.clone();\n        cluster.must_put(&k, &v);\n    }\n\n    // wait log gc.\n    sleep_ms(500);\n\n    let (sender, receiver) = mpsc::channel();\n    let sync_sender = Mutex::new(sender);\n    fail::cfg_callback(\"on_async_fetch_return\", move || {\n        let sender = sync_sender.lock().unwrap();\n        sender.send(true).unwrap();\n    })\n    .unwrap();\n    cluster.run_node(1).unwrap();\n\n    // limit has not reached, should not gc.\n    for (&id, engines) in &cluster.engines {\n        let mut state: RaftApplyState = engines\n            .kv\n            .get_msg_cf(CF_RAFT, &keys::apply_state_key(1))\n            .unwrap()\n            .unwrap_or_default();\n        let after_state = state.take_truncated_state();\n\n        let before_state = &before_states[&id];\n        let idx = after_state.get_index();\n        assert_eq!(idx, before_state.get_index());\n    }\n\n    assert_eq!(\n        receiver.recv_timeout(Duration::from_millis(500)).unwrap(),\n        true\n    );\n\n    // logs should be replicated to node 1 successfully.\n    for i in 1..60u32 {\n        let k = i.to_string().into_bytes();\n        let v = k.clone();\n        must_get_equal(&cluster.engines[&1].kv, &k, &v);\n    }\n\n    for i in 60..500u32 {\n        let k = i.to_string().into_bytes();\n        let v = k.clone();\n        cluster.must_put(&k, &v);\n        let v2 = cluster.get(&k);\n        assert_eq!(v2, Some(v));\n\n        if i > 100\n            && check_compacted(\n                &cluster.engines,\n                &before_states,\n                1,\n                false, // must_compacted\n            )\n        {\n            return;\n        }\n    }\n    check_compacted(\n        &cluster.engines,\n        &before_states,\n        1,\n        true, // must_compacted\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_backup.rs::backup_blocked_by_memory_lock", "test": "fn backup_blocked_by_memory_lock() {\n    let suite = TestSuite::new(1, 144 * 1024 * 1024, ApiVersion::V1);\n\n    fail::cfg(\"raftkv_async_write_finish\", \"pause\").unwrap();\n    let tikv_cli = suite.tikv_cli.clone();\n    let (k, v) = (b\"my_key\", b\"my_value\");\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.key = k.to_vec();\n    mutation.value = v.to_vec();\n    let mut prewrite_req = PrewriteRequest::default();\n    prewrite_req.set_context(suite.context.clone());\n    prewrite_req.mut_mutations().push(mutation);\n    prewrite_req.set_primary_lock(k.to_vec());\n    prewrite_req.set_start_version(20);\n    prewrite_req.set_lock_ttl(2000);\n    prewrite_req.set_use_async_commit(true);\n    let th = thread::spawn(move || tikv_cli.kv_prewrite(&prewrite_req).unwrap());\n\n    thread::sleep(Duration::from_millis(200));\n\n    // Trigger backup request.\n    let tmp = Builder::new().tempdir().unwrap();\n    let backup_ts = TimeStamp::from(21);\n    let storage_path = make_unique_dir(tmp.path());\n    let rx = suite.backup(\n        b\"a\".to_vec(), // start\n        b\"z\".to_vec(), // end\n        0.into(),      // begin_ts\n        backup_ts,\n        &storage_path,\n    );\n    let resp = block_on(rx.collect::<Vec<_>>());\n    match &resp[0].get_error().detail {\n        Some(Error_oneof_detail::KvError(key_error)) => {\n            assert!(key_error.has_locked());\n        }\n        _ => panic!(\"unexpected response\"),\n    }\n\n    fail::remove(\"raftkv_async_write_finish\");\n    th.join().unwrap();\n\n    suite.stop();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_cmd_epoch_checker.rs::test_reject_proposal_during_region_split", "test": "fn test_reject_proposal_during_region_split() {\n    let mut cluster = new_node_cluster(0, 3);\n    let pd_client = cluster.pd_client.clone();\n    pd_client.disable_default_operator();\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k\", b\"v\");\n\n    // Pause on applying so that region split is not finished.\n    let fp = \"apply_before_split\";\n    fail::cfg(fp, \"pause\").unwrap();\n\n    // Try to split region.\n    let (split_tx, split_rx) = mpsc::channel();\n    let cb = Callback::read(Box::new(move |resp: ReadResponse<RocksSnapshot>| {\n        split_tx.send(resp.response).unwrap()\n    }));\n    let r = cluster.get_region(b\"\");\n    cluster.split_region(&r, b\"k\", cb);\n    split_rx\n        .recv_timeout(Duration::from_millis(100))\n        .unwrap_err();\n\n    // Try to put a key.\n    let force_delay_propose_batch_raft_command_fp = \"force_delay_propose_batch_raft_command\";\n    let mut receivers = vec![];\n    for i in 0..2 {\n        if i == 1 {\n            // Test another path of calling proposed callback.\n            fail::cfg(force_delay_propose_batch_raft_command_fp, \"2*return\").unwrap();\n        }\n        let write_req = make_write_req(&mut cluster, b\"k1\");\n        let (cb, mut cb_receivers) = make_cb(&write_req);\n        cluster\n            .sim\n            .rl()\n            .async_command_on_node(1, write_req, cb)\n            .unwrap();\n        // The write request should be blocked until split is finished.\n        cb_receivers.assert_not_ready();\n        receivers.push(cb_receivers);\n    }\n\n    fail::remove(fp);\n    // Split is finished.\n    assert!(\n        !split_rx\n            .recv_timeout(Duration::from_secs(1))\n            .unwrap()\n            .get_header()\n            .has_error()\n    );\n\n    // The write request fails due to epoch not match.\n    for mut r in receivers {\n        r.assert_err();\n    }\n\n    // New write request can succeed.\n    let write_req = make_write_req(&mut cluster, b\"k1\");\n    let (cb, mut cb_receivers) = make_cb(&write_req);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, write_req, cb)\n        .unwrap();\n    cb_receivers.assert_ok();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_cmd_epoch_checker.rs::test_reject_proposal_during_region_merge", "test": "fn test_reject_proposal_during_region_merge() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    let pd_client = cluster.pd_client.clone();\n    pd_client.disable_default_operator();\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k\", b\"v\");\n\n    let r = cluster.get_region(b\"\");\n    cluster.must_split(&r, b\"k\");\n    // Let the new region catch up.\n    cluster.must_put(b\"a\", b\"v\");\n    cluster.must_put(b\"k\", b\"v\");\n\n    let prepare_merge_fp = \"apply_before_prepare_merge\";\n    let commit_merge_fp = \"apply_before_commit_merge\";\n\n    // Pause on applying so that prepare-merge is not finished.\n    fail::cfg(prepare_merge_fp, \"pause\").unwrap();\n    // Try to merge region.\n    let (merge_tx, merge_rx) = mpsc::channel();\n    let cb = Callback::read(Box::new(move |resp: ReadResponse<RocksSnapshot>| {\n        merge_tx.send(resp.response).unwrap()\n    }));\n    let source = cluster.get_region(b\"\");\n    let target = cluster.get_region(b\"k\");\n    cluster.merge_region(source.get_id(), target.get_id(), cb);\n    merge_rx\n        .recv_timeout(Duration::from_millis(100))\n        .unwrap_err();\n\n    // Try to put a key on the source region.\n    let force_delay_propose_batch_raft_command_fp = \"force_delay_propose_batch_raft_command\";\n    let mut receivers = vec![];\n    for i in 0..2 {\n        if i == 1 {\n            // Test another path of calling proposed callback.\n            fail::cfg(force_delay_propose_batch_raft_command_fp, \"2*return\").unwrap();\n        }\n        let write_req = make_write_req(&mut cluster, b\"a\");\n        let (cb, mut cb_receivers) = make_cb(&write_req);\n        cluster\n            .sim\n            .rl()\n            .async_command_on_node(1, write_req, cb)\n            .unwrap();\n        // The write request should be blocked until prepare-merge is finished.\n        cb_receivers.assert_not_ready();\n        receivers.push(cb_receivers);\n    }\n\n    // Pause on the second phase of region merge.\n    fail::cfg(commit_merge_fp, \"pause\").unwrap();\n\n    // prepare-merge is finished.\n    fail::remove(prepare_merge_fp);\n    assert!(\n        !merge_rx\n            .recv_timeout(Duration::from_secs(5))\n            .unwrap()\n            .get_header()\n            .has_error()\n    );\n    // The write request fails due to epoch not match.\n    for mut r in receivers {\n        r.assert_err();\n    }\n\n    // Write request is rejected because the source region is merging.\n    // It's not handled by epoch checker now.\n    for i in 0..2 {\n        if i == 1 {\n            // Test another path of calling proposed callback.\n            fail::cfg(force_delay_propose_batch_raft_command_fp, \"2*return\").unwrap();\n        }\n        let write_req = make_write_req(&mut cluster, b\"a\");\n        let (cb, mut cb_receivers) = make_cb(&write_req);\n        cluster\n            .sim\n            .rl()\n            .async_command_on_node(1, write_req, cb)\n            .unwrap();\n        cb_receivers.assert_err();\n    }\n\n    // Try to put a key on the target region.\n    let mut receivers = vec![];\n    for i in 0..2 {\n        if i == 1 {\n            // Test another path of calling proposed callback.\n            fail::cfg(force_delay_propose_batch_raft_command_fp, \"2*return\").unwrap();\n        }\n        let write_req = make_write_req(&mut cluster, b\"k\");\n        let (cb, mut cb_receivers) = make_cb(&write_req);\n        cluster\n            .sim\n            .rl()\n            .async_command_on_node(1, write_req, cb)\n            .unwrap();\n        // The write request should be blocked until commit-merge is finished.\n        cb_receivers.assert_not_ready();\n        receivers.push(cb_receivers);\n    }\n\n    // Wait for region merge done.\n    fail::remove(commit_merge_fp);\n    pd_client.check_merged_timeout(source.get_id(), Duration::from_secs(5));\n    // The write request fails due to epoch not match.\n    for mut r in receivers {\n        r.assert_err();\n    }\n\n    // New write request can succeed.\n    let write_req = make_write_req(&mut cluster, b\"k\");\n    let (cb, mut cb_receivers) = make_cb(&write_req);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, write_req, cb)\n        .unwrap();\n    cb_receivers.assert_ok();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_cmd_epoch_checker.rs::test_accept_proposal_during_conf_change", "test": "fn test_accept_proposal_during_conf_change() {\n    let mut cluster = new_node_cluster(0, 2);\n    cluster.pd_client.disable_default_operator();\n    let r = cluster.run_conf_change();\n    cluster.must_put(b\"a\", b\"v\");\n\n    let conf_change_fp = \"apply_on_conf_change_all_1\";\n    fail::cfg(conf_change_fp, \"pause\").unwrap();\n    let mut add_peer_rx = cluster.async_add_peer(r, new_peer(2, 2)).unwrap();\n    add_peer_rx\n        .recv_timeout(Duration::from_millis(100))\n        .unwrap_err();\n\n    // Conf change doesn't affect proposals.\n    let write_req = make_write_req(&mut cluster, b\"k\");\n    let (cb, mut cb_receivers) = make_cb(&write_req);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, write_req, cb)\n        .unwrap();\n    cb_receivers\n        .committed\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap();\n    cb_receivers.proposed.try_recv().unwrap();\n\n    fail::remove(conf_change_fp);\n    assert!(\n        !add_peer_rx\n            .recv_timeout(Duration::from_secs(1))\n            .unwrap()\n            .get_header()\n            .has_error()\n    );\n    assert!(\n        !cb_receivers\n            .applied\n            .recv_timeout(Duration::from_secs(1))\n            .unwrap()\n            .get_header()\n            .has_error()\n    );\n    must_get_equal(&cluster.get_engine(2), b\"k\", b\"v\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_cmd_epoch_checker.rs::test_not_invoke_committed_cb_when_fail_to_commit", "test": "fn test_not_invoke_committed_cb_when_fail_to_commit() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k\", b\"v\");\n\n    // Partition the leader and followers to let the leader fails to commit the\n    // proposal.\n    cluster.partition(vec![1], vec![2, 3]);\n    let write_req = make_write_req(&mut cluster, b\"k1\");\n    let (cb, mut cb_receivers) = make_cb(&write_req);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, write_req, cb)\n        .unwrap();\n    // Check the request is proposed but not committed.\n    cb_receivers\n        .committed\n        .recv_timeout(Duration::from_millis(200))\n        .unwrap_err();\n    cb_receivers.proposed.try_recv().unwrap();\n\n    // The election timeout is 250ms by default.\n    let election_timeout = cluster.cfg.raft_store.raft_base_tick_interval.0\n        * cluster.cfg.raft_store.raft_election_timeout_ticks as u32;\n    std::thread::sleep(2 * election_timeout);\n\n    // Make sure a new leader is elected and will discard the previous proposal when\n    // partition is recovered.\n    cluster.must_put(b\"k2\", b\"v\");\n    cluster.clear_send_filters();\n\n    let resp = cb_receivers\n        .applied\n        .recv_timeout(Duration::from_secs(1))\n        .unwrap();\n    assert!(resp.get_header().has_error(), \"{:?}\", resp);\n    // The committed callback shouldn't be invoked.\n    cb_receivers.committed.try_recv().unwrap_err();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_readpool_full", "test": "fn test_readpool_full() {\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"future_pool_spawn_full\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_region_error().has_server_is_busy());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_snapshot_failed_2", "test": "fn test_snapshot_failed_2() {\n    let product = ProductTable::new();\n    let (store, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    store.get_engine().trigger_not_leader();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_region_error().has_not_leader());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_disk_full.rs::test_almost_and_already_full_behavior", "test": "fn test_almost_and_already_full_behavior() {\n    let mut cluster = new_server_cluster(0, 5);\n    // To ensure the thread has full store disk usage infomation.\n    cluster.cfg.raft_store.store_batch_system.pool_size = 1;\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_transfer_leader(region.get_id(), new_peer(1, 1));\n    // To ensure followers have reported disk usages to the leader.\n    for i in [2u64, 3] {\n        fail::cfg(get_fp(DiskUsage::AlmostFull, i), \"return\").unwrap();\n    }\n    for i in [4u64, 5] {\n        fail::cfg(get_fp(DiskUsage::AlreadyFull, i), \"return\").unwrap();\n    }\n    for i in 1..5 {\n        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);\n    }\n\n    let lead_client = PeerClient::new(&cluster, 1, new_peer(1, 1));\n    let prewrite_ts = get_tso(&cluster.pd_client);\n    let res = lead_client.try_kv_prewrite(\n        vec![new_mutation(Op::Put, b\"k2\", b\"v2\")],\n        b\"k2\".to_vec(),\n        prewrite_ts,\n        DiskFullOpt::AllowedOnAlmostFull,\n    );\n    assert!(!res.get_region_error().has_disk_full());\n    lead_client.must_kv_commit(\n        vec![b\"k2\".to_vec()],\n        prewrite_ts,\n        get_tso(&cluster.pd_client),\n    );\n\n    let index_1 = cluster.raft_local_state(1, 1).last_index;\n    let index_2 = cluster.raft_local_state(1, 2).last_index;\n    let index_3 = cluster.raft_local_state(1, 3).last_index;\n    let index_4 = cluster.raft_local_state(1, 4).last_index;\n    let index_5 = cluster.raft_local_state(1, 5).last_index;\n    assert!(\n        index_1 >= index_2\n            && index_1 >= index_3\n            && index_2 > index_4\n            && index_2 > index_5\n            && index_3 > index_4\n            && index_3 > index_5\n    );\n\n    for i in [2u64, 3] {\n        fail::remove(get_fp(DiskUsage::AlmostFull, i));\n    }\n    for i in [4u64, 5] {\n        fail::remove(get_fp(DiskUsage::AlreadyFull, i));\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_disk_full.rs::test_down_node_when_disk_full", "test": "fn test_down_node_when_disk_full() {\n    let mut cluster = new_server_cluster(0, 5);\n    // To ensure the thread has full store disk usage infomation.\n    cluster.cfg.raft_store.store_batch_system.pool_size = 1;\n    cluster.cfg.raft_store.max_peer_down_duration = ReadableDuration::secs(1);\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    let region = cluster.get_region(b\"k1\");\n    for i in 3..6 {\n        fail::cfg(get_fp(DiskUsage::AlmostFull, i), \"return\").unwrap();\n        ensure_disk_usage_is_reported(&mut cluster, i, i, &region);\n    }\n\n    let lead_client = PeerClient::new(&cluster, 1, new_peer(1, 1));\n    let prewrite_ts = get_tso(&cluster.pd_client);\n    let res = lead_client.try_kv_prewrite(\n        vec![new_mutation(Op::Put, b\"k2\", b\"v2\")],\n        b\"k2\".to_vec(),\n        prewrite_ts,\n        DiskFullOpt::AllowedOnAlmostFull,\n    );\n    assert!(!res.get_region_error().has_disk_full());\n    lead_client.must_kv_commit(\n        vec![b\"k2\".to_vec()],\n        prewrite_ts,\n        get_tso(&cluster.pd_client),\n    );\n\n    cluster.stop_node(2);\n    wait_down_peers_reported(&cluster, 1, 2u64);\n\n    let prewrite_ts = get_tso(&cluster.pd_client);\n    let res = lead_client.try_kv_prewrite(\n        vec![new_mutation(Op::Put, b\"k3\", b\"v3\")],\n        b\"k3\".to_vec(),\n        prewrite_ts,\n        DiskFullOpt::AllowedOnAlmostFull,\n    );\n    assert!(!res.get_region_error().has_disk_full());\n    lead_client.must_kv_commit(\n        vec![b\"k3\".to_vec()],\n        prewrite_ts,\n        get_tso(&cluster.pd_client),\n    );\n\n    for i in 3..6 {\n        fail::remove(get_fp(DiskUsage::AlmostFull, i));\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_metrics.rs::test_txn_mvcc_filtered", "test": "fn test_txn_mvcc_filtered() {\n    MVCC_VERSIONS_HISTOGRAM.reset();\n    GC_COMPACTION_FILTERED.reset();\n\n    let mut engine = TestEngineBuilder::new().build().unwrap();\n    let raw_engine = engine.get_rocksdb();\n    let value = vec![b'v'; 512];\n    let mut gc_runner = TestGcRunner::new(0);\n\n    // GC can't delete keys after the given safe point.\n    must_prewrite_put(&mut engine, b\"zkey\", &value, b\"zkey\", 100);\n    must_commit(&mut engine, b\"zkey\", 100, 110);\n    gc_runner.safe_point(50).gc(&raw_engine);\n    must_get(&mut engine, b\"zkey\", 110, &value);\n\n    // GC can't delete keys before the safe ponit if they are latest versions.\n    gc_runner.safe_point(200).gc(&raw_engine);\n    must_get(&mut engine, b\"zkey\", 110, &value);\n\n    must_prewrite_put(&mut engine, b\"zkey\", &value, b\"zkey\", 120);\n    must_commit(&mut engine, b\"zkey\", 120, 130);\n\n    // GC can't delete the latest version before the safe ponit.\n    gc_runner.safe_point(115).gc(&raw_engine);\n    must_get(&mut engine, b\"zkey\", 110, &value);\n\n    // GC a version will also delete the key on default CF.\n    gc_runner.safe_point(200).gc(&raw_engine);\n    assert_eq!(\n        MVCC_VERSIONS_HISTOGRAM\n            .with_label_values(&[STAT_TXN_KEYMODE])\n            .get_sample_sum(),\n        4_f64\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTERED\n            .with_label_values(&[STAT_TXN_KEYMODE])\n            .get(),\n        1\n    );\n\n    MVCC_VERSIONS_HISTOGRAM.reset();\n    GC_COMPACTION_FILTERED.reset();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_metrics.rs::test_raw_mvcc_filtered", "test": "fn test_raw_mvcc_filtered() {\n    MVCC_VERSIONS_HISTOGRAM.reset();\n    GC_COMPACTION_FILTERED.reset();\n\n    let mut cfg = DbConfig::default();\n    cfg.defaultcf.disable_auto_compactions = true;\n    cfg.defaultcf.dynamic_level_bytes = false;\n\n    let engine = TestEngineBuilder::new()\n        .api_version(ApiVersion::V2)\n        .build_with_cfg(&cfg)\n        .unwrap();\n    let raw_engine = engine.get_rocksdb();\n    let mut gc_runner = TestGcRunner::new(0);\n\n    let user_key = b\"r\\0aaaaaaaaaaa\";\n\n    let test_raws = vec![\n        (user_key, 100, false),\n        (user_key, 90, false),\n        (user_key, 70, false),\n    ];\n\n    let modifies = test_raws\n        .into_iter()\n        .map(|(key, ts, is_delete)| {\n            (\n                make_key(key, ts),\n                ApiV2::encode_raw_value(RawValue {\n                    user_value: &[0; 10][..],\n                    expire_ts: Some(TimeStamp::max().into_inner()),\n                    is_delete,\n                }),\n            )\n        })\n        .map(|(k, v)| Modify::Put(CF_DEFAULT, Key::from_encoded_slice(k.as_slice()), v))\n        .collect();\n\n    let ctx = Context {\n        api_version: ApiVersion::V2,\n        ..Default::default()\n    };\n    let batch = WriteData::from_modifies(modifies);\n\n    engine.write(&ctx, batch).unwrap();\n\n    gc_runner.safe_point(80).gc_raw(&raw_engine);\n\n    assert_eq!(\n        MVCC_VERSIONS_HISTOGRAM\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get_sample_sum(),\n        1_f64\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTERED\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        1\n    );\n\n    MVCC_VERSIONS_HISTOGRAM.reset();\n    GC_COMPACTION_FILTERED.reset();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_import_service.rs::test_download_sst_blocking_sst_writer", "test": "fn test_download_sst_blocking_sst_writer() {\n    let (_cluster, ctx, tikv, import) = new_cluster_and_tikv_import_client();\n    let temp_dir = Builder::new()\n        .prefix(\"test_download_sst_blocking_sst_writer\")\n        .tempdir()\n        .unwrap();\n\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, _) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n    // Sleep 20s, make sure it is large than grpc_keepalive_timeout (3s).\n    let sst_writer_open_fp = \"on_open_sst_writer\";\n    fail::cfg(sst_writer_open_fp, \"sleep(20000)\").unwrap();\n\n    // Now perform a proper download.\n    let mut download = DownloadRequest::default();\n    download.set_sst(meta.clone());\n    download.set_storage_backend(external_storage_export::make_local_backend(temp_dir.path()));\n    download.set_name(\"test.sst\".to_owned());\n    download.mut_sst().mut_range().set_start(vec![sst_range.1]);\n    download\n        .mut_sst()\n        .mut_range()\n        .set_end(vec![sst_range.1 + 1]);\n    download.mut_sst().mut_range().set_start(Vec::new());\n    download.mut_sst().mut_range().set_end(Vec::new());\n    let result = import.download(&download).unwrap();\n    assert!(!result.get_is_empty());\n    assert_eq!(result.get_range().get_start(), &[sst_range.0]);\n    assert_eq!(result.get_range().get_end(), &[sst_range.1 - 1]);\n\n    fail::remove(sst_writer_open_fp);\n\n    // Do an ingest and verify the result is correct.\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error());\n\n    check_ingested_kvs(&tikv, &ctx, sst_range);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_import_service.rs::test_ingest_reentrant", "test": "fn test_ingest_reentrant() {\n    let (cluster, ctx, _tikv, import) = new_cluster_and_tikv_import_client();\n\n    let temp_dir = Builder::new()\n        .prefix(\"test_ingest_reentrant\")\n        .tempdir()\n        .unwrap();\n\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n    upload_sst(&import, &meta, &data).unwrap();\n\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx);\n    ingest.set_sst(meta.clone());\n\n    // Don't delete ingested sst file or we cannot find sst file in next ingest.\n    fail::cfg(\"dont_delete_ingested_sst\", \"1*return\").unwrap();\n\n    let node_id = *cluster.sim.rl().get_node_ids().iter().next().unwrap();\n    // Use sst save path to track the sst file checksum.\n    let save_path = cluster\n        .sim\n        .rl()\n        .importers\n        .get(&node_id)\n        .unwrap()\n        .get_path(&meta);\n\n    let checksum1 = calc_crc32(save_path.clone()).unwrap();\n    // Do ingest and it will ingest successs.\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error());\n\n    let checksum2 = calc_crc32(save_path).unwrap();\n    // TODO: Remove this once write_global_seqno is deprecated.\n    // Checksums are the same since the global seqno in the SST file no longer gets\n    // updated with the default setting, which is write_global_seqno=false.\n    assert_eq!(checksum1, checksum2);\n    // Do ingest again and it can be reentrant\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_import_service.rs::test_ingest_key_manager_delete_file_failed", "test": "fn test_ingest_key_manager_delete_file_failed() {\n    // test with tde\n    let (_tmp_key_dir, cluster, ctx, _tikv, import) = new_cluster_and_tikv_import_client_tde();\n\n    let temp_dir = Builder::new()\n        .prefix(\"test_download_sst_blocking_sst_writer\")\n        .tempdir()\n        .unwrap();\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n    upload_sst(&import, &meta, &data).unwrap();\n\n    let deregister_fp = \"key_manager_fails_before_delete_file\";\n    // the first delete is in check before ingest, the second is in ingest cleanup\n    // set the ingest clean up failed to trigger remove file but not remove key\n    // condition\n    fail::cfg(deregister_fp, \"1*off->1*return->off\").unwrap();\n\n    // Do an ingest and verify the result is correct. Though the ingest succeeded,\n    // the clone file is still in the key manager\n    // TODO: how to check the key manager contains the clone key\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta.clone());\n    let resp = import.ingest(&ingest).unwrap();\n\n    assert!(!resp.has_error());\n\n    fail::remove(deregister_fp);\n\n    let node_id = *cluster.sim.rl().get_node_ids().iter().next().unwrap();\n    let save_path = cluster\n        .sim\n        .rl()\n        .importers\n        .get(&node_id)\n        .unwrap()\n        .get_path(&meta);\n    // wait up to 5 seconds to make sure raw uploaded file is deleted by the async\n    // clean up task.\n    for _ in 0..50 {\n        if !save_path.as_path().exists() {\n            break;\n        }\n        std::thread::sleep(Duration::from_millis(100));\n    }\n    assert!(!save_path.as_path().exists());\n\n    // Do upload and ingest again, though key manager contains this file, the ingest\n    // action should success.\n    upload_sst(&import, &meta, &data).unwrap();\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx);\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_import_service.rs::test_ingest_file_twice_and_conflict", "test": "fn test_ingest_file_twice_and_conflict() {\n    // test with tde\n    let (_tmp_key_dir, _cluster, ctx, _tikv, import) = new_cluster_and_tikv_import_client_tde();\n\n    let temp_dir = Builder::new()\n        .prefix(\"test_ingest_file_twice_and_conflict\")\n        .tempdir()\n        .unwrap();\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n    upload_sst(&import, &meta, &data).unwrap();\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx);\n    ingest.set_sst(meta);\n\n    let latch_fp = \"import::sst_service::ingest\";\n    let (tx1, rx1) = channel();\n    let (tx2, rx2) = channel();\n    let tx1 = Arc::new(Mutex::new(tx1));\n    let rx2 = Arc::new(Mutex::new(rx2));\n    fail::cfg_callback(latch_fp, move || {\n        tx1.lock().unwrap().send(()).unwrap();\n        rx2.lock().unwrap().recv().unwrap();\n    })\n    .unwrap();\n    let resp_recv = import.ingest_async(&ingest).unwrap();\n\n    // Make sure the before request has acquired lock.\n    rx1.recv().unwrap();\n\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(resp.has_error());\n    assert_eq!(\"ingest file conflict\", resp.get_error().get_message());\n    tx2.send(()).unwrap();\n    let resp = block_on(resp_recv).unwrap();\n    assert!(!resp.has_error());\n\n    fail::remove(latch_fp);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(resp.has_error());\n    assert_eq!(\n        \"The file which would be ingested doest not exist.\",\n        resp.get_error().get_message()\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_import_service.rs::test_ingest_sst_v2", "test": "fn test_ingest_sst_v2() {\n    let mut cluster = test_raftstore_v2::new_server_cluster(1, 1);\n    let (ctx, _tikv, import) = open_cluster_and_tikv_import_client_v2(None, &mut cluster);\n    let temp_dir = Builder::new().prefix(\"test_ingest_sst\").tempdir().unwrap();\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n\n    // No region id and epoch.\n    send_upload_sst(&import, &meta, &data).unwrap();\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta.clone());\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n    send_upload_sst(&import, &meta, &data).unwrap();\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error(), \"{:?}\", resp.get_error());\n    fail::cfg(\"on_cleanup_import_sst\", \"return\").unwrap();\n    let (tx, rx) = channel::<()>();\n    let tx = Arc::new(Mutex::new(tx));\n    fail::cfg_callback(\"on_cleanup_import_sst_schedule\", move || {\n        tx.lock().unwrap().send(()).unwrap();\n    })\n    .unwrap();\n\n    rx.recv_timeout(std::time::Duration::from_secs(20)).unwrap();\n    let mut count = 0;\n    for path in &cluster.paths {\n        let sst_dir = path.path().join(\"import-sst\");\n        for entry in std::fs::read_dir(sst_dir).unwrap() {\n            let entry = entry.unwrap();\n            if entry.file_type().unwrap().is_file() {\n                count += 1;\n            }\n        }\n    }\n    fail::remove(\"on_cleanup_import_sst\");\n    fail::remove(\"on_cleanup_import_sst_schedule\");\n    assert_ne!(0, count);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_prewrite_before_max_ts_is_synced", "test": "fn test_prewrite_before_max_ts_is_synced() {\n    let mut cluster = new_server_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.run();\n\n    // Transfer leader to node 1 first to ensure all operations happen on node 1\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k2\");\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k3\");\n\n    let addr = cluster.sim.rl().get_addr(1);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env).connect(&addr);\n    let client = TikvClient::new(channel);\n\n    let do_prewrite = |cluster: &mut Cluster<ServerCluster>| {\n        let region_id = right.get_id();\n        let leader = cluster.leader_of_region(region_id).unwrap();\n        let epoch = cluster.get_region_epoch(region_id);\n        let mut ctx = Context::default();\n        ctx.set_region_id(region_id);\n        ctx.set_peer(leader);\n        ctx.set_region_epoch(epoch);\n\n        let mut req = PrewriteRequest::default();\n        req.set_context(ctx);\n        req.set_primary_lock(b\"key\".to_vec());\n        let mut mutation = Mutation::default();\n        mutation.set_op(Op::Put);\n        mutation.set_key(b\"key\".to_vec());\n        mutation.set_value(b\"value\".to_vec());\n        req.mut_mutations().push(mutation);\n        req.set_start_version(100);\n        req.set_lock_ttl(20000);\n        req.set_use_async_commit(true);\n        client.kv_prewrite(&req).unwrap()\n    };\n\n    fail::cfg(\"test_raftstore_get_tso\", \"return(50)\").unwrap();\n    cluster.pd_client.must_merge(left.get_id(), right.get_id());\n    let resp = do_prewrite(&mut cluster);\n    assert!(resp.get_region_error().has_max_timestamp_not_synced());\n    fail::remove(\"test_raftstore_get_tso\");\n    thread::sleep(Duration::from_millis(200));\n    let resp = do_prewrite(&mut cluster);\n    assert!(!resp.get_region_error().has_max_timestamp_not_synced());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_read_execution_tracker.rs::test_read_execution_tracking", "test": "fn test_read_execution_tracking() {\n    let (_cluster, client, ctx) = must_new_cluster_and_kv_client();\n    let (k1, v1) = (b\"k1\".to_vec(), b\"v1\".to_vec());\n    let (k2, v2) = (b\"k2\".to_vec(), b\"v2\".to_vec());\n\n    // write entries\n    let mut mutation1 = Mutation::default();\n    mutation1.set_op(Op::Put);\n    mutation1.set_key(k1.clone());\n    mutation1.set_value(v1);\n\n    let mut mutation2 = Mutation::default();\n    mutation2.set_op(Op::Put);\n    mutation2.set_key(k2.clone());\n    mutation2.set_value(v2);\n\n    must_kv_prewrite(\n        &client,\n        ctx.clone(),\n        vec![mutation1, mutation2],\n        k1.clone(),\n        10,\n    );\n    must_kv_commit(\n        &client,\n        ctx.clone(),\n        vec![k1.clone(), k2.clone()],\n        10,\n        30,\n        30,\n    );\n\n    let lease_read_checker = |scan_detail: &ScanDetailV2| {\n        assert!(\n            scan_detail.get_read_index_propose_wait_nanos() == 0,\n            \"resp lease read propose wait time={:?}\",\n            scan_detail.get_read_index_propose_wait_nanos()\n        );\n\n        assert!(\n            scan_detail.get_read_index_confirm_wait_nanos() == 0,\n            \"resp lease read confirm wait time={:?}\",\n            scan_detail.get_read_index_confirm_wait_nanos()\n        );\n\n        assert!(\n            scan_detail.get_read_pool_schedule_wait_nanos() > 0,\n            \"resp read pool scheduling wait time={:?}\",\n            scan_detail.get_read_pool_schedule_wait_nanos()\n        );\n    };\n\n    fail::cfg(\"perform_read_local\", \"return()\").unwrap();\n\n    // should perform lease read\n    let resp = kv_read(&client, ctx.clone(), k1.clone(), 100);\n\n    lease_read_checker(resp.get_exec_details_v2().get_scan_detail_v2());\n\n    // should perform lease read\n    let resp = kv_batch_read(&client, ctx.clone(), vec![k1.clone(), k2.clone()], 100);\n\n    lease_read_checker(resp.get_exec_details_v2().get_scan_detail_v2());\n\n    let product = ProductTable::new();\n    init_with_data(&product, &[(1, Some(\"name:0\"), 2)]);\n    let mut coprocessor_request = DagSelect::from(&product).build();\n    coprocessor_request.set_context(ctx.clone());\n    coprocessor_request.set_start_ts(100);\n\n    // should perform lease read\n    let resp = client.coprocessor(&coprocessor_request).unwrap();\n\n    lease_read_checker(resp.get_exec_details_v2().get_scan_detail_v2());\n\n    fail::remove(\"perform_read_local\");\n\n    let read_index_checker = |scan_detail: &ScanDetailV2| {\n        assert!(\n            scan_detail.get_read_index_propose_wait_nanos() > 0,\n            \"resp lease read propose wait time={:?}\",\n            scan_detail.get_read_index_propose_wait_nanos()\n        );\n\n        assert!(\n            scan_detail.get_read_index_confirm_wait_nanos() > 0,\n            \"resp lease read confirm wait time={:?}\",\n            scan_detail.get_read_index_confirm_wait_nanos()\n        );\n\n        assert!(\n            scan_detail.get_read_pool_schedule_wait_nanos() > 0,\n            \"resp read pool scheduling wait time={:?}\",\n            scan_detail.get_read_pool_schedule_wait_nanos()\n        );\n    };\n\n    fail::cfg(\"perform_read_index\", \"return()\").unwrap();\n\n    // should perform read index\n    let resp = kv_read(&client, ctx.clone(), k1.clone(), 100);\n\n    read_index_checker(resp.get_exec_details_v2().get_scan_detail_v2());\n\n    // should perform read index\n    let resp = kv_batch_read(&client, ctx, vec![k1, k2], 100);\n\n    read_index_checker(resp.get_exec_details_v2().get_scan_detail_v2());\n\n    // should perform read index\n    let resp = client.coprocessor(&coprocessor_request).unwrap();\n\n    read_index_checker(resp.get_exec_details_v2().get_scan_detail_v2());\n\n    fail::remove(\"perform_read_index\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_stale_read_basic_flow_replicate", "test": "fn test_stale_read_basic_flow_replicate() {\n    let (mut cluster, pd_client, mut leader_client) = prepare_for_stale_read(new_peer(1, 1));\n    let mut follower_client2 = PeerClient::new(&cluster, 1, new_peer(2, 2));\n    // Set the `stale_read` flag\n    leader_client.ctx.set_stale_read(true);\n    follower_client2.ctx.set_stale_read(true);\n\n    let commit_ts1 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Can read `value1` with the newest ts\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n\n    // Stop replicate data to follower 2\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend),\n    ));\n\n    // Update `key1`\n    let commit_ts2 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value2\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Follower 2 can still read `value1`, but can not read `value2` due\n    // to it don't have enough data\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), commit_ts1);\n    let resp1 = follower_client2.kv_read(b\"key1\".to_vec(), commit_ts2);\n    assert!(resp1.get_region_error().has_data_is_not_ready());\n\n    // Leader have up to date data so it can read `value2`\n    leader_client.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), get_tso(&pd_client));\n\n    // clear the `MsgAppend` filter\n    cluster.clear_send_filters();\n\n    // Now we can read `value2` with the newest ts\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), get_tso(&pd_client));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_stale_read_1pc_flow_replicate", "test": "fn test_stale_read_1pc_flow_replicate() {\n    let (mut cluster, pd_client, mut leader_client) = prepare_for_stale_read(new_peer(1, 1));\n    let mut follower_client2 = PeerClient::new(&cluster, 1, new_peer(2, 2));\n    // Set the `stale_read` flag\n    leader_client.ctx.set_stale_read(true);\n    follower_client2.ctx.set_stale_read(true);\n\n    let commit_ts1 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Can read `value1` with the newest ts\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n\n    // Stop replicate data to follower 2\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend),\n    ));\n    // Update `key1`\n    leader_client.must_kv_prewrite_one_pc(\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value2\"[..])],\n        b\"key1\".to_vec(),\n        get_tso(&pd_client),\n    );\n    let read_ts = get_tso(&pd_client);\n    // wait for advance_resolved_ts.\n    sleep_ms(200);\n    // Follower 2 can still read `value1`, but can not read `value2` due\n    // to it don't have enough data\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), commit_ts1);\n    let resp1 = follower_client2.kv_read(b\"key1\".to_vec(), read_ts);\n    assert!(resp1.get_region_error().has_data_is_not_ready());\n\n    // Leader have up to date data so it can read `value2`\n    leader_client.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), get_tso(&pd_client));\n\n    // clear the `MsgAppend` filter\n    cluster.clear_send_filters();\n\n    // Now we can read `value2` with the newest ts\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), get_tso(&pd_client));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_stale_read_basic_flow_lock", "test": "fn test_stale_read_basic_flow_lock() {\n    let (cluster, pd_client, leader_client) = prepare_for_stale_read(new_peer(1, 1));\n    let mut follower_client2 = PeerClient::new(&cluster, 1, new_peer(2, 2));\n    follower_client2.ctx.set_stale_read(true);\n\n    // Write `(key1, value1)`\n    let commit_ts1 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Prewrite on `key2` but not commit yet\n    let k2_prewrite_ts = get_tso(&pd_client);\n    leader_client.must_kv_prewrite(\n        vec![new_mutation(Op::Put, &b\"key2\"[..], &b\"value1\"[..])],\n        b\"key2\".to_vec(),\n        k2_prewrite_ts,\n    );\n    // Update `key1`\n    let commit_ts2 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value2\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Assert `(key1, value2)` can't be read with `commit_ts2` due to it's larger\n    // than the `start_ts` of `key2`.\n    let resp = follower_client2.kv_read(b\"key1\".to_vec(), commit_ts2);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n    // Still can read `(key1, value1)` since `commit_ts1` is less than the `key2`\n    // lock's `start_ts`\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), commit_ts1);\n\n    // Prewrite on `key3` but not commit yet\n    let k3_prewrite_ts = get_tso(&pd_client);\n    leader_client.must_kv_prewrite(\n        vec![new_mutation(Op::Put, &b\"key3\"[..], &b\"value1\"[..])],\n        b\"key3\".to_vec(),\n        k3_prewrite_ts,\n    );\n    // Commit on `key2`\n    let k2_commit_ts = get_tso(&pd_client);\n    leader_client.must_kv_commit(vec![b\"key2\".to_vec()], k2_prewrite_ts, k2_commit_ts);\n\n    // Although there is still lock on the region, but the min lock is refreshed\n    // to the `key3`'s lock, now we can read `(key1, value2)` but not `(key2,\n    // value1)`\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), commit_ts2);\n    let resp = follower_client2.kv_read(b\"key2\".to_vec(), k2_commit_ts);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    // Commit on `key3`\n    let k3_commit_ts = get_tso(&pd_client);\n    leader_client.must_kv_commit(vec![b\"key3\".to_vec()], k3_prewrite_ts, k3_commit_ts);\n\n    // Now there is not lock on the region, we can read any\n    // up to date data\n    follower_client2.must_kv_read_equal(b\"key2\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n    follower_client2.must_kv_read_equal(b\"key3\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_update_resoved_ts_before_apply_index", "test": "fn test_update_resoved_ts_before_apply_index() {\n    let (mut cluster, pd_client, mut leader_client) = prepare_for_stale_read(new_peer(1, 1));\n    let mut follower_client2 = PeerClient::new(&cluster, 1, new_peer(2, 2));\n    leader_client.ctx.set_stale_read(true);\n    follower_client2.ctx.set_stale_read(true);\n\n    // Write `(key1, value1)`\n    let commit_ts1 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), commit_ts1);\n\n    // Return before handling `apply_res`, to stop the leader updating the apply\n    // index\n    let on_apply_res_fp = \"on_apply_res\";\n    fail::cfg(on_apply_res_fp, \"return()\").unwrap();\n    // Stop replicate data to follower 2\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend),\n    ));\n\n    // Write `(key1, value2)`\n    let commit_ts2 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value2\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Wait `resolved_ts` be updated\n    sleep_ms(100);\n\n    // The leader can't handle stale read with `commit_ts2` because its `safe_ts`\n    // can't update due to its `apply_index` not update\n    let resp = leader_client.kv_read(b\"key1\".to_vec(), commit_ts2);\n    assert!(resp.get_region_error().has_data_is_not_ready(),);\n    // The follower can't handle stale read with `commit_ts2` because it don't\n    // have enough data\n    let resp = follower_client2.kv_read(b\"key1\".to_vec(), commit_ts2);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    fail::remove(on_apply_res_fp);\n    cluster.clear_send_filters();\n\n    leader_client.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), commit_ts2);\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value2\".to_vec(), commit_ts2);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_new_leader_init_resolver", "test": "fn test_new_leader_init_resolver() {\n    let (mut cluster, pd_client, mut peer_client1) = prepare_for_stale_read(new_peer(1, 1));\n    let mut peer_client2 = PeerClient::new(&cluster, 1, new_peer(2, 2));\n    peer_client1.ctx.set_stale_read(true);\n    peer_client2.ctx.set_stale_read(true);\n\n    // Write `(key1, value1)`\n    let commit_ts1 = peer_client1.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // There are no lock in the region, the `safe_ts` should keep updating by the\n    // new leader, so we can read `key1` with the newest ts\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n    peer_client1.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n\n    // Prewrite on `key2` but not commit yet\n    peer_client2.must_kv_prewrite(\n        vec![new_mutation(Op::Put, &b\"key2\"[..], &b\"value1\"[..])],\n        b\"key2\".to_vec(),\n        get_tso(&pd_client),\n    );\n\n    // There are locks in the region, the `safe_ts` can't be updated, so we can't\n    // read `key1` with the newest ts\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    let resp = peer_client2.kv_read(b\"key1\".to_vec(), get_tso(&pd_client));\n    assert!(resp.get_region_error().has_data_is_not_ready());\n    // But we can read `key1` with `commit_ts1`\n    peer_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), commit_ts1);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_stale_read_while_applying_snapshot", "test": "fn test_stale_read_while_applying_snapshot() {\n    let (mut cluster, pd_client, leader_client) =\n        prepare_for_stale_read_before_run(new_peer(1, 1), Some(Box::new(configure_for_snapshot)));\n    let mut follower_client2 = PeerClient::new(&cluster, 1, new_peer(2, 2));\n    follower_client2.ctx.set_stale_read(true);\n\n    let k1_commit_ts = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), k1_commit_ts);\n\n    // Stop replicate data to follower 2\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n\n    // Prewrite on `key3` but not commit yet\n    let k2_prewrite_ts = get_tso(&pd_client);\n    leader_client.must_kv_prewrite(\n        vec![new_mutation(Op::Put, &b\"key2\"[..], &b\"value1\"[..])],\n        b\"key2\".to_vec(),\n        k2_prewrite_ts,\n    );\n\n    // Compact logs to force requesting snapshot after clearing send filters.\n    let gc_limit = cluster.cfg.raft_store.raft_log_gc_count_limit();\n    for i in 1..gc_limit * 2 {\n        let (k, v) = (\n            format!(\"k{}\", i).into_bytes(),\n            format!(\"v{}\", i).into_bytes(),\n        );\n        leader_client.must_kv_write(&pd_client, vec![new_mutation(Op::Put, &k, &v)], k);\n    }\n    let last_index_on_store_2 = cluster.raft_local_state(1, 2).last_index;\n    cluster.wait_log_truncated(1, 1, last_index_on_store_2 + 1);\n\n    // Pasuse before applying snapshot is finish\n    let raft_before_applying_snap_finished = \"raft_before_applying_snap_finished\";\n    fail::cfg(raft_before_applying_snap_finished, \"pause\").unwrap();\n    cluster.clear_send_filters();\n\n    // Wait follower 2 start applying snapshot\n    cluster.wait_log_truncated(1, 2, last_index_on_store_2 + 1);\n    sleep_ms(100);\n\n    // We can't read while applying snapshot and the `safe_ts` should reset to 0\n    let resp = follower_client2.kv_read(b\"key1\".to_vec(), k1_commit_ts);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n    assert_eq!(\n        0,\n        resp.get_region_error()\n            .get_data_is_not_ready()\n            .get_safe_ts()\n    );\n\n    // Resume applying snapshot\n    fail::remove(raft_before_applying_snap_finished);\n\n    let last_index_on_store_1 = cluster.raft_local_state(1, 1).last_index;\n    cluster.wait_last_index(1, 2, last_index_on_store_1, Duration::from_secs(3));\n\n    // We can read `key1` after applied snapshot\n    follower_client2.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), k1_commit_ts);\n    // There is still lock on the region, we can't read `key1` with the newest ts\n    let resp = follower_client2.kv_read(b\"key1\".to_vec(), get_tso(&pd_client));\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    // Commit `key2`\n    leader_client.must_kv_commit(vec![b\"key2\".to_vec()], k2_prewrite_ts, get_tso(&pd_client));\n    // We can read `key1` with the newest ts now\n    follower_client2.must_kv_read_equal(b\"key2\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_stale_read_while_region_merge", "test": "fn test_stale_read_while_region_merge() {\n    let (mut cluster, pd_client, _) =\n        prepare_for_stale_read_before_run(new_peer(1, 1), Some(Box::new(configure_for_merge)));\n\n    cluster.must_split(&cluster.get_region(&[]), b\"key3\");\n    let source = pd_client.get_region(b\"key1\").unwrap();\n    let target = pd_client.get_region(b\"key5\").unwrap();\n\n    cluster.must_transfer_leader(target.get_id(), new_peer(1, 1));\n    let target_leader = PeerClient::new(&cluster, target.get_id(), new_peer(1, 1));\n    // Write `(key5, value1)`\n    target_leader.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key5\"[..], &b\"value1\"[..])],\n        b\"key5\".to_vec(),\n    );\n\n    let source_leader = cluster.leader_of_region(source.get_id()).unwrap();\n    let source_leader = PeerClient::new(&cluster, source.get_id(), source_leader);\n    // Prewrite on `key1` but not commit yet\n    let k1_prewrite_ts = get_tso(&pd_client);\n    source_leader.must_kv_prewrite(\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n        k1_prewrite_ts,\n    );\n\n    // Write `(key5, value2)`\n    let k5_commit_ts = target_leader.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key5\"[..], &b\"value2\"[..])],\n        b\"key5\".to_vec(),\n    );\n\n    // Merge source region into target region, the lock on source region should also\n    // merge into the target region and cause the target region's `safe_ts`\n    // decrease\n    pd_client.must_merge(source.get_id(), target.get_id());\n\n    let mut follower_client2 = PeerClient::new(&cluster, target.get_id(), new_peer(2, 2));\n    follower_client2.ctx.set_stale_read(true);\n    // We can read `(key5, value1)` with `k1_prewrite_ts`\n    follower_client2.must_kv_read_equal(b\"key5\".to_vec(), b\"value1\".to_vec(), k1_prewrite_ts);\n    // Can't read `key5` with `k5_commit_ts` because `k1_prewrite_ts` is smaller\n    // than `k5_commit_ts`\n    let resp = follower_client2.kv_read(b\"key5\".to_vec(), k5_commit_ts);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    let target_leader = PeerClient::new(&cluster, target.get_id(), new_peer(1, 1));\n    // Commit on `key1`\n    target_leader.must_kv_commit(vec![b\"key1\".to_vec()], k1_prewrite_ts, get_tso(&pd_client));\n    // We can read `(key5, value2)` now\n    follower_client2.must_kv_read_equal(b\"key5\".to_vec(), b\"value2\".to_vec(), get_tso(&pd_client));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_read_source_region_after_target_region_merged", "test": "fn test_read_source_region_after_target_region_merged() {\n    let (mut cluster, pd_client, leader_client) =\n        prepare_for_stale_read_before_run(new_peer(1, 1), Some(Box::new(configure_for_merge)));\n\n    // Write on source region\n    let k1_commit_ts1 = leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    cluster.must_split(&cluster.get_region(&[]), b\"key3\");\n    let source = pd_client.get_region(b\"key1\").unwrap();\n    let target = pd_client.get_region(b\"key5\").unwrap();\n    // Transfer the target region leader to store 1 and the source region leader to\n    // store 2\n    cluster.must_transfer_leader(target.get_id(), new_peer(1, 1));\n    cluster.must_transfer_leader(source.get_id(), find_peer(&source, 2).unwrap().clone());\n    // Get the source region follower on store 3\n    let mut source_follower_client3 = PeerClient::new(\n        &cluster,\n        source.get_id(),\n        find_peer(&source, 3).unwrap().clone(),\n    );\n    source_follower_client3.ctx.set_stale_read(true);\n    source_follower_client3.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), k1_commit_ts1);\n\n    // Pause on source region `prepare_merge` on store 2 and store 3\n    let apply_before_prepare_merge_2_3 = \"apply_before_prepare_merge_2_3\";\n    fail::cfg(apply_before_prepare_merge_2_3, \"pause\").unwrap();\n\n    // Merge source region into target region\n    pd_client.must_merge(source.get_id(), target.get_id());\n\n    // Leave a lock on the original source region key range through the target\n    // region leader\n    let target_leader = PeerClient::new(&cluster, target.get_id(), new_peer(1, 1));\n    let k1_prewrite_ts2 = get_tso(&pd_client);\n    target_leader.must_kv_prewrite(\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value2\"[..])],\n        b\"key1\".to_vec(),\n        k1_prewrite_ts2,\n    );\n\n    // Wait for the source region leader to update `safe_ts` (if it can)\n    sleep_ms(50);\n\n    // We still can read `key1` with `k1_commit_ts1` through source region\n    source_follower_client3.must_kv_read_equal(b\"key1\".to_vec(), b\"value1\".to_vec(), k1_commit_ts1);\n    // But can't read `key2` with `k1_prewrite_ts2` because the source leader can't\n    // update `safe_ts` after source region is merged into target region even\n    // though the source leader didn't know the merge is complement\n    let resp = source_follower_client3.kv_read(b\"key1\".to_vec(), k1_prewrite_ts2);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    fail::remove(apply_before_prepare_merge_2_3);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_stale_read.rs::test_stale_read_future_ts_not_update_max_ts", "test": "fn test_stale_read_future_ts_not_update_max_ts() {\n    let (_cluster, pd_client, mut leader_client) = prepare_for_stale_read(new_peer(1, 1));\n    leader_client.ctx.set_stale_read(true);\n\n    // Write `(key1, value1)`\n    leader_client.must_kv_write(\n        &pd_client,\n        vec![new_mutation(Op::Put, &b\"key1\"[..], &b\"value1\"[..])],\n        b\"key1\".to_vec(),\n    );\n\n    // Perform stale read with a future ts should return error\n    let read_ts = get_tso(&pd_client) + 10000000;\n    let resp = leader_client.kv_read(b\"key1\".to_vec(), read_ts);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    // The `max_ts` should not updated by the stale read request, so we can prewrite\n    // and commit `async_commit` transaction with a ts that smaller than the\n    // `read_ts`\n    let prewrite_ts = get_tso(&pd_client);\n    assert!(prewrite_ts < read_ts);\n    leader_client.must_kv_prewrite_async_commit(\n        vec![new_mutation(Op::Put, &b\"key2\"[..], &b\"value1\"[..])],\n        b\"key2\".to_vec(),\n        prewrite_ts,\n    );\n    let commit_ts = get_tso(&pd_client);\n    assert!(commit_ts < read_ts);\n    leader_client.must_kv_commit(vec![b\"key2\".to_vec()], prewrite_ts, commit_ts);\n    leader_client.must_kv_read_equal(b\"key2\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n\n    // Perform stale read with a future ts should return error\n    let read_ts = get_tso(&pd_client) + 10000000;\n    let resp = leader_client.kv_read(b\"key1\".to_vec(), read_ts);\n    assert!(resp.get_region_error().has_data_is_not_ready());\n\n    // The `max_ts` should not updated by the stale read request, so 1pc transaction\n    // with a ts that smaller than the `read_ts` should not be fallbacked to 2pc\n    let prewrite_ts = get_tso(&pd_client);\n    assert!(prewrite_ts < read_ts);\n    leader_client.must_kv_prewrite_one_pc(\n        vec![new_mutation(Op::Put, &b\"key3\"[..], &b\"value1\"[..])],\n        b\"key3\".to_vec(),\n        prewrite_ts,\n    );\n    // `key3` is write as 1pc transaction so we can read `key3` without commit\n    leader_client.must_kv_read_equal(b\"key3\".to_vec(), b\"value1\".to_vec(), get_tso(&pd_client));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_sst_recovery.rs::test_sst_recovery_overlap_range_sst_exist", "test": "fn test_sst_recovery_overlap_range_sst_exist() {\n    let (mut cluster, pd_client, engine1) = create_tikv_cluster_with_one_node_damaged();\n\n    // create a new sst [1,7] flushed to L0.\n    cluster.must_put_cf(CF_DEFAULT, b\"1\", b\"val_1\");\n    cluster.must_put_cf(CF_DEFAULT, b\"3\", b\"val_1\");\n    cluster.must_put_cf(CF_DEFAULT, b\"4\", b\"val_1\");\n    cluster.must_put_cf(CF_DEFAULT, b\"5\", b\"val_1\");\n    cluster.must_put_cf(CF_DEFAULT, b\"7\", b\"val_1\");\n    cluster.flush_data();\n\n    let files = engine1.as_inner().get_live_files();\n    assert_eq!(files.get_files_count(), 4);\n\n    // Remove peers for safe deletion of files in sst recovery.\n    let region = cluster.get_region(b\"2\");\n    let peer = find_peer(&region, 1).unwrap();\n    pd_client.must_remove_peer(region.id, peer.clone());\n    let region = cluster.get_region(b\"4\");\n    let peer = find_peer(&region, 1).unwrap();\n    pd_client.must_remove_peer(region.id, peer.clone());\n\n    // Peer has been removed from store 1 so it won't get this replica.\n    cluster.must_put_cf(CF_DEFAULT, b\"4\", b\"val_2\");\n\n    std::thread::sleep(CHECK_DURATION);\n    must_get_equal(&engine1, b\"1\", b\"val_1\");\n    must_get_equal(&engine1, b\"4\", b\"val_1\");\n    must_get_equal(&engine1, b\"7\", b\"val_1\");\n\n    // Validate the damaged sst has been deleted.\n    compact_files_to_target_level(&engine1, true, 3).unwrap();\n    let files = engine1.as_inner().get_live_files();\n    assert_eq!(files.get_files_count(), 1);\n\n    must_get_equal(&engine1, b\"4\", b\"val_1\");\n    assert_eq!(cluster.must_get(b\"4\").unwrap(), b\"val_2\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_stale_read.rs::test_read_index_when_transfer_leader_2", "test": "fn test_read_index_when_transfer_leader_2() {\n    let mut cluster = new_node_cluster(0, 3);\n\n    // Increase the election tick to make this test case running reliably.\n    configure_for_lease_read(&mut cluster.cfg, Some(50), Some(10_000));\n    // Stop log compaction to transfer leader with filter easier.\n    configure_for_request_snapshot(&mut cluster);\n    let max_lease = Duration::from_secs(2);\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration(max_lease);\n\n    // Add peer 2 and 3 and wait them to apply it.\n    cluster.pd_client.disable_default_operator();\n    let r1 = cluster.run_conf_change();\n    cluster.must_put(b\"k0\", b\"v0\");\n    cluster.pd_client.must_add_peer(r1, new_peer(2, 2));\n    cluster.pd_client.must_add_peer(r1, new_peer(3, 3));\n    must_get_equal(&cluster.get_engine(2), b\"k0\", b\"v0\");\n    must_get_equal(&cluster.get_engine(3), b\"k0\", b\"v0\");\n\n    // Put and test again to ensure that peer 3 get the latest writes by message\n    // append instead of snapshot, so that transfer leader to peer 3 can 100%\n    // success.\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    let r1 = cluster.get_region(b\"k1\");\n    let old_leader = cluster.leader_of_region(r1.get_id()).unwrap();\n\n    // Use a macro instead of a closure to avoid any capture of local variables.\n    macro_rules! read_on_old_leader {\n        () => {{\n            let (tx, rx) = mpsc::sync_channel(1);\n            let mut read_request = new_request(\n                r1.get_id(),\n                r1.get_region_epoch().clone(),\n                vec![new_get_cmd(b\"k1\")],\n                true, // read quorum\n            );\n            read_request.mut_header().set_peer(new_peer(1, 1));\n            let sim = cluster.sim.wl();\n            sim.async_command_on_node(\n                old_leader.get_id(),\n                read_request,\n                Callback::read(Box::new(move |resp| tx.send(resp.response).unwrap())),\n            )\n            .unwrap();\n            rx\n        }};\n    }\n\n    // Delay all raft messages to peer 1.\n    let dropped_msgs = Arc::new(Mutex::new(Vec::new()));\n    let filter = Box::new(\n        RegionPacketFilter::new(r1.get_id(), old_leader.get_store_id())\n            .direction(Direction::Recv)\n            .skip(MessageType::MsgTransferLeader)\n            .when(Arc::new(AtomicBool::new(true)))\n            .reserve_dropped(Arc::clone(&dropped_msgs)),\n    );\n    cluster\n        .sim\n        .wl()\n        .add_recv_filter(old_leader.get_id(), filter);\n\n    let resp1 = read_on_old_leader!();\n\n    cluster.must_transfer_leader(r1.get_id(), new_peer(3, 3));\n\n    let resp2 = read_on_old_leader!();\n\n    // Unpark all pending messages and clear all filters.\n    let router = cluster.sim.wl().get_router(old_leader.get_id()).unwrap();\n    let mut reserved_msgs = Vec::new();\n    'LOOP: loop {\n        for raft_msg in std::mem::take(&mut *dropped_msgs.lock().unwrap()) {\n            let msg_type = raft_msg.get_message().get_msg_type();\n            if msg_type == MessageType::MsgHeartbeatResponse || msg_type == MessageType::MsgAppend {\n                reserved_msgs.push(raft_msg);\n                if msg_type == MessageType::MsgAppend {\n                    break 'LOOP;\n                }\n            }\n        }\n    }\n\n    // Resume reserved messages in one batch to make sure the old leader can get\n    // read and role change in one `Ready`.\n    fail::cfg(\"pause_on_peer_collect_message\", \"pause\").unwrap();\n    for raft_msg in reserved_msgs {\n        router.send_raft_message(raft_msg).unwrap();\n    }\n    fail::cfg(\"pause_on_peer_collect_message\", \"off\").unwrap();\n    cluster.sim.wl().clear_recv_filters(old_leader.get_id());\n\n    let resp1 = resp1.recv().unwrap();\n    assert!(resp1.get_header().get_error().has_stale_command());\n\n    // Response 2 should contains an error.\n    let resp2 = resp2.recv().unwrap();\n    assert!(resp2.get_header().get_error().has_stale_command());\n    drop(cluster);\n    fail::remove(\"pause_on_peer_collect_message\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_scheduler_leader_change_twice", "test": "fn test_scheduler_leader_change_twice() {\n    let snapshot_fp = \"scheduler_async_snapshot_finish\";\n    let mut cluster = new_server_cluster(0, 2);\n    cluster.run();\n    let region0 = cluster.get_region(b\"\");\n    let peers = region0.get_peers();\n    cluster.must_transfer_leader(region0.get_id(), peers[0].clone());\n    let engine0 = cluster.sim.rl().storages[&peers[0].get_id()].clone();\n    let storage0 =\n        TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine0, MockLockManager::new())\n            .build()\n            .unwrap();\n\n    let mut ctx0 = Context::default();\n    ctx0.set_region_id(region0.get_id());\n    ctx0.set_region_epoch(region0.get_region_epoch().clone());\n    ctx0.set_peer(peers[0].clone());\n    let (prewrite_tx, prewrite_rx) = channel();\n    fail::cfg(snapshot_fp, \"pause\").unwrap();\n    storage0\n        .sched_txn_command(\n            commands::Prewrite::new(\n                vec![Mutation::make_put(Key::from_raw(b\"k\"), b\"v\".to_vec())],\n                b\"k\".to_vec(),\n                10.into(),\n                0,\n                false,\n                0,\n                TimeStamp::default(),\n                TimeStamp::default(),\n                None,\n                false,\n                AssertionLevel::Off,\n                ctx0,\n            ),\n            Box::new(move |res: storage::Result<_>| {\n                prewrite_tx.send(res).unwrap();\n            }),\n        )\n        .unwrap();\n    // Sleep to make sure the failpoint is triggered.\n    thread::sleep(Duration::from_millis(2000));\n    // Transfer leader twice, then unblock snapshot.\n    cluster.must_transfer_leader(region0.get_id(), peers[1].clone());\n    cluster.must_transfer_leader(region0.get_id(), peers[0].clone());\n    fail::remove(snapshot_fp);\n\n    match prewrite_rx.recv_timeout(Duration::from_secs(5)).unwrap() {\n        Err(Error(box ErrorInner::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n            box KvErrorInner::Request(ref e),\n        ))))))\n        | Err(Error(box ErrorInner::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n            box MvccErrorInner::Kv(KvError(box KvErrorInner::Request(ref e))),\n        ))))))\n        | Err(Error(box ErrorInner::Kv(KvError(box KvErrorInner::Request(ref e))))) => {\n            assert!(e.has_stale_command(), \"{:?}\", e);\n        }\n        res => {\n            panic!(\"expect stale command, but got {:?}\", res);\n        }\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_atomic_getting_max_ts_and_storing_memory_lock", "test": "fn test_atomic_getting_max_ts_and_storing_memory_lock() {\n    let engine = TestEngineBuilder::new().build().unwrap();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .build()\n        .unwrap();\n\n    let (prewrite_tx, prewrite_rx) = channel();\n    let (fp_tx, fp_rx) = sync_channel(1);\n    // sleep a while between getting max ts and store the lock in memory\n    fail::cfg_callback(\"before-set-lock-in-memory\", move || {\n        fp_tx.send(()).unwrap();\n        thread::sleep(Duration::from_millis(200));\n    })\n    .unwrap();\n    storage\n        .sched_txn_command(\n            commands::Prewrite::new(\n                vec![Mutation::make_put(Key::from_raw(b\"k\"), b\"v\".to_vec())],\n                b\"k\".to_vec(),\n                40.into(),\n                20000,\n                false,\n                1,\n                TimeStamp::default(),\n                TimeStamp::default(),\n                Some(vec![]),\n                false,\n                AssertionLevel::Off,\n                Context::default(),\n            ),\n            Box::new(move |res| {\n                prewrite_tx.send(res).unwrap();\n            }),\n        )\n        .unwrap();\n    fp_rx.recv().unwrap();\n    match block_on(storage.get(Context::default(), Key::from_raw(b\"k\"), 100.into())) {\n        // In this case, min_commit_ts is smaller than the start ts, but the lock is visible\n        // to the get.\n        Err(storage::Error(box storage::ErrorInner::Txn(txn::Error(\n            box txn::ErrorInner::Mvcc(mvcc::Error(box mvcc::ErrorInner::KeyIsLocked(lock))),\n        )))) => {\n            assert_eq!(lock.get_min_commit_ts(), 41);\n        }\n        res => panic!(\"unexpected result: {:?}\", res),\n    }\n    let res = prewrite_rx.recv().unwrap().unwrap();\n    assert_eq!(res.min_commit_ts, 41.into());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_update_max_ts_before_scan_memory_locks", "test": "fn test_update_max_ts_before_scan_memory_locks() {\n    let engine = TestEngineBuilder::new().build().unwrap();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .build()\n        .unwrap();\n\n    fail::cfg(\"before-storage-check-memory-locks\", \"sleep(500)\").unwrap();\n    let get_fut = storage.get(Context::default(), Key::from_raw(b\"k\"), 100.into());\n\n    thread::sleep(Duration::from_millis(200));\n\n    let (prewrite_tx, prewrite_rx) = channel();\n    storage\n        .sched_txn_command(\n            commands::Prewrite::new(\n                vec![Mutation::make_put(Key::from_raw(b\"k\"), b\"v\".to_vec())],\n                b\"k\".to_vec(),\n                10.into(),\n                20000,\n                false,\n                1,\n                TimeStamp::default(),\n                TimeStamp::default(),\n                Some(vec![]),\n                false,\n                AssertionLevel::Off,\n                Context::default(),\n            ),\n            Box::new(move |res| {\n                prewrite_tx.send(res).unwrap();\n            }),\n        )\n        .unwrap();\n\n    // The prewritten lock is not seen by the reader\n    assert_eq!(block_on(get_fut).unwrap().0, None);\n    // But we make sure in this case min_commit_ts is greater than start_ts.\n    let res = prewrite_rx.recv().unwrap().unwrap();\n    assert_eq!(res.min_commit_ts, 101.into());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_pessimistic_lock_check_epoch", "test": "fn test_pessimistic_lock_check_epoch() {\n    let mut cluster = new_server_cluster(0, 2);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    let region = cluster.get_region(b\"\");\n    let leader = region.get_peers()[0].clone();\n\n    let epoch = cluster.get_region_epoch(region.id);\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.id);\n    ctx.set_peer(leader.clone());\n    ctx.set_region_epoch(epoch);\n\n    let (fp_tx, fp_rx) = sync_channel(0);\n    fail::cfg_callback(\"acquire_pessimistic_lock\", move || {\n        fp_tx.send(()).unwrap();\n    })\n    .unwrap();\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n\n    let mut mutation = pb::Mutation::default();\n    mutation.set_op(Op::PessimisticLock);\n    mutation.key = b\"key\".to_vec();\n    let mut req = PessimisticLockRequest::default();\n    req.set_context(ctx.clone());\n    req.set_mutations(vec![mutation].into());\n    req.set_start_version(10);\n    req.set_for_update_ts(10);\n    req.set_primary_lock(b\"key\".to_vec());\n\n    let lock_resp = thread::spawn(move || client.kv_pessimistic_lock(&req).unwrap());\n    thread::sleep(Duration::from_millis(300));\n\n    // Transfer leader out and back, so the term should have changed.\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    fp_rx.recv().unwrap();\n\n    let resp = lock_resp.join().unwrap();\n    // Region leader changes, so we should get a StaleCommand error.\n    assert!(resp.get_region_error().has_stale_command());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_concurrent_write_after_transfer_leader_invalidates_locks", "test": "fn test_concurrent_write_after_transfer_leader_invalidates_locks() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    let txn_ext = cluster\n        .must_get_snapshot_of_region(1)\n        .ext()\n        .get_txn_ext()\n        .unwrap()\n        .clone();\n\n    let lock = PessimisticLock {\n        primary: b\"key\".to_vec().into_boxed_slice(),\n        start_ts: 10.into(),\n        ttl: 3000,\n        for_update_ts: 20.into(),\n        min_commit_ts: 30.into(),\n        last_change_ts: 5.into(),\n        versions_to_last_change: 3,\n    };\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(Key::from_raw(b\"key\"), lock.clone())])\n        .unwrap();\n\n    let region = cluster.get_region(b\"\");\n    let leader = region.get_peers()[0].clone();\n    fail::cfg(\"invalidate_locks_before_transfer_leader\", \"pause\").unwrap();\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n\n    let mut mutation = pb::Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.key = b\"key\".to_vec();\n    let mut req = PrewriteRequest::default();\n    req.set_context(ctx);\n    req.set_mutations(vec![mutation].into());\n    // Set a different start_ts. It should fail because the memory lock is still\n    // visible.\n    req.set_start_version(20);\n    req.set_primary_lock(b\"key\".to_vec());\n\n    // Prewrite should not be blocked because we have downgrade the write lock\n    // to a read lock, and it should return a locked error because it encounters\n    // the memory lock.\n    let resp = client.kv_prewrite(&req).unwrap();\n    assert_eq!(\n        resp.get_errors()[0].get_locked(),\n        &lock.into_lock().into_lock_info(b\"key\".to_vec())\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_prewrite_before_max_ts_is_synced", "test": "fn test_prewrite_before_max_ts_is_synced() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_heartbeat_ticks = 20;\n    cluster.run();\n\n    let addr = cluster.sim.rl().get_addr(1);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env).connect(&addr);\n    let client = TikvClient::new(channel);\n\n    let do_prewrite = |cluster: &mut Cluster<ServerCluster>| {\n        let region_id = 1;\n        let leader = cluster.leader_of_region(region_id).unwrap();\n        let epoch = cluster.get_region_epoch(region_id);\n        let mut ctx = Context::default();\n        ctx.set_region_id(region_id);\n        ctx.set_peer(leader);\n        ctx.set_region_epoch(epoch);\n\n        let mut req = PrewriteRequest::default();\n        req.set_context(ctx);\n        req.set_primary_lock(b\"key\".to_vec());\n        let mut mutation = Mutation::default();\n        mutation.set_op(Op::Put);\n        mutation.set_key(b\"key\".to_vec());\n        mutation.set_value(b\"value\".to_vec());\n        req.mut_mutations().push(mutation);\n        req.set_start_version(100);\n        req.set_lock_ttl(20000);\n        req.set_use_async_commit(true);\n        client.kv_prewrite(&req).unwrap()\n    };\n\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n    fail::cfg(\"test_raftstore_get_tso\", \"return(50)\").unwrap();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    let resp = do_prewrite(&mut cluster);\n    assert!(resp.get_region_error().has_max_timestamp_not_synced());\n    fail::remove(\"test_raftstore_get_tso\");\n    thread::sleep(Duration::from_millis(200));\n    let resp = do_prewrite(&mut cluster);\n    assert!(!resp.get_region_error().has_max_timestamp_not_synced());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_read_lock_after_become_follower", "test": "fn test_read_lock_after_become_follower() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_heartbeat_ticks = 20;\n    cluster.run();\n\n    let region_id = 1;\n    cluster.must_transfer_leader(1, new_peer(3, 3));\n\n    let start_ts = block_on(cluster.pd_client.get_tso()).unwrap();\n\n    // put kv after get start ts, then this commit will cause a\n    // PessimisticLockNotFound if the pessimistic lock get missing.\n    cluster.must_put(b\"key\", b\"value\");\n\n    let leader = cluster.leader_of_region(region_id).unwrap();\n    let snapshot = cluster.must_get_snapshot_of_region(region_id);\n    let txn_ext = snapshot.txn_ext.unwrap();\n    let for_update_ts = block_on(cluster.pd_client.get_tso()).unwrap();\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(\n            Key::from_raw(b\"key\"),\n            PessimisticLock {\n                primary: b\"key\".to_vec().into_boxed_slice(),\n                start_ts,\n                ttl: 1000,\n                for_update_ts,\n                min_commit_ts: for_update_ts,\n                last_change_ts: start_ts.prev(),\n                versions_to_last_change: 1,\n            },\n        )])\n        .unwrap();\n\n    let addr = cluster.sim.rl().get_addr(3);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env).connect(&addr);\n    let client = TikvClient::new(channel);\n\n    let mut req = PrewriteRequest::default();\n    let mut ctx = Context::default();\n    ctx.set_region_id(region_id);\n    ctx.set_region_epoch(cluster.get_region_epoch(region_id));\n    ctx.set_peer(leader);\n    req.set_context(ctx);\n    req.set_primary_lock(b\"key\".to_vec());\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.set_key(b\"key\".to_vec());\n    mutation.set_value(b\"value2\".to_vec());\n    req.mut_mutations().push(mutation);\n    req.set_start_version(start_ts.into_inner());\n    req.set_lock_ttl(20000);\n\n    // Pause the command before it executes prewrite.\n    fail::cfg(\"txn_before_process_write\", \"pause\").unwrap();\n    let (tx, resp_rx) = mpsc::channel();\n    thread::spawn(move || tx.send(client.kv_prewrite(&req).unwrap()).unwrap());\n\n    thread::sleep(Duration::from_millis(200));\n    resp_rx.try_recv().unwrap_err();\n\n    // And pause applying the write on the leader.\n    fail::cfg(\"on_apply_write_cmd\", \"pause\").unwrap();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    thread::sleep(Duration::from_millis(200));\n\n    // Transfer leader will not make the command fail.\n    fail::remove(\"txn_before_process_write\");\n    let resp = resp_rx.recv().unwrap();\n    // The term has changed, so we should get a stale command error instead a\n    // PessimisticLockNotFound.\n    assert!(resp.get_region_error().has_stale_command());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/backup/mod.rs::test_invalid_external_storage", "test": "fn test_invalid_external_storage() {\n    let mut suite = TestSuite::new(1, 144 * 1024 * 1024, ApiVersion::V1);\n    // Put some data.\n    suite.must_kv_put(3, 1);\n\n    // Set backup directory read-only. TiKV fails to backup.\n    let tmp = Builder::new().tempdir().unwrap();\n    let f = File::open(tmp.path()).unwrap();\n    let mut perms = f.metadata().unwrap().permissions();\n    perms.set_readonly(true);\n    f.set_permissions(perms.clone()).unwrap();\n\n    let backup_ts = suite.alloc_ts();\n    let storage_path = tmp.path();\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        storage_path,\n    );\n\n    // Wait util the backup request is handled.\n    let resps = block_on(rx.collect::<Vec<_>>());\n    assert!(resps[0].has_error());\n\n    perms.set_readonly(false);\n    f.set_permissions(perms).unwrap();\n\n    suite.stop();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/backup/mod.rs::test_backup_in_flashback", "test": "fn test_backup_in_flashback() {\n    let mut suite = TestSuite::new(3, 144 * 1024 * 1024, ApiVersion::V1);\n    suite.must_kv_put(3, 1);\n    // Prepare the flashback.\n    let region = suite.cluster.get_region(b\"key_0\");\n    suite.cluster.must_send_wait_flashback_msg(\n        region.get_id(),\n        kvproto::raft_cmdpb::AdminCmdType::PrepareFlashback,\n    );\n    // Start the backup.\n    let tmp = Builder::new().tempdir().unwrap();\n    let backup_ts = suite.alloc_ts();\n    let storage_path = make_unique_dir(tmp.path());\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        &storage_path,\n    );\n    let resp = block_on(rx.collect::<Vec<_>>());\n    assert!(!resp[0].has_error());\n    // Finish the flashback.\n    suite.cluster.must_send_wait_flashback_msg(\n        region.get_id(),\n        kvproto::raft_cmdpb::AdminCmdType::FinishFlashback,\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/mod.rs::test_serde_default_config", "test": "fn test_serde_default_config() {\n    let cfg: TikvConfig = toml::from_str(\"\").unwrap();\n    assert_eq!(cfg, TikvConfig::default());\n\n    let content = read_file_in_project_dir(\"integrations/config/test-default.toml\");\n    let cfg: TikvConfig = toml::from_str(&content).unwrap();\n    assert_eq!(cfg, TikvConfig::default());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/mod.rs::test_log_backward_compatible", "test": "fn test_log_backward_compatible() {\n    let content = read_file_in_project_dir(\"integrations/config/test-log-compatible.toml\");\n    let mut cfg: TikvConfig = toml::from_str(&content).unwrap();\n    assert_eq!(cfg.log.level, slog::Level::Info.into());\n    assert_eq!(cfg.log.file.filename, \"\");\n    assert_eq!(cfg.log.format, LogFormat::Text);\n    assert_eq!(cfg.log.file.max_size, 300);\n    cfg.logger_compatible_adjust();\n    assert_eq!(cfg.log.level, slog::Level::Critical.into());\n    assert_eq!(cfg.log.file.filename, \"foo\");\n    assert_eq!(cfg.log.format, LogFormat::Json);\n    assert_eq!(cfg.log.file.max_size, 1024);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/dynamic/gc_worker.rs::test_gc_worker_config_update", "test": "fn test_gc_worker_config_update() {\n    let (mut cfg, _dir) = TikvConfig::with_tmp().unwrap();\n    cfg.validate().unwrap();\n    let (gc_worker, cfg_controller) = setup_cfg_controller(cfg);\n    let scheduler = gc_worker.scheduler();\n\n    // update of other module's config should not effect gc worker config\n    cfg_controller\n        .update_config(\"raftstore.raft-log-gc-threshold\", \"2000\")\n        .unwrap();\n    validate(&scheduler, move |cfg: &GcConfig, _| {\n        assert_eq!(cfg, &GcConfig::default());\n    });\n\n    // Update gc worker config\n    let change = {\n        let mut change = std::collections::HashMap::new();\n        change.insert(\"gc.ratio-threshold\".to_owned(), \"1.23\".to_owned());\n        change.insert(\"gc.batch-keys\".to_owned(), \"1234\".to_owned());\n        change.insert(\"gc.max-write-bytes-per-sec\".to_owned(), \"1KB\".to_owned());\n        change.insert(\"gc.enable-compaction-filter\".to_owned(), \"true\".to_owned());\n        change\n    };\n    cfg_controller.update(change).unwrap();\n    validate(&scheduler, move |cfg: &GcConfig, _| {\n        assert_eq!(cfg.ratio_threshold, 1.23);\n        assert_eq!(cfg.batch_keys, 1234);\n        assert_eq!(cfg.max_write_bytes_per_sec, ReadableSize::kb(1));\n        assert!(cfg.enable_compaction_filter);\n    });\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/dynamic/raftstore.rs::test_update_raftstore_io_config", "test": "fn test_update_raftstore_io_config() {\n    // Test update raftstore configurations on io settings.\n    // Start from SYNC mode.\n    {\n        let (mut resize_config, _dir) = TikvConfig::with_tmp().unwrap();\n        resize_config.validate().unwrap();\n        let (cfg_controller, _, _, mut system) = start_raftstore(resize_config, &_dir);\n\n        // not allowed to resize from SYNC mode to ASYNC mode\n        let resize_store_writers_cfg = vec![(\"raftstore.store-io-pool-size\", \"2\")];\n        assert!(\n            cfg_controller\n                .update(new_changes(resize_store_writers_cfg))\n                .is_err()\n        );\n        system.shutdown();\n    }\n    // Start from ASYNC mode.\n    {\n        let (mut resize_config, _dir) = TikvConfig::with_tmp().unwrap();\n        resize_config.raft_store.store_io_pool_size = 2;\n        resize_config.validate().unwrap();\n        let (cfg_controller, _, _, mut system) = start_raftstore(resize_config, &_dir);\n\n        // not allowed to resize from ASYNC mode to SYNC mode\n        let resize_store_writers_cfg = vec![(\"raftstore.store-io-pool-size\", \"0\")];\n        assert!(\n            cfg_controller\n                .update(new_changes(resize_store_writers_cfg))\n                .is_err()\n        );\n        system.shutdown();\n    }\n    // Modify the size of async-ios.\n    {\n        let (mut resize_config, _dir) = TikvConfig::with_tmp().unwrap();\n        resize_config.raft_store.store_io_pool_size = 2;\n        resize_config.validate().unwrap();\n        let (cfg_controller, _, _, mut system) = start_raftstore(resize_config, &_dir);\n\n        // resize the count of ios to 1 by decreasing.\n        let resize_store_writers_cfg = vec![(\"raftstore.store-io-pool-size\", \"1\")];\n        cfg_controller\n            .update(new_changes(resize_store_writers_cfg))\n            .unwrap();\n        // resize the count of ios to 4 by increasing.\n        let resize_store_writers_cfg = vec![(\"raftstore.store-io-pool-size\", \"4\")];\n        cfg_controller\n            .update(new_changes(resize_store_writers_cfg))\n            .unwrap();\n        system.shutdown();\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_scan_detail", "test": "fn test_scan_detail() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = {\n        let engine = TestEngineBuilder::new().build().unwrap();\n        let mut cfg = Config::default();\n        cfg.end_point_batch_row_limit = 50;\n        init_data_with_details(Context::default(), engine, &product, &data, true, &cfg)\n    };\n\n    let reqs = vec![\n        DagSelect::from(&product).build(),\n        DagSelect::from_index(&product, &product[\"name\"]).build(),\n    ];\n\n    for mut req in reqs {\n        req.mut_context().set_record_scan_stat(true);\n        req.mut_context().set_record_time_stat(true);\n\n        let resp = handle_request(&endpoint, req);\n        assert!(resp.get_exec_details().has_time_detail());\n        let scan_detail = resp.get_exec_details().get_scan_detail();\n        // Values would occur in data cf are inlined in write cf.\n        assert_eq!(scan_detail.get_write().get_total(), 5);\n        assert_eq!(scan_detail.get_write().get_processed(), 4);\n        assert_eq!(scan_detail.get_lock().get_total(), 1);\n\n        assert!(resp.get_exec_details_v2().has_time_detail());\n        assert!(resp.get_exec_details_v2().has_time_detail_v2());\n        let scan_detail_v2 = resp.get_exec_details_v2().get_scan_detail_v2();\n        assert_eq!(scan_detail_v2.get_total_versions(), 5);\n        assert_eq!(scan_detail_v2.get_processed_versions(), 4);\n        assert!(scan_detail_v2.get_processed_versions_size() > 0);\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_handle_truncate", "test": "fn test_handle_truncate() {\n    use tidb_query_datatype::{FieldTypeAccessor, FieldTypeTp};\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &data);\n    let cols = product.columns_info();\n    let cases = vec![\n        {\n            // count > \"2x\"\n            let mut col = Expr::default();\n            col.set_tp(ExprType::ColumnRef);\n            col.mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            let count_offset = offset_for_column(&cols, product[\"count\"].id);\n            col.mut_val().encode_i64(count_offset).unwrap();\n\n            // \"2x\" will be truncated.\n            let mut value = Expr::default();\n            value\n                .mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::String);\n            value.set_tp(ExprType::String);\n            value.set_val(String::from(\"2x\").into_bytes());\n\n            let mut right = Expr::default();\n            right\n                .mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            right.set_tp(ExprType::ScalarFunc);\n            right.set_sig(ScalarFuncSig::CastStringAsInt);\n            right.mut_children().push(value);\n\n            let mut cond = Expr::default();\n            cond.mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            cond.set_tp(ExprType::ScalarFunc);\n            cond.set_sig(ScalarFuncSig::LtInt);\n            cond.mut_children().push(col);\n            cond.mut_children().push(right);\n            cond\n        },\n        {\n            // id\n            let mut col_id = Expr::default();\n            col_id\n                .mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            col_id.set_tp(ExprType::ColumnRef);\n            let id_offset = offset_for_column(&cols, product[\"id\"].id);\n            col_id.mut_val().encode_i64(id_offset).unwrap();\n\n            // \"3x\" will be truncated.\n            let mut value = Expr::default();\n            value\n                .mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::String);\n            value.set_tp(ExprType::String);\n            value.set_val(String::from(\"3x\").into_bytes());\n\n            let mut int_3 = Expr::default();\n            int_3\n                .mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            int_3.set_tp(ExprType::ScalarFunc);\n            int_3.set_sig(ScalarFuncSig::CastStringAsInt);\n            int_3.mut_children().push(value);\n\n            // count\n            let mut col_count = Expr::default();\n            col_count\n                .mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            col_count.set_tp(ExprType::ColumnRef);\n            let count_offset = offset_for_column(&cols, product[\"count\"].id);\n            col_count.mut_val().encode_i64(count_offset).unwrap();\n\n            // \"3x\" + count\n            let mut plus = Expr::default();\n            plus.mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            plus.set_tp(ExprType::ScalarFunc);\n            plus.set_sig(ScalarFuncSig::PlusInt);\n            plus.mut_children().push(int_3);\n            plus.mut_children().push(col_count);\n\n            // id = \"3x\" + count\n            let mut cond = Expr::default();\n            cond.mut_field_type()\n                .as_mut_accessor()\n                .set_tp(FieldTypeTp::LongLong);\n            cond.set_tp(ExprType::ScalarFunc);\n            cond.set_sig(ScalarFuncSig::EqInt);\n            cond.mut_children().push(col_id);\n            cond.mut_children().push(plus);\n            cond\n        },\n    ];\n\n    for cond in cases {\n        // Ignore truncate error.\n        let req = DagSelect::from(&product)\n            .where_expr(cond.clone())\n            .build_with(Context::default(), &[FLAG_IGNORE_TRUNCATE]);\n        let resp = handle_select(&endpoint, req);\n        assert!(!resp.has_error());\n        assert!(resp.get_warnings().is_empty());\n\n        // truncate as warning\n        let req = DagSelect::from(&product)\n            .where_expr(cond.clone())\n            .build_with(Context::default(), &[FLAG_TRUNCATE_AS_WARNING]);\n        let mut resp = handle_select(&endpoint, req);\n        assert!(!resp.has_error());\n        assert!(!resp.get_warnings().is_empty());\n        // check data\n        let mut spliter = DagChunkSpliter::new(resp.take_chunks().into(), 3);\n        let row = spliter.next().unwrap();\n        let (id, name, cnt) = data[2];\n        let name_datum = name.map(|s| s.as_bytes()).into();\n        let expected_encoded = datum::encode_value(\n            &mut EvalContext::default(),\n            &[Datum::I64(id), name_datum, cnt.into()],\n        )\n        .unwrap();\n        let result_encoded = datum::encode_value(&mut EvalContext::default(), &row).unwrap();\n        assert_eq!(&*result_encoded, &*expected_encoded);\n        assert_eq!(spliter.next().is_none(), true);\n\n        // Do NOT ignore truncate error.\n        let req = DagSelect::from(&product).where_expr(cond.clone()).build();\n        let resp = handle_select(&endpoint, req);\n        assert!(resp.has_error());\n        assert!(resp.get_warnings().is_empty());\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_output_counts", "test": "fn test_output_counts() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &data);\n\n    let req = DagSelect::from(&product).build();\n    let resp = handle_select(&endpoint, req);\n    assert_eq!(resp.get_output_counts(), &[data.len() as i64]);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_exec_details", "test": "fn test_exec_details() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &data);\n\n    let flags = &[0];\n\n    let ctx = Context::default();\n    let req = DagSelect::from(&product).build_with(ctx, flags);\n    let resp = handle_request(&endpoint, req);\n    assert!(resp.has_exec_details());\n    let exec_details = resp.get_exec_details();\n    assert!(exec_details.has_time_detail());\n    assert!(exec_details.has_scan_detail());\n    assert!(resp.has_exec_details_v2());\n    let exec_details = resp.get_exec_details_v2();\n    assert!(exec_details.has_time_detail());\n    assert!(exec_details.has_time_detail_v2());\n    assert!(exec_details.has_scan_detail_v2());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_snapshot_failed", "test": "fn test_snapshot_failed() {\n    let product = ProductTable::new();\n    let (_cluster, raft_engine, ctx) = new_raft_engine(1, \"\");\n\n    let (_, endpoint, _) = init_data_with_engine_and_commit(ctx, raft_engine, &product, &[], true);\n\n    // Use an invalid context to make errors.\n    let req = DagSelect::from(&product).build_with(Context::default(), &[0]);\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_region_error().has_store_not_match());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_cache", "test": "fn test_cache() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_cluster, raft_engine, ctx) = new_raft_engine(1, \"\");\n\n    let (_, endpoint, _) =\n        init_data_with_engine_and_commit(ctx.clone(), raft_engine, &product, &data, true);\n\n    let req = DagSelect::from(&product).build_with(ctx, &[0]);\n    let resp = handle_request(&endpoint, req.clone());\n\n    assert!(!resp.get_is_cache_hit());\n    let cache_version = resp.get_cache_last_version();\n\n    // Cache version must be >= 5 because Raft apply index must be >= 5.\n    assert!(cache_version >= 5);\n\n    // Send the request again using is_cache_enabled == false (default) and a\n    // matching version. The request should be processed as usual.\n\n    let mut req2 = req.clone();\n    req2.set_cache_if_match_version(cache_version);\n    let resp2 = handle_request(&endpoint, req2);\n\n    assert!(!resp2.get_is_cache_hit());\n    assert_eq!(\n        resp.get_cache_last_version(),\n        resp2.get_cache_last_version()\n    );\n    assert_eq!(resp.get_data(), resp2.get_data());\n\n    // Send the request again using is_cached_enabled == true and a matching\n    // version. The request should be skipped.\n\n    let mut req3 = req.clone();\n    req3.set_is_cache_enabled(true);\n    req3.set_cache_if_match_version(cache_version);\n    let resp3 = handle_request(&endpoint, req3);\n\n    assert!(resp3.get_is_cache_hit());\n    assert!(resp3.get_data().is_empty());\n\n    // Send the request using a non-matching version. The request should be\n    // processed.\n\n    let mut req4 = req;\n    req4.set_is_cache_enabled(true);\n    req4.set_cache_if_match_version(cache_version + 1);\n    let resp4 = handle_request(&endpoint, req4);\n\n    assert!(!resp4.get_is_cache_hit());\n    assert_eq!(\n        resp.get_cache_last_version(),\n        resp4.get_cache_last_version()\n    );\n    assert_eq!(resp.get_data(), resp4.get_data());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_buckets", "test": "fn test_buckets() {\n    let product = ProductTable::new();\n    let (mut cluster, raft_engine, ctx) = new_raft_engine(1, \"\");\n\n    let (_, endpoint, _) =\n        init_data_with_engine_and_commit(ctx.clone(), raft_engine, &product, &[], true);\n\n    let req = DagSelect::from(&product).build_with(ctx, &[0]);\n    let resp = handle_request(&endpoint, req.clone());\n    assert_eq!(resp.get_latest_buckets_version(), 0);\n\n    let mut bucket_key = product.get_record_range_all().get_start().to_owned();\n    bucket_key.push(0);\n    let region = cluster.get_region(&bucket_key);\n    let bucket = Bucket {\n        keys: vec![bucket_key],\n        size: 1024,\n    };\n    cluster.refresh_region_bucket_keys(&region, vec![bucket], None, None);\n\n    let wait_refresh_buckets = |old_buckets_ver| {\n        let mut resp = Default::default();\n        for _ in 0..10 {\n            resp = handle_request(&endpoint, req.clone());\n            if resp.get_latest_buckets_version() != old_buckets_ver {\n                break;\n            }\n            thread::sleep(Duration::from_millis(100));\n        }\n        assert_ne!(resp.get_latest_buckets_version(), old_buckets_ver);\n    };\n\n    wait_refresh_buckets(0);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_ingest_sst", "test": "fn test_ingest_sst() {\n    let mut cfg = TikvConfig::default();\n    cfg.server.grpc_concurrency = 1;\n    let (_cluster, ctx, _tikv, import) = open_cluster_and_tikv_import_client(Some(cfg));\n\n    let temp_dir = Builder::new().prefix(\"test_ingest_sst\").tempdir().unwrap();\n\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n\n    // No region id and epoch.\n    send_upload_sst(&import, &meta, &data).unwrap();\n\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta.clone());\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(resp.has_error());\n\n    // Set region id and epoch.\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n    send_upload_sst(&import, &meta, &data).unwrap();\n    // Can't upload the same file again.\n    assert_to_string_contains!(\n        send_upload_sst(&import, &meta, &data).unwrap_err(),\n        \"FileExists\"\n    );\n\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error(), \"{:?}\", resp.get_error());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_upload_and_ingest_with_tde", "test": "fn test_upload_and_ingest_with_tde() {\n    let (_tmp_dir, _cluster, ctx, tikv, import) = new_cluster_and_tikv_import_client_tde();\n\n    let temp_dir = Builder::new().prefix(\"test_ingest_sst\").tempdir().unwrap();\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n    send_upload_sst(&import, &meta, &data).unwrap();\n\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error(), \"{:?}\", resp.get_error());\n\n    check_ingested_kvs(&tikv, &ctx, sst_range);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_ingest_sst_without_crc32", "test": "fn test_ingest_sst_without_crc32() {\n    let (_cluster, ctx, tikv, import) = new_cluster_and_tikv_import_client();\n\n    let temp_dir = Builder::new()\n        .prefix(\"test_ingest_sst_without_crc32\")\n        .tempdir()\n        .unwrap();\n\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n    // Set crc32 == 0 and length != 0 still ingest success\n    send_upload_sst(&import, &meta, &data).unwrap();\n    meta.set_crc32(0);\n\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error(), \"{:?}\", resp.get_error());\n\n    // Check ingested kvs\n    check_ingested_kvs(&tikv, &ctx, sst_range);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_download_sst", "test": "fn test_download_sst() {\n    let (_cluster, ctx, tikv, import) = new_cluster_and_tikv_import_client();\n    let temp_dir = Builder::new()\n        .prefix(\"test_download_sst\")\n        .tempdir()\n        .unwrap();\n\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, _) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n    // Checks that downloading a non-existing storage returns error.\n    let mut download = DownloadRequest::default();\n    download.set_sst(meta.clone());\n    download.set_storage_backend(external_storage_export::make_local_backend(temp_dir.path()));\n    download.set_name(\"missing.sst\".to_owned());\n\n    let result = import.download(&download).unwrap();\n    assert!(\n        result.has_error(),\n        \"unexpected download reply: {:?}\",\n        result\n    );\n\n    // Checks that downloading an empty SST returns OK (but cannot be ingested)\n    download.set_name(\"test.sst\".to_owned());\n    download.mut_sst().mut_range().set_start(vec![sst_range.1]);\n    download\n        .mut_sst()\n        .mut_range()\n        .set_end(vec![sst_range.1 + 1]);\n    let result = import.download(&download).unwrap();\n    assert!(result.get_is_empty());\n\n    // Now perform a proper download.\n    download.mut_sst().mut_range().set_start(Vec::new());\n    download.mut_sst().mut_range().set_end(Vec::new());\n    let result = import.download(&download).unwrap();\n    assert!(!result.get_is_empty());\n    assert_eq!(result.get_range().get_start(), &[sst_range.0]);\n    assert_eq!(result.get_range().get_end(), &[sst_range.1 - 1]);\n\n    // Do an ingest and verify the result is correct.\n\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(!resp.has_error());\n\n    check_ingested_kvs(&tikv, &ctx, sst_range);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_ingest_sst_region_not_found", "test": "fn test_ingest_sst_region_not_found() {\n    let (_cluster, mut ctx_not_found, _, import) = new_cluster_and_tikv_import_client();\n\n    let temp_dir = Builder::new()\n        .prefix(\"test_ingest_sst_errors\")\n        .tempdir()\n        .unwrap();\n\n    ctx_not_found.set_region_id(1 << 31); // A large region id that must no exists.\n    let sst_path = temp_dir.path().join(\"test_split.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, _data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx_not_found.get_region_id());\n    meta.set_region_epoch(ctx_not_found.get_region_epoch().clone());\n\n    let mut ingest = IngestRequest::default();\n    ingest.set_context(ctx_not_found);\n    ingest.set_sst(meta);\n    let resp = import.ingest(&ingest).unwrap();\n    assert!(resp.get_error().has_region_not_found());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_ingest_multiple_sst", "test": "fn test_ingest_multiple_sst() {\n    let (_cluster, ctx, tikv, import) = new_cluster_and_tikv_import_client();\n\n    let temp_dir = Builder::new()\n        .prefix(\"test_ingest_multiple_sst\")\n        .tempdir()\n        .unwrap();\n\n    let sst_path = temp_dir.path().join(\"test.sst\");\n    let sst_range1 = (0, 100);\n    let (mut meta1, data1) = gen_sst_file(sst_path, sst_range1);\n    meta1.set_region_id(ctx.get_region_id());\n    meta1.set_region_epoch(ctx.get_region_epoch().clone());\n\n    let sst_path2 = temp_dir.path().join(\"write-test.sst\");\n    let sst_range2 = (100, 200);\n    let (mut meta2, data2) = gen_sst_file(sst_path2, sst_range2);\n    meta2.set_region_id(ctx.get_region_id());\n    meta2.set_region_epoch(ctx.get_region_epoch().clone());\n    meta2.set_cf_name(\"write\".to_owned());\n\n    send_upload_sst(&import, &meta1, &data1).unwrap();\n    send_upload_sst(&import, &meta2, &data2).unwrap();\n\n    let mut ingest = MultiIngestRequest::default();\n    ingest.set_context(ctx.clone());\n    ingest.mut_ssts().push(meta1);\n    ingest.mut_ssts().push(meta2);\n    let resp = import.multi_ingest(&ingest).unwrap();\n    assert!(!resp.has_error(), \"{:?}\", resp.get_error());\n\n    // Check ingested kvs\n    check_ingested_kvs(&tikv, &ctx, sst_range1);\n    check_ingested_kvs_cf(&tikv, &ctx, \"write\", sst_range2);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_duplicate_and_close", "test": "fn test_duplicate_and_close() {\n    let (_cluster, ctx, _, import) = new_cluster_and_tikv_import_client();\n    let mut req = SwitchModeRequest::default();\n    req.set_mode(SwitchMode::Import);\n    import.switch_mode(&req).unwrap();\n\n    let data_count: u64 = 4096;\n    for commit_ts in 0..4 {\n        let mut meta = new_sst_meta(0, 0);\n        meta.set_region_id(ctx.get_region_id());\n        meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n        let mut keys = vec![];\n        let mut values = vec![];\n        for i in 1000..data_count {\n            let key = i.to_string();\n            keys.push(key.as_bytes().to_vec());\n            values.push(key.as_bytes().to_vec());\n        }\n        let resp = send_write_sst(&import, &meta, keys, values, commit_ts).unwrap();\n        for m in resp.metas.into_iter() {\n            let mut ingest = IngestRequest::default();\n            ingest.set_context(ctx.clone());\n            ingest.set_sst(m.clone());\n            let resp = import.ingest(&ingest).unwrap();\n            assert!(!resp.has_error());\n        }\n    }\n\n    let mut duplicate = DuplicateDetectRequest::default();\n    duplicate.set_context(ctx);\n    duplicate.set_start_key((0_u64).to_string().as_bytes().to_vec());\n    let mut stream = import.duplicate_detect(&duplicate).unwrap();\n    let ret = block_on(async move {\n        let mut ret: Vec<KvPair> = vec![];\n        while let Some(resp) = stream.next().await {\n            match resp {\n                Ok(mut resp) => {\n                    if resp.has_key_error() || resp.has_region_error() {\n                        break;\n                    }\n                    let pairs = resp.take_pairs();\n                    ret.append(&mut pairs.into());\n                }\n                Err(e) => {\n                    println!(\"receive error: {:?}\", e);\n                    break;\n                }\n            }\n        }\n        ret\n    });\n    assert_eq!(ret.len(), (data_count - 1000) as usize * 4);\n    req.set_mode(SwitchMode::Normal);\n    import.switch_mode(&req).unwrap();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_validate_endpoints", "test": "fn test_validate_endpoints() {\n    let eps_count = 3;\n    let server = MockServer::with_case(eps_count, Arc::new(Split::new()));\n    let env = Arc::new(\n        EnvBuilder::new()\n            .cq_count(1)\n            .name_prefix(thd_name!(\"test-pd\"))\n            .build(),\n    );\n    let eps = server.bind_addrs();\n\n    let mgr = Arc::new(SecurityManager::new(&SecurityConfig::default()).unwrap());\n    let connector = PdConnector::new(env, mgr);\n    assert!(block_on(connector.validate_endpoints(&new_config(eps), true)).is_err());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_validate_endpoints_retry", "test": "fn test_validate_endpoints_retry() {\n    let eps_count = 3;\n    let server = MockServer::with_case(eps_count, Arc::new(Split::new()));\n    let env = Arc::new(\n        EnvBuilder::new()\n            .cq_count(1)\n            .name_prefix(thd_name!(\"test-pd\"))\n            .build(),\n    );\n    let mut eps = server.bind_addrs();\n    let mock_port = 65535;\n    eps.insert(0, (\"127.0.0.1\".to_string(), mock_port));\n    eps.pop();\n    let mgr = Arc::new(SecurityManager::new(&SecurityConfig::default()).unwrap());\n    let connector = PdConnector::new(env, mgr);\n    assert!(block_on(connector.validate_endpoints(&new_config(eps), true)).is_err());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_pd_client_ok_when_cluster_not_ready", "test": "fn test_pd_client_ok_when_cluster_not_ready() {\n    let pd_client_cluster_id_zero = \"cluster_id_is_not_ready\";\n    let server = MockServer::with_case(3, Arc::new(AlreadyBootstrapped));\n    let eps = server.bind_addrs();\n\n    let mut client = new_client_v2(eps, None);\n    fail::cfg(pd_client_cluster_id_zero, \"return()\").unwrap();\n    // wait 100ms to let client load member.\n    thread::sleep(Duration::from_millis(101));\n    assert_eq!(client.reconnect().is_err(), true);\n    fail::remove(pd_client_cluster_id_zero);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_validate_endpoints", "test": "fn test_validate_endpoints() {\n    let eps_count = 3;\n    let server = MockServer::with_case(eps_count, Arc::new(Split::new()));\n    let env = Arc::new(\n        EnvBuilder::new()\n            .cq_count(1)\n            .name_prefix(thd_name!(\"test-pd\"))\n            .build(),\n    );\n    let eps = server.bind_addrs();\n\n    let mgr = Arc::new(SecurityManager::new(&SecurityConfig::default()).unwrap());\n    let connector = PdConnector::new(env, mgr);\n    assert!(block_on(connector.validate_endpoints(&new_config(eps), false)).is_err());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_validate_endpoints_retry", "test": "fn test_validate_endpoints_retry() {\n    let eps_count = 3;\n    let server = MockServer::with_case(eps_count, Arc::new(Split::new()));\n    let env = Arc::new(\n        EnvBuilder::new()\n            .cq_count(1)\n            .name_prefix(thd_name!(\"test-pd\"))\n            .build(),\n    );\n    let mut eps = server.bind_addrs();\n    let mock_port = 65535;\n    eps.insert(0, (\"127.0.0.1\".to_string(), mock_port));\n    eps.pop();\n    let mgr = Arc::new(SecurityManager::new(&SecurityConfig::default()).unwrap());\n    let connector = PdConnector::new(env, mgr);\n    assert!(block_on(connector.validate_endpoints(&new_config(eps), false)).is_err());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_pd_client_ok_when_cluster_not_ready", "test": "fn test_pd_client_ok_when_cluster_not_ready() {\n    let pd_client_cluster_id_zero = \"cluster_id_is_not_ready\";\n    let server = MockServer::with_case(3, Arc::new(AlreadyBootstrapped));\n    let eps = server.bind_addrs();\n\n    let client = new_client(eps, None);\n    fail::cfg(pd_client_cluster_id_zero, \"return()\").unwrap();\n    // wait 100ms to let client load member.\n    thread::sleep(Duration::from_millis(101));\n    assert_eq!(client.reconnect().is_err(), true);\n    fail::remove(pd_client_cluster_id_zero);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_early_apply.rs::test_update_internal_apply_index", "test": "fn test_update_internal_apply_index() {\n    let mut cluster = new_node_cluster(0, 4);\n    cluster.pd_client.disable_default_operator();\n    // So compact log will not be triggered automatically.\n    configure_for_request_snapshot(&mut cluster);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(3, 3));\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    let filter = RegionPacketFilter::new(1, 3)\n        .msg_type(MessageType::MsgAppendResponse)\n        .direction(Direction::Recv);\n    cluster.add_send_filter(CloneFilterFactory(filter));\n    let last_index = cluster.raft_local_state(1, 1).get_last_index();\n    cluster.async_remove_peer(1, new_peer(4, 4)).unwrap();\n    cluster.async_put(b\"k2\", b\"v2\").unwrap();\n    let mut snaps = Vec::new();\n    for id in 1..3 {\n        cluster.wait_last_index(1, id, last_index + 2, Duration::from_secs(3));\n        snaps.push((id, cluster.get_raft_engine(id).dump_all_data(id)));\n    }\n    cluster.clear_send_filters();\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(2), b\"k2\", b\"v2\");\n\n    // Simulate data lost in raft cf.\n    for (id, mut batch) in snaps {\n        cluster.stop_node(id);\n        delete_old_data(&cluster.get_raft_engine(id), id);\n        cluster\n            .get_raft_engine(id)\n            .consume(&mut batch, true /* sync */)\n            .unwrap();\n        cluster.run_node(id).unwrap();\n    }\n\n    let region = cluster.get_region(b\"k1\");\n    // Issues a heartbeat to followers so they will re-commit the logs.\n    let resp = read_on_peer(\n        &mut cluster,\n        new_peer(3, 3),\n        region,\n        b\"k1\",\n        true,\n        Duration::from_secs(3),\n    )\n    .unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    cluster.stop_node(3);\n    cluster.must_put(b\"k3\", b\"v3\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_flashback.rs::test_flashback_for_local_read", "test": "fn test_flashback_for_local_read() {\n    let mut cluster = new_node_cluster(0, 3);\n    let election_timeout = configure_for_lease_read(&mut cluster.cfg, Some(50), None);\n    // Avoid triggering the log compaction in this test case.\n    cluster.cfg.raft_store.raft_log_gc_threshold = 100;\n    cluster.run();\n    cluster.must_put(TEST_KEY, TEST_VALUE);\n    let mut region = cluster.get_region(TEST_KEY);\n    let store_id = 3;\n    let peer = new_peer(store_id, 3);\n    cluster.must_transfer_leader(region.get_id(), peer);\n\n    // Check local read before prepare flashback\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    let last_index = state.get_last_index();\n    // Make sure the leader transfer procedure timeouts.\n    sleep(election_timeout * 2);\n    must_request_without_flashback_flag(&mut cluster, &mut region.clone(), new_get_cmd(TEST_KEY));\n    // Check the leader does a local read.\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    assert_eq!(state.get_last_index(), last_index);\n\n    // Prepare flashback.\n    cluster.must_send_wait_flashback_msg(region.get_id(), AdminCmdType::PrepareFlashback);\n    // Check the leader does a local read.\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    assert_eq!(state.get_last_index(), last_index + 1);\n    // Wait for apply_res to set leader lease.\n    sleep_ms(500);\n    // Read should fail.\n    must_get_flashback_in_progress_error(&mut cluster, &mut region.clone(), new_get_cmd(TEST_KEY));\n    // Wait for the leader's lease to expire to ensure that a renew lease interval\n    // has elapsed.\n    sleep(election_timeout * 2);\n    // Read should fail.\n    must_get_flashback_in_progress_error(&mut cluster, &mut region.clone(), new_get_cmd(TEST_KEY));\n    // Also check read by propose was blocked\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    assert_eq!(state.get_last_index(), last_index + 1);\n    // Finish flashback.\n    cluster.must_send_wait_flashback_msg(region.get_id(), AdminCmdType::FinishFlashback);\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    assert_eq!(state.get_last_index(), last_index + 2);\n\n    // Check local read after finish flashback\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    let last_index = state.get_last_index();\n    // Make sure the leader transfer procedure timeouts.\n    sleep(election_timeout * 2);\n    must_request_without_flashback_flag(&mut cluster, &mut region.clone(), new_get_cmd(TEST_KEY));\n    // Check the leader does a local read.\n    let state = cluster.raft_local_state(region.get_id(), store_id);\n    assert_eq!(state.get_last_index(), last_index);\n    // A local read with flashback flag will not be blocked since it won't have any\n    // side effects.\n    must_request_with_flashback_flag(&mut cluster, &mut region, new_get_cmd(TEST_KEY));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_lease_read.rs::test_read_index_stale_in_suspect_lease", "test": "fn test_read_index_stale_in_suspect_lease() {\n    let mut cluster = new_node_cluster(0, 3);\n\n    // Increase the election tick to make this test case running reliably.\n    configure_for_lease_read(&mut cluster.cfg, Some(50), Some(10_000));\n    let max_lease = Duration::from_secs(2);\n    // Stop log compaction to transfer leader with filter easier.\n    configure_for_request_snapshot(&mut cluster);\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration(max_lease);\n\n    cluster.pd_client.disable_default_operator();\n    let r1 = cluster.run_conf_change();\n    cluster.pd_client.must_add_peer(r1, new_peer(2, 2));\n    cluster.pd_client.must_add_peer(r1, new_peer(3, 3));\n\n    let r1 = cluster.get_region(b\"k1\");\n    // Put and test again to ensure that peer 3 get the latest writes by message\n    // append instead of snapshot, so that transfer leader to peer 3 can 100%\n    // success.\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(3), b\"k2\", b\"v2\");\n    // Ensure peer 3 is ready to become leader.\n    let resp_ch = async_read_on_peer(&mut cluster, new_peer(3, 3), r1.clone(), b\"k2\", true, true);\n    let resp = block_on_timeout(resp_ch, Duration::from_secs(3)).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    assert_eq!(\n        resp.get_responses()[0].get_get().get_value(),\n        b\"v2\",\n        \"{:?}\",\n        resp\n    );\n    let old_leader = cluster.leader_of_region(r1.get_id()).unwrap();\n\n    // Use a macro instead of a closure to avoid any capture of local variables.\n    macro_rules! read_on_old_leader {\n        () => {{\n            let (tx, rx) = mpsc::sync_channel(1);\n            let mut read_request = new_request(\n                r1.get_id(),\n                r1.get_region_epoch().clone(),\n                vec![new_get_cmd(b\"k1\")],\n                true, // read quorum\n            );\n            read_request.mut_header().set_peer(new_peer(1, 1));\n            let sim = cluster.sim.wl();\n            sim.async_command_on_node(\n                old_leader.get_id(),\n                read_request,\n                Callback::read(Box::new(move |resp| tx.send(resp.response).unwrap())),\n            )\n            .unwrap();\n            rx\n        }};\n    }\n\n    // Delay all raft messages to peer 1.\n    let dropped_msgs = Arc::new(Mutex::new(Vec::new()));\n    let filter = Box::new(\n        RegionPacketFilter::new(r1.id, old_leader.store_id)\n            .direction(Direction::Recv)\n            .skip(MessageType::MsgTransferLeader)\n            .reserve_dropped(Arc::clone(&dropped_msgs)),\n    );\n    cluster\n        .sim\n        .wl()\n        .add_recv_filter(old_leader.get_id(), filter);\n\n    let resp1 = read_on_old_leader!();\n\n    cluster.must_transfer_leader(r1.get_id(), new_peer(3, 3));\n\n    let resp2 = read_on_old_leader!();\n\n    // Unpark all pending messages and clear all filters.\n    let router = cluster.sim.wl().get_router(old_leader.get_id()).unwrap();\n    'LOOP: loop {\n        for raft_msg in mem::take::<Vec<_>>(dropped_msgs.lock().unwrap().as_mut()) {\n            let msg_type = raft_msg.get_message().get_msg_type();\n            if msg_type == MessageType::MsgHeartbeatResponse {\n                router.send_raft_message(raft_msg).unwrap();\n                continue;\n            }\n            cluster.sim.wl().clear_recv_filters(old_leader.get_id());\n            break 'LOOP;\n        }\n    }\n\n    let resp1 = resp1.recv().unwrap();\n    assert!(\n        resp1.get_header().get_error().has_stale_command()\n            || resp1.get_responses()[0].get_get().get_value() == b\"v1\"\n    );\n\n    // Response 2 should contains an error.\n    let resp2 = resp2.recv().unwrap();\n    assert!(resp2.get_header().get_error().has_stale_command());\n    drop(cluster);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_lease_read.rs::test_read_index_after_write", "test": "fn test_read_index_after_write() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_lease_read(&mut cluster.cfg, Some(50), Some(10));\n    let heartbeat_interval = cluster.cfg.raft_store.raft_heartbeat_interval();\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    let region_on_store1 = find_peer(&region, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), region_on_store1.clone());\n\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    // Add heartbeat msg filter to prevent the leader to reply the read index\n    // response.\n    let filter = Box::new(\n        RegionPacketFilter::new(region.get_id(), 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgHeartbeat),\n    );\n    cluster.sim.wl().add_recv_filter(2, filter);\n\n    let mut req = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_read_index_cmd()],\n        true,\n    );\n    req.mut_header()\n        .set_peer(new_peer(1, region_on_store1.get_id()));\n    // Don't care about the first one's read index\n    let (cb, _) = make_cb(&req);\n    cluster.sim.rl().async_command_on_node(1, req, cb).unwrap();\n\n    cluster.must_put(b\"k2\", b\"v2\");\n    let applied_index = cluster.apply_state(region.get_id(), 1).get_applied_index();\n\n    let mut req = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_read_index_cmd()],\n        true,\n    );\n    req.mut_header()\n        .set_peer(new_peer(1, region_on_store1.get_id()));\n    let (cb, mut rx) = make_cb(&req);\n    cluster.sim.rl().async_command_on_node(1, req, cb).unwrap();\n\n    cluster.sim.wl().clear_recv_filters(2);\n\n    let response = rx.recv_timeout(heartbeat_interval).unwrap();\n    assert!(\n        response.get_responses()[0]\n            .get_read_index()\n            .get_read_index()\n            >= applied_index\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_multi.rs::test_node_leader_change_with_log_overlap", "test": "fn test_node_leader_change_with_log_overlap() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;\n    // disable compact log to make test more stable.\n    cluster.cfg.raft_store.raft_log_gc_threshold = 1000;\n    // We use three peers([1, 2, 3]) for this test.\n    cluster.run();\n\n    sleep_ms(500);\n\n    // guarantee peer 1 is leader\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    // So peer 3 won't replicate any message of the region but still can vote.\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 3).msg_type(MessageType::MsgAppend),\n    ));\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    // peer 1 and peer 2 must have k1, but peer 3 must not.\n    for i in 1..3 {\n        let engine = cluster.get_engine(i);\n        must_get_equal(&engine, b\"k1\", b\"v1\");\n    }\n\n    let engine3 = cluster.get_engine(3);\n    must_get_none(&engine3, b\"k1\");\n\n    // now only peer 1 and peer 2 can step to leader.\n    // Make peer 1's msg won't be replicated,\n    // so the proposed entries won't be committed.\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1)\n            .msg_type(MessageType::MsgAppend)\n            .direction(Direction::Send),\n    ));\n    let put_msg = vec![new_put_cmd(b\"k2\", b\"v2\")];\n    let region = cluster.get_region(b\"\");\n    let mut put_req = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        put_msg,\n        false,\n    );\n    put_req.mut_header().set_peer(new_peer(1, 1));\n    let called = Arc::new(AtomicBool::new(false));\n    let called_ = Arc::clone(&called);\n    cluster\n        .sim\n        .rl()\n        .get_node_router(1)\n        .send_command(\n            put_req,\n            Callback::write(Box::new(move |resp: WriteResponse| {\n                called_.store(true, Ordering::SeqCst);\n                assert!(resp.response.get_header().has_error());\n                assert!(resp.response.get_header().get_error().has_stale_command());\n            })),\n            RaftCmdExtraOpts::default(),\n        )\n        .unwrap();\n\n    // Now let peer(1, 1) steps down. Can't use transfer leader here, because\n    // it still has pending proposed entries.\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1)\n            .msg_type(MessageType::MsgHeartbeat)\n            .direction(Direction::Send),\n    ));\n    // make sure k2 has not been committed.\n    must_get_none(&cluster.get_engine(1), b\"k2\");\n\n    // Here just use `must_transfer_leader` to wait for peer (2, 2) becomes leader.\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n\n    must_get_none(&cluster.get_engine(2), b\"k2\");\n\n    cluster.clear_send_filters();\n\n    for _ in 0..50 {\n        sleep_ms(100);\n        if called.load(Ordering::SeqCst) {\n            return;\n        }\n    }\n    panic!(\"callback has not been called after 5s.\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_region_heartbeat.rs::test_region_heartbeat_timestamp", "test": "fn test_region_heartbeat_timestamp() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n\n    // transfer leader to (2, 2) first to make address resolve happen early.\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n    let reported_ts = cluster.pd_client.get_region_last_report_ts(1).unwrap();\n    assert_ne!(reported_ts, PdInstant::zero());\n\n    sleep(Duration::from_millis(1000));\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    sleep(Duration::from_millis(1000));\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n    for _ in 0..100 {\n        sleep_ms(100);\n        let reported_ts_now = cluster.pd_client.get_region_last_report_ts(1).unwrap();\n        if reported_ts_now > reported_ts {\n            return;\n        }\n    }\n    panic!(\"reported ts should be updated\");\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_region_info_accessor.rs::test_node_cluster_region_info_accessor", "test": "fn test_node_cluster_region_info_accessor() {\n    let mut cluster = new_node_cluster(1, 3);\n    configure_for_merge(&mut cluster.cfg);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    // Create a RegionInfoAccessor on node 1\n    let (tx, rx) = channel();\n    cluster\n        .sim\n        .wl()\n        .post_create_coprocessor_host(Box::new(move |id, host| {\n            if id == 1 {\n                let c = RegionInfoAccessor::new(host);\n                tx.send(c).unwrap();\n            }\n        }));\n    cluster.run_conf_change();\n    let c = rx.recv().unwrap();\n    // We only created it on the node whose id == 1 so we shouldn't receive more\n    // than one item.\n    assert!(rx.try_recv().is_err());\n\n    test_region_info_accessor_impl(&mut cluster, &c);\n\n    drop(cluster);\n    c.stop();\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_replica_read.rs::test_read_hibernated_region", "test": "fn test_read_hibernated_region() {\n    let mut cluster = new_node_cluster(0, 3);\n    // Initialize the cluster.\n    configure_for_lease_read(&mut cluster.cfg, Some(100), Some(8));\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration(Duration::from_millis(1));\n    cluster.cfg.raft_store.check_leader_lease_interval = ReadableDuration::hours(10);\n    cluster.pd_client.disable_default_operator();\n    let r1 = cluster.run_conf_change();\n    let p2 = new_peer(2, 2);\n    cluster.pd_client.must_add_peer(r1, p2.clone());\n    let p3 = new_peer(3, 3);\n    cluster.pd_client.must_add_peer(r1, p3.clone());\n    cluster.must_put(b\"k0\", b\"v0\");\n    let region = cluster.get_region(b\"k0\");\n    cluster.must_transfer_leader(region.get_id(), p3);\n    // Make sure leader writes the data.\n    must_get_equal(&cluster.get_engine(3), b\"k0\", b\"v0\");\n    // Wait for region is hibernated.\n    thread::sleep(Duration::from_secs(1));\n    cluster.stop_node(2);\n    cluster.run_node(2).unwrap();\n\n    let store2_sent_msgs = Arc::new(Mutex::new(Vec::new()));\n    let filter = Box::new(\n        RegionPacketFilter::new(1, 2)\n            .direction(Direction::Send)\n            .reserve_dropped(Arc::clone(&store2_sent_msgs)),\n    );\n    cluster.sim.wl().add_send_filter(2, filter);\n    cluster.pd_client.trigger_leader_info_loss();\n    // This request will fail because no valid leader.\n    let resp1_ch = async_read_on_peer(&mut cluster, p2.clone(), region.clone(), b\"k1\", true, true);\n    let resp1 = block_on_timeout(resp1_ch, Duration::from_secs(5)).unwrap();\n    assert!(\n        resp1.get_header().get_error().has_not_leader(),\n        \"{:?}\",\n        resp1.get_header()\n    );\n    thread::sleep(Duration::from_millis(300));\n    cluster.sim.wl().clear_send_filters(2);\n    let mut has_extra_message = false;\n    for msg in std::mem::take(&mut *store2_sent_msgs.lock().unwrap()) {\n        let to_store = msg.get_to_peer().get_store_id();\n        assert_ne!(to_store, 0, \"{:?}\", msg);\n        if to_store == 3 && msg.has_extra_msg() {\n            has_extra_message = true;\n        }\n        let router = cluster.sim.wl().get_router(to_store).unwrap();\n        router.send_raft_message(msg).unwrap();\n    }\n    // Had a wakeup message from 2 to 3.\n    assert!(has_extra_message);\n    // Wait for the leader is woken up.\n    thread::sleep(Duration::from_millis(500));\n    let resp2_ch = async_read_on_peer(&mut cluster, p2, region, b\"k1\", true, true);\n    let resp2 = block_on_timeout(resp2_ch, Duration::from_secs(5)).unwrap();\n    assert!(!resp2.get_header().has_error(), \"{:?}\", resp2);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_replication_mode.rs::test_check_conf_change", "test": "fn test_check_conf_change() {\n    let mut cluster = prepare_cluster();\n    run_cluster(&mut cluster);\n    let pd_client = cluster.pd_client.clone();\n    pd_client.must_remove_peer(1, new_peer(2, 2));\n    must_get_none(&cluster.get_engine(2), b\"k1\");\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n    pd_client.must_add_peer(1, new_learner_peer(2, 4));\n    let region = cluster.get_region(b\"k1\");\n    // Peer 4 can be promoted as there will be enough quorum alive.\n    let cc = new_change_peer_request(ConfChangeType::AddNode, new_peer(2, 4));\n    let req = new_admin_request(region.get_id(), region.get_region_epoch(), cc);\n    let res = cluster\n        .call_command_on_leader(req, Duration::from_secs(3))\n        .unwrap();\n    assert!(!res.get_header().has_error(), \"{:?}\", res);\n    must_get_none(&cluster.get_engine(2), b\"k1\");\n    cluster.clear_send_filters();\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v0\");\n\n    pd_client.must_remove_peer(1, new_peer(3, 3));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    pd_client.must_add_peer(1, new_learner_peer(3, 5));\n    let region = cluster.get_region(b\"k1\");\n    // Peer 5 can not be promoted as there is no enough quorum alive.\n    let cc = new_change_peer_request(ConfChangeType::AddNode, new_peer(3, 5));\n    let req = new_admin_request(region.get_id(), region.get_region_epoch(), cc);\n    let res = cluster\n        .call_command_on_leader(req, Duration::from_secs(3))\n        .unwrap();\n    assert!(\n        res.get_header()\n            .get_error()\n            .get_message()\n            .contains(\"promoted commit index\"),\n        \"{:?}\",\n        res\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_status_command.rs::test_region_detail", "test": "fn test_region_detail() {\n    let count = 5;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.run();\n\n    let leader = cluster.leader_of_region(1).unwrap();\n    let region_detail = cluster.region_detail(1, 1);\n    assert!(region_detail.has_region());\n    let region = region_detail.get_region();\n    assert_eq!(region.get_id(), 1);\n    assert!(region.get_start_key().is_empty());\n    assert!(region.get_end_key().is_empty());\n    assert_eq!(region.get_peers().len(), 5);\n    let epoch = region.get_region_epoch();\n    assert_eq!(epoch.get_conf_ver(), 1);\n    assert_eq!(epoch.get_version(), 1);\n\n    assert!(region_detail.has_leader());\n    assert_eq!(region_detail.get_leader(), &leader);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_trigger_snapshot", "test": "fn test_force_leader_trigger_snapshot() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(10);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 10;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(90);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(8);\n    cluster.cfg.raft_store.merge_max_log_gap = 3;\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(10);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store1 = find_peer(&region, 1).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // Isolate node 2\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n\n    // Compact logs to force requesting snapshot after clearing send filters.\n    let state = cluster.truncated_state(region.get_id(), 1);\n    // Write some data to trigger snapshot.\n    for i in 100..150 {\n        let key = format!(\"k{}\", i);\n        let value = format!(\"v{}\", i);\n        cluster.must_put(key.as_bytes(), value.as_bytes());\n    }\n    cluster.wait_log_truncated(region.get_id(), 1, state.get_index() + 40);\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    // Recover the isolation of 2, but still don't permit snapshot\n    let recv_filter = Box::new(\n        RegionPacketFilter::new(region.get_id(), 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgSnapshot),\n    );\n    cluster.sim.wl().add_recv_filter(2, recv_filter);\n    cluster.clear_send_filters();\n\n    // wait election timeout\n    sleep_ms(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 5,\n    );\n    cluster.enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n\n    sleep_ms(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 3,\n    );\n    let cmd = new_change_peer_request(\n        ConfChangeType::RemoveNode,\n        find_peer(&region, 3).unwrap().clone(),\n    );\n    let req = new_admin_request(region.get_id(), region.get_region_epoch(), cmd);\n    // Though it has a force leader now, but the command can't committed because the\n    // log is not replicated to all the alive peers.\n    assert!(\n        cluster\n            .call_command_on_leader(req, Duration::from_millis(1000))\n            .unwrap()\n            .get_header()\n            .has_error() /* error \"there is a pending conf change\" indicating no committed log\n                          * after being the leader */\n    );\n\n    // Permit snapshot message, snapshot should be applied and advance commit index\n    // now.\n    cluster.sim.wl().clear_recv_filters(2);\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), None);\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n    cluster.must_transfer_leader(region.get_id(), find_peer(&region, 1).unwrap().clone());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_v1_v2_mixed.rs::test_v1_receive_snap_from_v2", "test": "fn test_v1_receive_snap_from_v2() {\n    let test_receive_snap = |key_num| {\n        let mut cluster_v1 = test_raftstore::new_server_cluster(1, 1);\n        let mut cluster_v2 = test_raftstore_v2::new_server_cluster(1, 1);\n        let mut cluster_v1_tikv = test_raftstore::new_server_cluster(1, 1);\n\n        cluster_v1.cfg.raft_store.enable_v2_compatible_learner = true;\n\n        cluster_v1.run();\n        cluster_v2.run();\n        cluster_v1_tikv.run();\n\n        let s1_addr = cluster_v1.get_addr(1);\n        let s2_addr = cluster_v1_tikv.get_addr(1);\n        let region = cluster_v2.get_region(b\"\");\n        let region_id = region.get_id();\n        let engine = cluster_v2.get_engine(1);\n        let tablet = engine.get_tablet_by_id(region_id).unwrap();\n\n        for i in 0..key_num {\n            let k = format!(\"zk{:04}\", i);\n            tablet.put(k.as_bytes(), &random_long_vec(1024)).unwrap();\n        }\n\n        let snap_mgr = cluster_v2.get_snap_mgr(1);\n        let security_mgr = cluster_v2.get_security_mgr();\n        let (msg, snap_key) = generate_snap(&engine, region_id, &snap_mgr);\n        let cfg = tikv::server::Config::default();\n        let limit = Limiter::new(f64::INFINITY);\n        let env = Arc::new(Environment::new(1));\n        let _ = block_on(async {\n            send_snap_v2(\n                env.clone(),\n                snap_mgr.clone(),\n                security_mgr.clone(),\n                &cfg,\n                &s1_addr,\n                msg.clone(),\n                limit.clone(),\n            )\n            .unwrap()\n            .await\n        });\n        let send_result = block_on(async {\n            send_snap_v2(env, snap_mgr, security_mgr, &cfg, &s2_addr, msg, limit)\n                .unwrap()\n                .await\n        });\n        // snapshot should be rejected by cluster v1 tikv, and the snapshot should be\n        // deleted.\n        assert!(send_result.is_err());\n        let dir = cluster_v2.get_snap_dir(1);\n        let read_dir = std::fs::read_dir(dir).unwrap();\n        assert_eq!(0, read_dir.count());\n\n        // The snapshot has been received by cluster v1, so check it's completeness\n        let snap_mgr = cluster_v1.get_snap_mgr(1);\n        let path = snap_mgr\n            .tablet_snap_manager()\n            .unwrap()\n            .final_recv_path(&snap_key);\n        let rocksdb = engine_rocks::util::new_engine_opt(\n            path.as_path().to_str().unwrap(),\n            RocksDbOptions::default(),\n            LARGE_CFS\n                .iter()\n                .map(|&cf| (cf, RocksCfOptions::default()))\n                .collect(),\n        )\n        .unwrap();\n\n        for i in 0..key_num {\n            let k = format!(\"zk{:04}\", i);\n            assert!(\n                rocksdb\n                    .get_value_cf(\"default\", k.as_bytes())\n                    .unwrap()\n                    .is_some()\n            );\n        }\n    };\n\n    // test small snapshot\n    test_receive_snap(20);\n\n    // test large snapshot\n    test_receive_snap(5000);\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_cpu.rs::test_prewrite", "test": "pub fn test_prewrite() {\n    let tag = \"tag_prewrite\";\n\n    let (test_suite, mut store, _) = setup_test_suite();\n    fail::cfg_callback(\"scheduler_process\", || cpu_load(Duration::from_millis(100))).unwrap();\n    defer!(fail::remove(\"scheduler_process\"));\n\n    let jh = test_suite\n        .rt\n        .spawn(require_cpu_time_not_zero(&test_suite, tag));\n\n    let mut ctx = Context::default();\n    ctx.set_resource_group_tag(tag.as_bytes().to_vec());\n    let table = ProductTable::new();\n    let insert = prepare_insert(&mut store, &table);\n    insert.execute_with_ctx(ctx);\n\n    assert!(block_on(jh).unwrap());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_cpu.rs::test_commit", "test": "pub fn test_commit() {\n    let tag = \"tag_commit\";\n\n    let (test_suite, mut store, _) = setup_test_suite();\n    fail::cfg_callback(\"scheduler_process\", || cpu_load(Duration::from_millis(100))).unwrap();\n    defer!(fail::remove(\"scheduler_process\"));\n\n    let jh = test_suite\n        .rt\n        .spawn(require_cpu_time_not_zero(&test_suite, tag));\n\n    let table = ProductTable::new();\n    let insert = prepare_insert(&mut store, &table);\n    insert.execute();\n\n    let mut ctx = Context::default();\n    ctx.set_resource_group_tag(tag.as_bytes().to_vec());\n    store.commit_with_ctx(ctx);\n\n    assert!(block_on(jh).unwrap());\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_dynamic_config.rs::test_report_interval", "test": "pub fn test_report_interval() {\n    let port = alloc_port();\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        receiver_address: format!(\"127.0.0.1:{}\", port),\n        report_receiver_interval: ReadableDuration::secs(3),\n        max_resource_groups: 5000,\n        precision: ReadableDuration::secs(1),\n    });\n    test_suite.start_receiver_at(port);\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n\n    // | Report Interval |\n    // |       3s        |\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n\n    // | Report Interval |\n    // |       1s        |\n    test_suite.cfg_report_receiver_interval(\"1s\");\n\n    const RETRY_TIMES: usize = 3;\n    let (_, mut first_recv_time) = (test_suite.block_receive_one(), Instant::now());\n    for _ in 0..RETRY_TIMES {\n        let (_, second_recv_time) = (test_suite.block_receive_one(), Instant::now());\n        let duration = second_recv_time - first_recv_time;\n\n        if Duration::from_millis(800) < duration && duration < Duration::from_millis(1200) {\n            // test passed\n            return;\n        }\n        first_recv_time = second_recv_time;\n    }\n    panic!(\"failed {} times\", RETRY_TIMES)\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_dynamic_config.rs::test_max_resource_groups", "test": "pub fn test_max_resource_groups() {\n    let port = alloc_port();\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        receiver_address: format!(\"127.0.0.1:{}\", port),\n        report_receiver_interval: ReadableDuration::secs(4),\n        max_resource_groups: 5000,\n        precision: ReadableDuration::secs(2),\n    });\n    test_suite.start_receiver_at(port);\n\n    // Workload\n    // [req-{1..3} * 6, req-{4..5} * 1]\n    let mut wl = iter::repeat(1..=3)\n        .take(6)\n        .flatten()\n        .chain(4..=5)\n        .map(|n| format!(\"req-{}\", n))\n        .collect::<Vec<_>>();\n    wl.shuffle(&mut rand::thread_rng());\n    test_suite.setup_workload(wl);\n\n    // | Max Resource Groups |\n    // |       5000          |\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n    assert!(res.contains_key(\"req-3\"));\n    assert!(res.contains_key(\"req-4\"));\n    assert!(res.contains_key(\"req-5\"));\n\n    // | Max Resource Groups |\n    // |        3            |\n    test_suite.cfg_max_resource_groups(3);\n    test_suite.flush_receiver();\n    let res = test_suite.block_receive_one();\n    assert_eq!(res.len(), 4);\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n    assert!(res.contains_key(\"req-3\"));\n    assert!(res.contains_key(\"\"));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_receiver.rs::test_alter_receiver_address", "test": "pub fn test_alter_receiver_address() {\n    let port = alloc_port();\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        receiver_address: format!(\"127.0.0.1:{}\", port),\n        report_receiver_interval: ReadableDuration::secs(3),\n        max_resource_groups: 5000,\n        precision: ReadableDuration::secs(1),\n    });\n    test_suite.start_receiver_at(port);\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n\n    // | Address |\n    // |   o     |\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n\n    // | Address |\n    // |   !     |\n    test_suite.cfg_receiver_address(format!(\"127.0.0.1:{}\", port + 1));\n    test_suite.flush_receiver();\n    sleep(Duration::from_millis(3500));\n    assert!(test_suite.nonblock_receiver_all().is_empty());\n\n    // | Address |\n    // |   o     |\n    test_suite.cfg_receiver_address(format!(\"127.0.0.1:{}\", port));\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_receiver.rs::test_receiver_blocking", "test": "pub fn test_receiver_blocking() {\n    let port = alloc_port();\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        receiver_address: format!(\"127.0.0.1:{}\", port),\n        report_receiver_interval: ReadableDuration::secs(3),\n        max_resource_groups: 5000,\n        precision: ReadableDuration::secs(1),\n    });\n    test_suite.start_receiver_at(port);\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n\n    // | Block Receiver |\n    // |       x        |\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n\n    // | Block Receiver |\n    // |       o        |\n    test_suite.block_receiver();\n    test_suite.flush_receiver();\n    sleep(Duration::from_millis(3500));\n    assert!(test_suite.nonblock_receiver_all().is_empty());\n\n    // | Block Receiver |\n    // |       x        |\n    test_suite.unblock_receiver();\n    sleep(Duration::from_millis(3500));\n    test_suite.flush_receiver();\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_receiver.rs::test_receiver_shutdown", "test": "pub fn test_receiver_shutdown() {\n    let port = alloc_port();\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        receiver_address: format!(\"127.0.0.1:{}\", port),\n        report_receiver_interval: ReadableDuration::secs(3),\n        max_resource_groups: 5000,\n        precision: ReadableDuration::secs(1),\n    });\n    test_suite.start_receiver_at(port);\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n\n    // | Receiver Alive |\n    // |       o        |\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n\n    // Workload\n    // [req-3, req-4]\n    test_suite.cancel_workload();\n    test_suite.setup_workload(vec![\"req-3\", \"req-4\"]);\n\n    // | Receiver Alive |\n    // |       x        |\n    test_suite.shutdown_receiver();\n    test_suite.flush_receiver();\n    sleep(Duration::from_millis(3500));\n    assert!(test_suite.nonblock_receiver_all().is_empty());\n\n    // | Receiver Alive |\n    // |       o        |\n    test_suite.start_receiver_at(port);\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-3\"));\n    assert!(res.contains_key(\"req-4\"));\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raft_storage.rs::test_raft_storage_rollback_before_prewrite", "test": "fn test_raft_storage_rollback_before_prewrite() {\n    let (_cluster, storage, ctx) = new_raft_storage();\n    storage\n        .rollback(ctx.clone(), vec![Key::from_raw(b\"key\")], 10)\n        .unwrap();\n    let ret = storage.prewrite(\n        ctx,\n        vec![Mutation::make_put(Key::from_raw(b\"key\"), b\"value\".to_vec())],\n        b\"key\".to_vec(),\n        10,\n    );\n    assert!(ret.is_err());\n    let err = ret.unwrap_err();\n    match err {\n        StorageError(box StorageErrorInner::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n            box MvccErrorInner::WriteConflict { .. },\n        ))))) => {}\n        _ => {\n            panic!(\"expect WriteConflict error, but got {:?}\", err);\n        }\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raft_storage.rs::test_engine_leader_change_twice", "test": "fn test_engine_leader_change_twice() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n\n    let region = cluster.get_region(b\"\");\n    let peers = region.get_peers();\n\n    cluster.must_transfer_leader(region.get_id(), peers[0].clone());\n    let engine = cluster.sim.rl().storages[&peers[0].get_id()].clone();\n\n    let term = cluster\n        .request(b\"\", vec![new_get_cmd(b\"\")], true, Duration::from_secs(5))\n        .get_header()\n        .get_current_term();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(peers[0].clone());\n    ctx.set_term(term);\n\n    // Not leader.\n    cluster.must_transfer_leader(region.get_id(), peers[1].clone());\n    engine\n        .put(&ctx, Key::from_raw(b\"a\"), b\"a\".to_vec())\n        .unwrap_err();\n    // Term not match.\n    cluster.must_transfer_leader(region.get_id(), peers[0].clone());\n    let res = engine.put(&ctx, Key::from_raw(b\"a\"), b\"a\".to_vec());\n    if let KvError(box KvErrorInner::Request(ref e)) = *res.as_ref().err().unwrap() {\n        assert!(e.has_stale_command());\n    } else {\n        panic!(\"expect stale command, but got {:?}\", res);\n    }\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_invalid_read_index_when_no_leader", "test": "fn test_invalid_read_index_when_no_leader() {\n    // Initialize cluster\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_lease_read(&mut cluster.cfg, Some(10), Some(6));\n    cluster.cfg.raft_store.raft_heartbeat_ticks = 1;\n    cluster.cfg.raft_store.hibernate_regions = false;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    // Set region and peers\n    cluster.run();\n    cluster.must_put(b\"k0\", b\"v0\");\n    // Transfer leader to p2\n    let region = cluster.get_region(b\"k0\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let mut follower_peers = region.get_peers().to_vec();\n    follower_peers.retain(|p| p.get_id() != leader.get_id());\n    let follower = follower_peers.pop().unwrap();\n\n    // Delay all raft messages on follower.\n    cluster.sim.wl().add_recv_filter(\n        follower.get_store_id(),\n        Box::new(\n            RegionPacketFilter::new(region.get_id(), follower.get_store_id())\n                .direction(Direction::Recv)\n                .msg_type(MessageType::MsgHeartbeat)\n                .msg_type(MessageType::MsgAppend)\n                .msg_type(MessageType::MsgRequestVoteResponse)\n                .when(Arc::new(AtomicBool::new(true))),\n        ),\n    );\n\n    // wait for election timeout\n    thread::sleep(time::Duration::from_millis(300));\n    // send read index requests to follower\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_read_index_cmd()],\n        true,\n    );\n    request.mut_header().set_peer(follower.clone());\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(follower.get_store_id(), request, cb)\n        .unwrap();\n\n    let resp = rx.recv_timeout(time::Duration::from_millis(500)).unwrap();\n    assert!(\n        resp.get_header().get_error().has_not_leader(),\n        \"{:?}\",\n        resp.get_header()\n    );\n}", "error": "Not Definition Found"}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_storage.rs::test_isolation_multi_inc", "test": "fn test_isolation_multi_inc() {\n    const THREAD_NUM: usize = 4;\n    const KEY_NUM: usize = 4;\n    const INC_PER_THREAD: usize = 100;\n\n    let store = AssertionStorage::default();\n    let oracle = Arc::new(Oracle::new());\n    let mut threads = vec![];\n    for _ in 0..THREAD_NUM {\n        let (store, oracle) = (store.clone(), Arc::clone(&oracle));\n        threads.push(thread::spawn(move || {\n            for _ in 0..INC_PER_THREAD {\n                assert!(inc_multi(&store.store, &oracle, KEY_NUM));\n            }\n        }));\n    }\n    for t in threads {\n        t.join().unwrap();\n    }\n    for n in 0..KEY_NUM {\n        assert_eq!(\n            inc(&store.store, &oracle, &format_key(n)).unwrap() as usize,\n            THREAD_NUM * INC_PER_THREAD\n        );\n    }\n}", "error": "Not Definition Found"}
