{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/batch-system/tests/cases/batch.rs::test_batch", "test": "fn test_batch() {\n    let (control_tx, control_fsm) = Runner::new(10);\n    let (router, mut system) =\n        batch_system::create_system(&Config::default(), control_tx, control_fsm, None);\n    let builder = Builder::new();\n    let metrics = builder.metrics.clone();\n    system.spawn(\"test\".to_owned(), builder);\n    let mut expected_metrics = HandleMetrics::default();\n    assert_eq!(*metrics.lock().unwrap(), expected_metrics);\n    let (tx, rx) = mpsc::unbounded();\n    let tx_ = tx.clone();\n    let r = router.clone();\n    router\n        .send_control(Message::Callback(Box::new(\n            move |_: &Handler, _: &mut Runner| {\n                let (tx, runner) = Runner::new(10);\n                let mailbox = BasicMailbox::new(tx, runner, Arc::default());\n                r.register(1, mailbox);\n                tx_.send(1).unwrap();\n            },\n        )))\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(1));\n    // sleep to wait Batch-System to finish calling end().\n    sleep(Duration::from_millis(20));\n    router\n        .send(\n            1,\n            Message::Callback(Box::new(move |_: &Handler, _: &mut Runner| {\n                tx.send(2).unwrap();\n            })),\n        )\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(2));\n    system.shutdown();\n    expected_metrics.control = 1;\n    expected_metrics.normal = 1;\n    expected_metrics.begin = 2;\n    assert_eq!(*metrics.lock().unwrap(), expected_metrics);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/txn_ext.rs::lock", "code": "fn lock(primary: &[u8]) -> PessimisticLock {\n        PessimisticLock {\n            primary: primary.to_vec().into_boxed_slice(),\n            start_ts: 100.into(),\n            ttl: 3000,\n            for_update_ts: 110.into(),\n            min_commit_ts: 110.into(),\n            last_change_ts: 105.into(),\n            versions_to_last_change: 2,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/batch-system/tests/cases/batch.rs::test_priority", "test": "fn test_priority() {\n    let (control_tx, control_fsm) = Runner::new(10);\n    let (router, mut system) =\n        batch_system::create_system(&Config::default(), control_tx, control_fsm, None);\n    let builder = Builder::new();\n    system.spawn(\"test\".to_owned(), builder);\n    let (tx, rx) = mpsc::unbounded();\n    let tx_ = tx.clone();\n    let r = router.clone();\n    let state_cnt = Arc::new(AtomicUsize::new(0));\n    router\n        .send_control(Message::Callback(Box::new(\n            move |_: &Handler, _: &mut Runner| {\n                let (tx, runner) = Runner::new(10);\n                r.register(1, BasicMailbox::new(tx, runner, state_cnt.clone()));\n                let (tx2, mut runner2) = Runner::new(10);\n                runner2.set_priority(Priority::Low);\n                r.register(2, BasicMailbox::new(tx2, runner2, state_cnt));\n                tx_.send(1).unwrap();\n            },\n        )))\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(1));\n\n    let tx_ = tx.clone();\n    router\n        .send(\n            1,\n            Message::Callback(Box::new(move |h: &Handler, r: &mut Runner| {\n                assert_eq!(h.get_priority(), Priority::Normal);\n                assert_eq!(h.get_priority(), r.get_priority());\n                tx_.send(2).unwrap();\n            })),\n        )\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(2));\n\n    router\n        .send(\n            2,\n            Message::Callback(Box::new(move |h: &Handler, r: &mut Runner| {\n                assert_eq!(h.get_priority(), Priority::Low);\n                assert_eq!(h.get_priority(), r.get_priority());\n                tx.send(3).unwrap();\n            })),\n        )\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(3));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/batch-system/tests/cases/batch.rs::test_resource_group", "test": "fn test_resource_group() {\n    let (control_tx, control_fsm) = Runner::new(10);\n    let resource_manager = ResourceGroupManager::default();\n\n    let get_group = |name: &str, read_tokens: u64, write_tokens: u64| -> ResourceGroup {\n        let mut group = ResourceGroup::new();\n        group.set_name(name.to_string());\n        group.set_mode(GroupMode::RawMode);\n        let mut resource_setting = GroupRawResourceSettings::new();\n        resource_setting\n            .mut_cpu()\n            .mut_settings()\n            .set_fill_rate(read_tokens);\n        resource_setting\n            .mut_io_write()\n            .mut_settings()\n            .set_fill_rate(write_tokens);\n        group.set_raw_resource_settings(resource_setting);\n        group\n    };\n\n    resource_manager.add_resource_group(get_group(\"group1\", 10, 10));\n    resource_manager.add_resource_group(get_group(\"group2\", 100, 100));\n\n    let mut cfg = Config::default();\n    cfg.pool_size = 1;\n    let (router, mut system) = batch_system::create_system(\n        &cfg,\n        control_tx,\n        control_fsm,\n        Some(resource_manager.derive_controller(\"test\".to_string(), false)),\n    );\n    let builder = Builder::new();\n    system.spawn(\"test\".to_owned(), builder);\n    let (tx, rx) = mpsc::unbounded();\n    let tx_ = tx.clone();\n    let r = router.clone();\n    let state_cnt = Arc::new(AtomicUsize::new(0));\n    router\n        .send_control(Message::Callback(Box::new(\n            move |_: &Handler, _: &mut Runner| {\n                let (tx, runner) = Runner::new(10);\n                r.register(1, BasicMailbox::new(tx, runner, state_cnt.clone()));\n                let (tx2, runner2) = Runner::new(10);\n                r.register(2, BasicMailbox::new(tx2, runner2, state_cnt));\n                tx_.send(0).unwrap();\n            },\n        )))\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(0));\n\n    let tx_ = tx.clone();\n    let (tx1, rx1) = std::sync::mpsc::sync_channel(0);\n    // block the thread\n    router\n        .send_control(Message::Callback(Box::new(\n            move |_: &Handler, _: &mut Runner| {\n                tx_.send(0).unwrap();\n                tx1.send(0).unwrap();\n            },\n        )))\n        .unwrap();\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(0));\n\n    router\n        .send(1, Message::Resource(\"group1\".to_string(), 1))\n        .unwrap();\n    let tx_ = tx.clone();\n    router\n        .send(\n            1,\n            Message::Callback(Box::new(move |_: &Handler, _: &mut Runner| {\n                tx_.send(1).unwrap();\n            })),\n        )\n        .unwrap();\n\n    router\n        .send(2, Message::Resource(\"group2\".to_string(), 1))\n        .unwrap();\n    router\n        .send(\n            2,\n            Message::Callback(Box::new(move |_: &Handler, _: &mut Runner| {\n                tx.send(2).unwrap();\n            })),\n        )\n        .unwrap();\n\n    // pause the blocking thread\n    assert_eq!(rx1.recv_timeout(Duration::from_secs(3)), Ok(0));\n\n    // should recv from group2 first, because group2 has more tokens and it would be\n    // handled with higher priority.\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(2));\n    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(1));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/batch-system/tests/cases/router.rs::test_router_trace", "test": "fn test_router_trace() {\n    let (control_tx, control_fsm) = Runner::new(10);\n    let (router, mut system) =\n        batch_system::create_system(&Config::default(), control_tx, control_fsm, None);\n    let builder = Builder::new();\n    system.spawn(\"test\".to_owned(), builder);\n\n    let register_runner = |addr| {\n        let (sender, runner) = Runner::new(10);\n        let mailbox = BasicMailbox::new(sender, runner, router.state_cnt().clone());\n        router.register(addr, mailbox);\n    };\n    let close_runner = |addr| {\n        router.close(addr);\n    };\n\n    let mut mailboxes = vec![];\n    for i in 0..10 {\n        register_runner(i);\n        mailboxes.push(router.mailbox(i).unwrap());\n    }\n    assert_eq!(router.alive_cnt(), 10);\n    assert_eq!(router.state_cnt().load(Ordering::Relaxed), 11);\n    for i in 0..10 {\n        close_runner(i);\n    }\n    assert_eq!(router.alive_cnt(), 0);\n    assert_eq!(router.state_cnt().load(Ordering::Relaxed), 11);\n    drop(mailboxes);\n    assert_eq!(router.alive_cnt(), 0);\n    assert_eq!(router.state_cnt().load(Ordering::Relaxed), 1);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/batch-system/src/router.rs::alive_cnt", "code": "pub fn alive_cnt(&self) -> usize {\n        self.normals.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/failpoints/test_endpoint.rs::test_old_value_cache_without_downstreams", "test": "fn test_old_value_cache_without_downstreams() {\n    fn check_old_value_cache(scheduler: &Scheduler<Task>, updates: usize) {\n        let (tx, rx) = mpsc::sync_channel(1);\n        let checker = move |c: &OldValueCache| tx.send(c.update_count()).unwrap();\n        scheduler\n            .schedule(Task::Validate(Validate::OldValueCache(Box::new(checker))))\n            .unwrap();\n        assert_eq!(rx.recv().unwrap(), updates);\n    }\n\n    let mutation = || {\n        let mut mutation = Mutation::default();\n        mutation.set_op(Op::Put);\n        mutation.key = b\"key\".to_vec();\n        mutation.value = b\"value\".to_vec();\n        mutation\n    };\n\n    fail::cfg(\"cdc_flush_old_value_metrics\", \"return\").unwrap();\n\n    let cluster = new_server_cluster(0, 1);\n    let mut suite = TestSuiteBuilder::new().cluster(cluster).build();\n    let scheduler = suite.endpoints[&1].scheduler();\n\n    // Add a subscription and then check old value cache.\n    let (mut req_tx, event_feed, receive_event) = new_event_feed(suite.get_region_cdc_client(1));\n    let req = suite.new_changedata_request(1);\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n    receive_event(false); // Wait until the initialization finishes.\n\n    // Old value cache will be updated because there is 1 capture.\n    suite.must_kv_prewrite(1, vec![mutation()], b\"key\".to_vec(), 3.into());\n    suite.must_kv_commit(1, vec![b\"key\".to_vec()], 3.into(), 4.into());\n    check_old_value_cache(&scheduler, 1);\n\n    drop(req_tx);\n    drop(event_feed);\n    drop(receive_event);\n    sleep_ms(200);\n\n    // Old value cache won't be updated because there is no captures.\n    suite.must_kv_prewrite(1, vec![mutation()], b\"key\".to_vec(), 5.into());\n    suite.must_kv_commit(1, vec![b\"key\".to_vec()], 5.into(), 6.into());\n    check_old_value_cache(&scheduler, 1);\n\n    fail::remove(\"cdc_flush_old_value_metrics\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/fsm/peer.rs::recv", "code": "pub fn recv(&mut self, peer_msg_buf: &mut Vec<PeerMsg>, batch_size: usize) -> usize {\n        let l = peer_msg_buf.len();\n        for i in l..batch_size {\n            match self.receiver.try_recv() {\n                Ok(msg) => peer_msg_buf.push(msg),\n                Err(e) => {\n                    if let TryRecvError::Disconnected = e {\n                        self.is_stopped = true;\n                    }\n                    return i - l;\n                }\n            }\n        }\n        batch_size - l\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/failpoints/test_endpoint.rs::test_cdc_rawkv_resolved_ts", "test": "fn test_cdc_rawkv_resolved_ts() {\n    let mut suite = TestSuite::new(1, ApiVersion::V2);\n    let cluster = &suite.cluster;\n\n    let region = cluster.get_region(b\"\");\n    let region_id = region.get_id();\n    let leader = region.get_peers()[0].clone();\n    let node_id = leader.get_id();\n    let ts_provider = cluster.sim.rl().get_causal_ts_provider(node_id).unwrap();\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut req = suite.new_changedata_request(region_id);\n    req.set_kv_api(ChangeDataRequestKvApi::RawKv);\n    let (mut req_tx, _event_feed_wrap, receive_event) =\n        new_event_feed(suite.get_region_cdc_client(region_id));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n\n    let event = receive_event(false);\n    event\n        .events\n        .into_iter()\n        .for_each(|e| match e.event.unwrap() {\n            Event_oneof_event::Entries(es) => {\n                assert!(es.entries.len() == 1, \"{:?}\", es);\n                let e = &es.entries[0];\n                assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n            }\n            other => panic!(\"unknown event {:?}\", other),\n        });\n    // Sleep a while to make sure the stream is registered.\n    sleep_ms(1000);\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n    ctx.set_api_version(ApiVersion::V2);\n    let mut put_req = RawPutRequest::default();\n    put_req.set_context(ctx);\n    put_req.key = b\"rk3\".to_vec();\n    put_req.value = b\"v3\".to_vec();\n\n    let pause_write_fp = \"raftkv_async_write\";\n    fail::cfg(pause_write_fp, \"pause\").unwrap();\n    let ts = block_on(ts_provider.async_get_ts()).unwrap();\n    let handle = thread::spawn(move || {\n        let _ = client.raw_put(&put_req).unwrap();\n    });\n\n    sleep_ms(100);\n\n    let event = receive_event(true).resolved_ts.unwrap();\n    assert!(\n        ts.next() >= TimeStamp::from(event.ts),\n        \"{} {}\",\n        ts,\n        TimeStamp::from(event.ts)\n    );\n    // Receive again to make sure resolved ts <= ongoing request's ts.\n    let event = receive_event(true).resolved_ts.unwrap();\n    assert!(\n        ts.next() >= TimeStamp::from(event.ts),\n        \"{} {}\",\n        ts,\n        TimeStamp::from(event.ts)\n    );\n\n    fail::remove(pause_write_fp);\n    handle.join().unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/chunked_vec_bytes.rs::len", "code": "fn len(&self) -> usize {\n        self.length\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_cdc_rawkv_basic", "test": "fn test_cdc_rawkv_basic() {\n    let mut suite = TestSuite::new(1, ApiVersion::V2);\n\n    // rawkv\n    let mut req = suite.new_changedata_request(1);\n    req.set_kv_api(ChangeDataRequestKvApi::RawKv);\n    let (mut req_tx, _event_feed_wrap, receive_event) =\n        new_event_feed(suite.get_region_cdc_client(1));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n\n    let event = receive_event(false);\n    event.events.into_iter().for_each(|e| {\n        match e.event.unwrap() {\n            // Even if there is no write,\n            // it should always outputs an Initialized event.\n            Event_oneof_event::Entries(es) => {\n                assert!(es.entries.len() == 1, \"{:?}\", es);\n                let e = &es.entries[0];\n                assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n            }\n            other => panic!(\"unknown event {:?}\", other),\n        }\n    });\n    // Sleep a while to make sure the stream is registered.\n    sleep_ms(1000);\n    // There must be a delegate.\n    let scheduler = suite.endpoints.values().next().unwrap().scheduler();\n    scheduler\n        .schedule(Task::Validate(Validate::Region(\n            1,\n            Box::new(|delegate| {\n                let d = delegate.unwrap();\n                assert_eq!(d.downstreams().len(), 1);\n            }),\n        )))\n        .unwrap();\n\n    // If tikv enable ApiV2, raw key needs to start with 'r';\n    let (k, v) = (b\"rkey1\".to_vec(), b\"value\".to_vec());\n    suite.must_kv_put(1, k, v);\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1, \"{:?}\", events);\n\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Entries(entries) => {\n            assert_eq!(entries.entries.len(), 1);\n            assert_eq!(entries.entries[0].get_type(), EventLogType::Committed);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    // boundary case\n    let (k, v) = (b\"r\\0\".to_vec(), b\"value\".to_vec());\n    suite.must_kv_put(1, k, v);\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1, \"{:?}\", events);\n\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Entries(entries) => {\n            assert_eq!(entries.entries.len(), 1);\n            assert_eq!(entries.entries[0].get_type(), EventLogType::Committed);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/chunked_vec_bytes.rs::len", "code": "fn len(&self) -> usize {\n        self.length\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_cdc_rawkv_scan", "test": "fn test_cdc_rawkv_scan() {\n    let mut suite = TestSuite::new(3, ApiVersion::V2);\n\n    let (k1, v1) = (b\"rkey1\".to_vec(), b\"value1\".to_vec());\n    suite.must_kv_put(1, k1, v1);\n\n    let (k2, v2) = (b\"rkey2\".to_vec(), b\"value2\".to_vec());\n    suite.must_kv_put(1, k2, v2);\n\n    let ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    suite.flush_causal_timestamp_for_region(1);\n\n    let (k3, v3) = (b\"rkey3\".to_vec(), b\"value3\".to_vec());\n    suite.must_kv_put(1, k3.clone(), v3.clone());\n\n    let (k4, v4) = (b\"rkey4\".to_vec(), b\"value4\".to_vec());\n    suite.must_kv_put(1, k4.clone(), v4.clone());\n\n    let mut req = suite.new_changedata_request(1);\n    req.set_kv_api(ChangeDataRequestKvApi::RawKv);\n    req.set_checkpoint_ts(ts.into_inner());\n    let (mut req_tx, event_feed_wrap, receive_event) =\n        new_event_feed(suite.get_region_cdc_client(1));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n    let mut events = receive_event(false).events.to_vec();\n    if events.len() == 1 {\n        events.extend(receive_event(false).events.into_iter());\n    }\n    assert_eq!(events.len(), 2, \"{:?}\", events);\n\n    match events.remove(0).event.unwrap() {\n        // Batch size is set to 3.\n        Event_oneof_event::Entries(es) => {\n            assert!(es.entries.len() == 2, \"{:?}\", es);\n            let e = &es.entries[0];\n            assert_eq!(e.get_type(), EventLogType::Committed, \"{:?}\", es);\n            assert_eq!(e.key, k3, \"{:?}\", es);\n            assert_eq!(e.value, v3, \"{:?}\", es);\n\n            let e = &es.entries[1];\n            assert_eq!(e.get_type(), EventLogType::Committed, \"{:?}\", es);\n            assert_eq!(e.key, k4, \"{:?}\", es);\n            assert_eq!(e.value, v4, \"{:?}\", es);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    match events.pop().unwrap().event.unwrap() {\n        // Then it outputs Initialized event.\n        Event_oneof_event::Entries(es) => {\n            assert!(es.entries.len() == 1, \"{:?}\", es);\n            let e = &es.entries[0];\n            assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    event_feed_wrap.replace(None);\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_region_split", "test": "fn test_region_split() {\n    let cluster = new_server_cluster(1, 1);\n    cluster.pd_client.disable_default_operator();\n    let mut suite = TestSuiteBuilder::new().cluster(cluster).build();\n\n    let region = suite.cluster.get_region(&[]);\n    let mut req = suite.new_changedata_request(region.get_id());\n    let (mut req_tx, event_feed_wrap, receive_event) =\n        new_event_feed(suite.get_region_cdc_client(region.get_id()));\n    block_on(req_tx.send((req.clone(), WriteFlags::default()))).unwrap();\n    // Make sure region 1 is registered.\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1);\n    match events.pop().unwrap().event.unwrap() {\n        // Even if there is no write,\n        // it should always outputs an Initialized event.\n        Event_oneof_event::Entries(es) => {\n            assert!(es.entries.len() == 1, \"{:?}\", es);\n            let e = &es.entries[0];\n            assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n    // Split region.\n    suite.cluster.must_split(&region, b\"k0\");\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1);\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Error(err) => {\n            assert!(err.has_epoch_not_match(), \"{:?}\", err);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n    // Try to subscribe region again.\n    let region = suite.cluster.get_region(b\"k0\");\n    // Ensure it is the previous region.\n    assert_eq!(req.get_region_id(), region.get_id());\n    req.set_region_epoch(region.get_region_epoch().clone());\n    block_on(req_tx.send((req.clone(), WriteFlags::default()))).unwrap();\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1);\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Entries(es) => {\n            assert!(es.entries.len() == 1, \"{:?}\", es);\n            let e = &es.entries[0];\n            assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    // Try to subscribe region again.\n    let region1 = suite.cluster.get_region(&[]);\n    req.region_id = region1.get_id();\n    req.set_region_epoch(region1.get_region_epoch().clone());\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1);\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Entries(es) => {\n            assert!(es.entries.len() == 1, \"{:?}\", es);\n            let e = &es.entries[0];\n            assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    // Make sure resolved ts can be advanced normally.\n    let mut counter = 0;\n    let mut previous_ts = 0;\n    loop {\n        // Even if there is no write,\n        // resolved ts should be advanced regularly.\n        let event = receive_event(true);\n        if let Some(resolved_ts) = event.resolved_ts.as_ref() {\n            assert!(resolved_ts.ts >= previous_ts);\n            assert!(\n                resolved_ts.regions == vec![region.id, region1.id]\n                    || resolved_ts.regions == vec![region1.id, region.id]\n            );\n            previous_ts = resolved_ts.ts;\n            counter += 1;\n        }\n        if counter > 5 {\n            break;\n        }\n    }\n\n    event_feed_wrap.replace(None);\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_resolved_ts_with_learners", "test": "fn test_resolved_ts_with_learners() {\n    let cluster = new_server_cluster(0, 2);\n    cluster.pd_client.disable_default_operator();\n    let mut suite = TestSuiteBuilder::new()\n        .cluster(cluster)\n        .build_with_cluster_runner(|cluster| {\n            let r = cluster.run_conf_change();\n            cluster.pd_client.must_add_peer(r, new_learner_peer(2, 2));\n        });\n\n    let rid = suite.cluster.get_region(&[]).id;\n    let req = suite.new_changedata_request(rid);\n    let (mut req_tx, _, receive_event) = new_event_feed(suite.get_region_cdc_client(rid));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n\n    for _ in 0..10 {\n        let event = receive_event(true);\n        if event.has_resolved_ts() {\n            assert!(event.get_resolved_ts().regions == vec![rid]);\n            drop(receive_event);\n            suite.stop();\n            return;\n        }\n    }\n    panic!(\"resolved timestamp should be advanced correctly\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/util.rs::get_resolved_ts", "code": "pub fn get_resolved_ts(&self, region_id: &u64) -> Option<u64> {\n        self.registry\n            .lock()\n            .unwrap()\n            .get(region_id)\n            .map(|rp| rp.resolved_ts())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_flashback", "test": "fn test_flashback() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.cfg.resolved_ts.advance_ts_interval = ReadableDuration::millis(50);\n    let mut suite = TestSuiteBuilder::new().cluster(cluster).build();\n\n    let key = Key::from_raw(b\"a\");\n    let region = suite.cluster.get_region(key.as_encoded());\n    let region_id = region.get_id();\n    let req = suite.new_changedata_request(region_id);\n    let (mut req_tx, _, receive_event) = new_event_feed(suite.get_region_cdc_client(region_id));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n    let event = receive_event(false);\n    event.events.into_iter().for_each(|e| {\n        match e.event.unwrap() {\n            // Even if there is no write,\n            // it should always outputs an Initialized event.\n            Event_oneof_event::Entries(es) => {\n                assert!(es.entries.len() == 1, \"{:?}\", es);\n                let e = &es.entries[0];\n                assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n            }\n            other => panic!(\"unknown event {:?}\", other),\n        }\n    });\n    // Sleep a while to make sure the stream is registered.\n    sleep_ms(1000);\n    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    for i in 0..2 {\n        let (k, v) = (\n            format!(\"key{}\", i).as_bytes().to_vec(),\n            format!(\"value{}\", i).as_bytes().to_vec(),\n        );\n        // Prewrite\n        let start_ts1 = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n        let mut mutation = Mutation::default();\n        mutation.set_op(Op::Put);\n        mutation.key = k.clone();\n        mutation.value = v;\n        suite.must_kv_prewrite(1, vec![mutation], k.clone(), start_ts1);\n        // Commit\n        let commit_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n        suite.must_kv_commit(1, vec![k.clone()], start_ts1, commit_ts);\n    }\n    let (start_key, end_key) = (b\"key0\".to_vec(), b\"key2\".to_vec());\n    // Prepare flashback.\n    let flashback_start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    suite.must_kv_prepare_flashback(region_id, &start_key, &end_key, flashback_start_ts);\n    // resolved ts should not be advanced anymore.\n    let mut counter = 0;\n    let mut last_resolved_ts = 0;\n    loop {\n        let event = receive_event(true);\n        if let Some(resolved_ts) = event.resolved_ts.as_ref() {\n            if resolved_ts.ts == last_resolved_ts {\n                counter += 1;\n            }\n            last_resolved_ts = resolved_ts.ts;\n        }\n        if counter > 20 {\n            break;\n        }\n        sleep_ms(50);\n    }\n    // Flashback.\n    let flashback_commit_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    suite.must_kv_flashback(\n        region_id,\n        &start_key,\n        &end_key,\n        flashback_start_ts,\n        flashback_commit_ts,\n        start_ts,\n    );\n    // Check the flashback event.\n    let mut resolved_ts = 0;\n    let mut event_counter = 0;\n    loop {\n        let mut cde = receive_event(true);\n        if cde.get_resolved_ts().get_ts() > resolved_ts {\n            resolved_ts = cde.get_resolved_ts().get_ts();\n        }\n        let events = cde.mut_events();\n        if !events.is_empty() {\n            assert_eq!(events.len(), 1);\n            match events.pop().unwrap().event.unwrap() {\n                Event_oneof_event::Entries(entries) => {\n                    assert_eq!(entries.entries.len(), 1);\n                    event_counter += 1;\n                    let e = &entries.entries[0];\n                    assert!(e.commit_ts > resolved_ts);\n                    assert_eq!(e.get_op_type(), EventRowOpType::Delete);\n                    match e.get_type() {\n                        EventLogType::Committed => {\n                            // First entry should be a 1PC flashback.\n                            assert_eq!(e.get_key(), b\"key1\");\n                            assert_eq!(event_counter, 1);\n                        }\n                        EventLogType::Commit => {\n                            // Second entry should be a 2PC commit.\n                            assert_eq!(e.get_key(), b\"key0\");\n                            assert_eq!(event_counter, 2);\n                            break;\n                        }\n                        _ => panic!(\"unknown event type {:?}\", e.get_type()),\n                    }\n                }\n                other => panic!(\"unknown event {:?}\", other),\n            }\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/chunked_vec_bytes.rs::len", "code": "fn len(&self) -> usize {\n        self.length\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_cdc.rs::test_cdc_filter_key_range", "test": "fn test_cdc_filter_key_range() {\n    let mut suite = TestSuite::new(1, ApiVersion::V1);\n\n    let req = suite.new_changedata_request(1);\n\n    // Observe range [key1, key3).\n    let mut req_1_3 = req.clone();\n    req_1_3.request_id = 13;\n    req_1_3.start_key = Key::from_raw(b\"key1\").into_encoded();\n    req_1_3.end_key = Key::from_raw(b\"key3\").into_encoded();\n    let (mut req_tx13, _event_feed_wrap13, receive_event13) =\n        new_event_feed(suite.get_region_cdc_client(1));\n    block_on(req_tx13.send((req_1_3, WriteFlags::default()))).unwrap();\n    let event = receive_event13(false);\n    event\n        .events\n        .into_iter()\n        .for_each(|e| match e.event.unwrap() {\n            Event_oneof_event::Entries(es) => {\n                assert!(es.entries.len() == 1, \"{:?}\", es);\n                let e = &es.entries[0];\n                assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n            }\n            other => panic!(\"unknown event {:?}\", other),\n        });\n\n    let (mut req_tx24, _event_feed_wrap24, receive_event24) =\n        new_event_feed(suite.get_region_cdc_client(1));\n    let mut req_2_4 = req;\n    req_2_4.request_id = 24;\n    req_2_4.start_key = Key::from_raw(b\"key2\").into_encoded();\n    req_2_4.end_key = Key::from_raw(b\"key4\").into_encoded();\n    block_on(req_tx24.send((req_2_4, WriteFlags::default()))).unwrap();\n    let event = receive_event24(false);\n    event\n        .events\n        .into_iter()\n        .for_each(|e| match e.event.unwrap() {\n            Event_oneof_event::Entries(es) => {\n                assert!(es.entries.len() == 1, \"{:?}\", es);\n                let e = &es.entries[0];\n                assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n            }\n            other => panic!(\"unknown event {:?}\", other),\n        });\n\n    // Sleep a while to make sure the stream is registered.\n    sleep_ms(1000);\n\n    let receive_and_check_events = |is13: bool, is24: bool| -> Vec<Event> {\n        if is13 && is24 {\n            let mut events = receive_event13(false).events.to_vec();\n            let mut events24 = receive_event24(false).events.to_vec();\n            events.append(&mut events24);\n            events\n        } else if is13 {\n            let events = receive_event13(false).events.to_vec();\n            let event = receive_event24(true);\n            assert!(event.resolved_ts.is_some(), \"{:?}\", event);\n            events\n        } else if is24 {\n            let events = receive_event24(false).events.to_vec();\n            let event = receive_event13(true);\n            assert!(event.resolved_ts.is_some(), \"{:?}\", event);\n            events\n        } else {\n            let event = receive_event13(true);\n            assert!(event.resolved_ts.is_some(), \"{:?}\", event);\n            let event = receive_event24(true);\n            assert!(event.resolved_ts.is_some(), \"{:?}\", event);\n            vec![]\n        }\n    };\n    for case in &[\n        (\"key1\", true, false, true /* commit */),\n        (\"key1\", true, false, false /* rollback */),\n        (\"key2\", true, true, true),\n        (\"key3\", false, true, true),\n        (\"key4\", false, false, true),\n    ] {\n        let (k, v) = (case.0.to_owned(), \"value\".to_owned());\n        // Prewrite\n        let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n        let mut mutation = Mutation::default();\n        mutation.set_op(Op::Put);\n        mutation.key = k.clone().into_bytes();\n        mutation.value = v.into_bytes();\n        suite.must_kv_prewrite(1, vec![mutation], k.clone().into_bytes(), start_ts);\n        let mut events = receive_and_check_events(case.1, case.2);\n        while let Some(event) = events.pop() {\n            match event.event.unwrap() {\n                Event_oneof_event::Entries(entries) => {\n                    assert_eq!(entries.entries.len(), 1);\n                    assert_eq!(entries.entries[0].get_type(), EventLogType::Prewrite);\n                }\n                other => panic!(\"unknown event {:?}\", other),\n            }\n        }\n\n        if case.3 {\n            // Commit\n            let commit_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n            suite.must_kv_commit(1, vec![k.into_bytes()], start_ts, commit_ts);\n            let mut events = receive_and_check_events(case.1, case.2);\n            while let Some(event) = events.pop() {\n                match event.event.unwrap() {\n                    Event_oneof_event::Entries(entries) => {\n                        assert_eq!(entries.entries.len(), 1);\n                        assert_eq!(entries.entries[0].get_type(), EventLogType::Commit);\n                    }\n                    other => panic!(\"unknown event {:?}\", other),\n                }\n            }\n        } else {\n            // Rollback\n            suite.must_kv_rollback(1, vec![k.into_bytes()], start_ts);\n            let mut events = receive_and_check_events(case.1, case.2);\n            while let Some(event) = events.pop() {\n                match event.event.unwrap() {\n                    Event_oneof_event::Entries(entries) => {\n                        assert_eq!(entries.entries.len(), 1);\n                        assert_eq!(entries.entries[0].get_type(), EventLogType::Rollback);\n                    }\n                    other => panic!(\"unknown event {:?}\", other),\n                }\n            }\n        }\n    }\n\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/chunked_vec_bytes.rs::len", "code": "fn len(&self) -> usize {\n        self.length\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/tests/integrations/test_flow_control.rs::test_cdc_congest", "test": "fn test_cdc_congest() {\n    let mut cluster = new_server_cluster(1, 1);\n    // Increase the Raft tick interval to make this test case running reliably.\n    configure_for_lease_read(&mut cluster.cfg, Some(100), None);\n    let memory_quota = 1024; // 1KB\n    let mut suite = TestSuiteBuilder::new()\n        .cluster(cluster)\n        .memory_quota(memory_quota)\n        .build();\n\n    let req = suite.new_changedata_request(1);\n    let (mut req_tx, _event_feed_wrap, receive_event) =\n        new_event_feed(suite.get_region_cdc_client(1));\n    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();\n    let event = receive_event(false);\n    event.events.into_iter().for_each(|e| {\n        match e.event.unwrap() {\n            // Even if there is no write,\n            // it should always outputs an Initialized event.\n            Event_oneof_event::Entries(es) => {\n                assert!(es.entries.len() == 1, \"{:?}\", es);\n                let e = &es.entries[0];\n                assert_eq!(e.get_type(), EventLogType::Initialized, \"{:?}\", es);\n            }\n            other => panic!(\"unknown event {:?}\", other),\n        }\n    });\n\n    // Client must receive messages when there is no congest error.\n    let value_size = memory_quota / 2;\n    let (k, v) = (\"key1\".to_owned(), vec![5; value_size]);\n    // Prewrite\n    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.key = k.clone().into_bytes();\n    mutation.value = v;\n    suite.must_kv_prewrite(1, vec![mutation], k.into_bytes(), start_ts);\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1, \"{:?}\", events);\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Entries(entries) => {\n            assert_eq!(entries.entries.len(), 1);\n            assert_eq!(entries.entries[0].get_type(), EventLogType::Prewrite);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    // Trigger congest error.\n    let value_size = memory_quota * 2;\n    let (k, v) = (\"key2\".to_owned(), vec![5; value_size]);\n    // Prewrite\n    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.key = k.clone().into_bytes();\n    mutation.value = v;\n    suite.must_kv_prewrite(1, vec![mutation], k.into_bytes(), start_ts);\n    let mut events = receive_event(false).events.to_vec();\n    assert_eq!(events.len(), 1, \"{:?}\", events);\n    match events.pop().unwrap().event.unwrap() {\n        Event_oneof_event::Error(e) => {\n            // Unknown errors are translated into region_not_found.\n            assert!(e.has_region_not_found(), \"{:?}\", e);\n        }\n        other => panic!(\"unknown event {:?}\", other),\n    }\n\n    // The delegate must be removed.\n    let scheduler = suite.endpoints.values().next().unwrap().scheduler();\n    let (tx, rx) = mpsc::channel();\n    scheduler\n        .schedule(Task::Validate(Validate::Region(\n            1,\n            Box::new(move |delegate| {\n                tx.send(delegate.is_none()).unwrap();\n            }),\n        )))\n        .unwrap();\n\n    assert!(\n        rx.recv_timeout(Duration::from_millis(1000)).unwrap(),\n        \"find unexpected delegate\"\n    );\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/chunked_vec_bytes.rs::len", "code": "fn len(&self) -> usize {\n        self.length\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/cf_names.rs::default_names", "test": "fn default_names() {\n    let db = default_engine();\n    let names = db.engine.cf_names();\n    assert_eq!(names.len(), 1);\n    assert_eq!(names[0], CF_DEFAULT);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/cf_names.rs::cf_names", "test": "fn cf_names() {\n    let db = engine_cfs(ALL_CFS);\n    let names = db.engine.cf_names();\n    assert_eq!(names.len(), ALL_CFS.len());\n    for cf in ALL_CFS {\n        assert!(names.contains(cf));\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/checkpoint.rs::test_encrypted_checkpoint", "test": "fn test_encrypted_checkpoint() {\n    let dir = tempdir();\n    let root_path = dir.path();\n\n    let encryption_cfg = test_util::new_file_security_config(root_path);\n    let key_manager = Arc::new(\n        data_key_manager_from_config(&encryption_cfg, root_path.to_str().unwrap())\n            .unwrap()\n            .unwrap(),\n    );\n\n    let mut db_opts = DbOptions::default();\n    db_opts.set_key_manager(Some(key_manager));\n    let cf_opts: Vec<_> = ALL_CFS.iter().map(|cf| (*cf, CfOptions::new())).collect();\n\n    let path1 = root_path.join(\"1\").to_str().unwrap().to_owned();\n    let db1 = KvTestEngine::new_kv_engine_opt(&path1, db_opts.clone(), cf_opts.clone()).unwrap();\n    db1.put(b\"foo\", b\"bar\").unwrap();\n    db1.sync().unwrap();\n\n    let path2 = root_path.join(\"2\");\n    let mut checkpointer = db1.new_checkpointer().unwrap();\n    checkpointer.create_at(&path2, None, 0).unwrap();\n    let db2 =\n        KvTestEngine::new_kv_engine_opt(path2.to_str().unwrap(), db_opts.clone(), cf_opts.clone())\n            .unwrap();\n    assert_eq!(\n        db2.get_value_cf(CF_DEFAULT, b\"foo\").unwrap().unwrap(),\n        b\"bar\"\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/ctor.rs::new_engine_opt_renamed_dir", "test": "fn new_engine_opt_renamed_dir() {\n    use std::sync::Arc;\n    let dir = tempdir();\n    let root_path = dir.path();\n\n    let encryption_cfg = test_util::new_file_security_config(root_path);\n    let key_manager = Arc::new(\n        data_key_manager_from_config(&encryption_cfg, root_path.to_str().unwrap())\n            .unwrap()\n            .unwrap(),\n    );\n\n    let mut db_opts = DbOptions::default();\n    db_opts.set_key_manager(Some(key_manager.clone()));\n    let cf_opts: Vec<_> = ALL_CFS.iter().map(|cf| (*cf, CfOptions::new())).collect();\n\n    let path = root_path.join(\"missing\").to_str().unwrap().to_owned();\n    {\n        let db = KvTestEngine::new_kv_engine_opt(&path, db_opts.clone(), cf_opts.clone()).unwrap();\n        db.put(b\"foo\", b\"bar\").unwrap();\n        db.sync().unwrap();\n    }\n    let new_path = root_path.join(\"new\").to_str().unwrap().to_owned();\n    key_manager.link_file(&path, &new_path).unwrap();\n    fs::rename(&path, &new_path).unwrap();\n    key_manager.delete_file(&path).unwrap();\n    {\n        let db =\n            KvTestEngine::new_kv_engine_opt(&new_path, db_opts.clone(), cf_opts.clone()).unwrap();\n        assert_eq!(\n            db.get_value_cf(CF_DEFAULT, b\"foo\").unwrap().unwrap(),\n            b\"bar\"\n        );\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/misc.rs::path", "test": "fn path() {\n    let db = default_engine();\n    let path = db.tempdir.path().to_str().unwrap();\n    assert_eq!(db.engine.path(), path);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/kv/test_engine_builder.rs::path", "code": "pub fn path(mut self, path: impl AsRef<Path>) -> Self {\n        self.path = Some(path.as_ref().to_path_buf());\n        self\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/read_consistency.rs::snapshot_with_writes", "test": "fn snapshot_with_writes() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"aa\").unwrap();\n\n    let snapshot = db.engine.snapshot();\n\n    assert_eq!(snapshot.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n\n    db.engine.put(b\"b\", b\"bb\").unwrap();\n\n    assert!(snapshot.get_value(b\"b\").unwrap().is_none());\n    assert_eq!(db.engine.get_value(b\"b\").unwrap().unwrap(), b\"bb\");\n\n    db.engine.delete(b\"a\").unwrap();\n\n    assert_eq!(snapshot.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/sst.rs::delete", "test": "fn delete() -> Result<()> {\n    let tempdir = tempdir();\n    let sst_path = tempdir\n        .path()\n        .join(\"test-data.sst\")\n        .to_string_lossy()\n        .to_string();\n    let sst_builder = <KvTestEngine as SstExt>::SstWriterBuilder::new();\n    let mut sst_writer = sst_builder.build(&sst_path)?;\n\n    sst_writer.delete(b\"k1\")?;\n    sst_writer.finish()?;\n\n    let sst_reader = <KvTestEngine as SstExt>::SstReader::open(&sst_path)?;\n    let mut iter = sst_reader.iter(IterOptions::default()).unwrap();\n\n    iter.seek_to_first()?;\n\n    assert_eq!(iter.valid()?, false);\n\n    iter.prev().unwrap_err();\n    iter.next().unwrap_err();\n    recover_safe(|| {\n        iter.key();\n    })\n    .unwrap_err();\n    recover_safe(|| {\n        iter.value();\n    })\n    .unwrap_err();\n\n    assert_eq!(iter.seek_to_first()?, false);\n    assert_eq!(iter.seek_to_last()?, false);\n    assert_eq!(iter.seek(b\"foo\")?, false);\n    assert_eq!(iter.seek_for_prev(b\"foo\")?, false);\n\n    Ok(())\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_rocks/src/engine_iterator.rs::valid", "code": "fn valid(&self) -> Result<bool> {\n        self.0.valid().map_err(r2e)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/sst.rs::other_external_sst_info", "test": "fn other_external_sst_info() -> Result<()> {\n    let tempdir = tempdir();\n    let sst_path = tempdir\n        .path()\n        .join(\"test-data.sst\")\n        .to_string_lossy()\n        .to_string();\n    let sst_builder = <KvTestEngine as SstExt>::SstWriterBuilder::new();\n    let mut sst_writer = sst_builder.build(&sst_path)?;\n\n    sst_writer.put(b\"k1\", b\"v11\")?;\n    sst_writer.put(b\"k9\", b\"v9\")?;\n\n    let info = sst_writer.finish()?;\n\n    assert_eq!(b\"k1\", info.smallest_key());\n    assert_eq!(b\"k9\", info.largest_key());\n    assert_eq!(2, info.num_entries());\n\n    let size = fs::metadata(&sst_path).unwrap().len();\n\n    assert_eq!(size, info.file_size());\n\n    Ok(())\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/sst.rs::smallest_key", "code": "fn smallest_key(&self) -> &[u8] {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/sst.rs::external_sst_info_key_values_with_delete", "test": "fn external_sst_info_key_values_with_delete() -> Result<()> {\n    let tempdir = tempdir();\n    let sst_path = tempdir\n        .path()\n        .join(\"test-data.sst\")\n        .to_string_lossy()\n        .to_string();\n    let sst_builder = <KvTestEngine as SstExt>::SstWriterBuilder::new();\n    let mut sst_writer = sst_builder.build(&sst_path)?;\n\n    sst_writer.delete(b\"k1\")?;\n\n    let info = sst_writer.finish()?;\n\n    assert_eq!(b\"k1\", info.smallest_key());\n    assert_eq!(b\"k1\", info.largest_key());\n    assert_eq!(1, info.num_entries());\n\n    let size = fs::metadata(&sst_path).unwrap().len();\n\n    assert_eq!(size, info.file_size());\n\n    Ok(())\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/sst.rs::smallest_key", "code": "fn smallest_key(&self) -> &[u8] {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_put", "test": "fn write_batch_put() {\n    let db = default_engine();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n\n    let db = multi_batch_write_engine();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    for i in 0..128_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"aa\").unwrap();\n    for i in 128..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete", "test": "fn write_batch_delete() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"aa\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete(b\"a\").unwrap();\n\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n\n    let db = multi_batch_write_engine();\n\n    for i in 0..127_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n    db.engine.put(b\"a\", b\"aa\").unwrap();\n    for i in 127..255_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    for i in 0..255_usize {\n        let k = i.to_be_bytes();\n        wb.delete(&k).unwrap();\n    }\n    wb.delete(b\"a\").unwrap();\n\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n    for i in 0..255_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_none(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_write_twice_1", "test": "fn write_batch_write_twice_1() {\n    let db = default_engine();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n\n    let db = multi_batch_write_engine();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    for i in 0..123_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n    for i in 0..123_usize {\n        let x = i.to_be_bytes();\n        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_write_twice_2", "test": "fn write_batch_write_twice_2() {\n    let db = default_engine();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n\n    db.engine.put(b\"a\", b\"b\").unwrap();\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"b\");\n\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n\n    let db = multi_batch_write_engine();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    for i in 0..128_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n\n    db.engine.put(b\"a\", b\"b\").unwrap();\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"b\");\n\n    for i in 0..128_usize {\n        let k = i.to_be_bytes();\n        let v = (2 * i + 1).to_be_bytes();\n        db.engine.put(&k, &v).unwrap();\n    }\n    for i in 0..128_usize {\n        let k = i.to_be_bytes();\n        let v = (2 * i + 1).to_be_bytes();\n        assert_eq!(db.engine.get_value(&k).unwrap().unwrap(), &v);\n    }\n\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n    for i in 0..128_usize {\n        let x = i.to_be_bytes();\n        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_write_twice_3", "test": "fn write_batch_write_twice_3() {\n    let db = default_engine();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n    db.engine.put(b\"a\", b\"b\").unwrap();\n    wb.put(b\"b\", b\"bb\").unwrap();\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n    assert_eq!(db.engine.get_value(b\"b\").unwrap().unwrap(), b\"bb\");\n\n    let db = multi_batch_write_engine();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    for i in 0..128_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"aa\").unwrap();\n\n    wb.write().unwrap();\n    for i in 0..128_usize {\n        let k = i.to_be_bytes();\n        let v = (2 * i + 1).to_be_bytes();\n        db.engine.put(&k, &v).unwrap();\n    }\n    db.engine.put(b\"a\", b\"b\").unwrap();\n    for i in 128..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"b\", b\"bb\").unwrap();\n    wb.write().unwrap();\n\n    assert_eq!(db.engine.get_value(b\"a\").unwrap().unwrap(), b\"aa\");\n    assert_eq!(db.engine.get_value(b\"b\").unwrap().unwrap(), b\"bb\");\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_basic", "test": "fn write_batch_delete_range_basic() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(&32_usize.to_be_bytes(), &128_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n    for i in 0..32_usize {\n        let x = i.to_be_bytes();\n        assert!(db.engine.get_value(&x).unwrap().is_some());\n    }\n    for i in 32..128_usize {\n        let x = i.to_be_bytes();\n        assert!(db.engine.get_value(&x).unwrap().is_none());\n    }\n    for i in 128..256_usize {\n        let x = i.to_be_bytes();\n        assert!(db.engine.get_value(&x).unwrap().is_some());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_inexact", "test": "fn write_batch_delete_range_inexact() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n    db.engine.put(b\"g\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"f\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"f\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"g\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n    db.engine.put(b\"g\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    for i in (0..256_usize).step_by(2) {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n\n    wb.delete_range(b\"b\", b\"f\").unwrap();\n    wb.delete_range(&0_usize.to_be_bytes(), &252_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"f\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"g\").unwrap().is_some());\n    for i in 0..252_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n    assert!(\n        db.engine\n            .get_value(&252_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(\n        db.engine\n            .get_value(&253_usize.to_be_bytes())\n            .unwrap()\n            .is_none()\n    );\n    assert!(\n        db.engine\n            .get_value(&254_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_after_put", "test": "fn write_batch_delete_range_after_put() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.put(b\"c\", b\"\").unwrap();\n    wb.put(b\"d\", b\"\").unwrap();\n    wb.put(b\"e\", b\"\").unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.put(b\"c\", b\"\").unwrap();\n    wb.put(b\"d\", b\"\").unwrap();\n    wb.put(b\"e\", b\"\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &255_usize.to_be_bytes())\n        .unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    for i in 1..255_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n    assert!(\n        db.engine\n            .get_value(&255_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_none", "test": "fn write_batch_delete_range_none() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    for i in 1..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_twice", "test": "fn write_batch_delete_range_twice() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    for i in 1..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_twice_1", "test": "fn write_batch_delete_range_twice_1() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    for i in 1..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_twice_2", "test": "fn write_batch_delete_range_twice_2() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n    db.engine.put(b\"e\", b\"\").unwrap();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    for i in 64..128_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n    wb.delete_range(b\"b\", b\"e\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"d\").unwrap().is_none());\n    assert!(db.engine.get_value(b\"e\").unwrap().is_some());\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    for i in 1..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_empty_range", "test": "fn write_batch_delete_range_empty_range() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"b\", b\"b\").unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.delete_range(b\"b\", b\"b\").unwrap();\n    wb.delete_range(&1_usize.to_be_bytes(), &1_usize.to_be_bytes())\n        .unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_some());\n    for i in 0..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_delete_range_backward_range", "test": "fn write_batch_delete_range_backward_range() {\n    let db = default_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n\n    let mut wb = db.engine.write_batch();\n\n    wb.delete_range(b\"c\", b\"a\").unwrap();\n    recover_safe(|| {\n        wb.write().unwrap();\n    })\n    .unwrap_err();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"b\", b\"\").unwrap();\n    db.engine.put(b\"c\", b\"\").unwrap();\n\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        db.engine.put(&x, &x).unwrap();\n    }\n\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.delete_range(b\"c\", b\"a\").unwrap();\n    wb.delete_range(&256_usize.to_be_bytes(), &0_usize.to_be_bytes())\n        .unwrap();\n\n    recover_safe(|| {\n        wb.write().unwrap();\n    })\n    .unwrap_err();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"b\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"c\").unwrap().is_some());\n    for i in 0..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_is_empty", "test": "fn write_batch_is_empty() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    assert!(wb.is_empty());\n    wb.put(b\"a\", b\"\").unwrap();\n    assert!(!wb.is_empty());\n    wb.write().unwrap();\n    assert!(!wb.is_empty());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    assert!(wb.is_empty());\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    assert!(!wb.is_empty());\n    wb.write().unwrap();\n    assert!(!wb.is_empty());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.len() == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_count", "test": "fn write_batch_count() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    assert_eq!(wb.count(), 0);\n    wb.put(b\"a\", b\"\").unwrap();\n    assert_eq!(wb.count(), 1);\n    wb.write().unwrap();\n    assert_eq!(wb.count(), 1);\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    assert_eq!(wb.count(), 0);\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    assert_eq!(wb.count(), 256);\n    wb.write().unwrap();\n    assert_eq!(wb.count(), 256);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/write_batch.rs::count", "code": "fn count(&self) -> usize {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_count_2", "test": "fn write_batch_count_2() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    assert_eq!(wb.count(), 0);\n    wb.put(b\"a\", b\"\").unwrap();\n    assert_eq!(wb.count(), 1);\n    wb.delete(b\"a\").unwrap();\n    assert_eq!(wb.count(), 2);\n    wb.delete_range(b\"a\", b\"b\").unwrap();\n    assert_eq!(wb.count(), 3);\n    wb.write().unwrap();\n    assert_eq!(wb.count(), 3);\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    assert_eq!(wb.count(), 0);\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    assert_eq!(wb.count(), 257);\n    wb.delete(b\"a\").unwrap();\n    assert_eq!(wb.count(), 258);\n    wb.delete_range(b\"a\", b\"b\").unwrap();\n    assert_eq!(wb.count(), 259);\n    wb.write().unwrap();\n    assert_eq!(wb.count(), 259);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/write_batch.rs::count", "code": "fn count(&self) -> usize {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::write_batch_clear", "test": "fn write_batch_clear() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.clear();\n    assert!(wb.is_empty());\n    assert_eq!(wb.count(), 0);\n    wb.write().unwrap();\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.clear();\n    assert!(wb.is_empty());\n    assert_eq!(wb.count(), 0);\n    wb.write().unwrap();\n    for i in 0..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.len() == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::cap_zero", "test": "fn cap_zero() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch_with_cap(0);\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.put(b\"c\", b\"\").unwrap();\n    wb.put(b\"d\", b\"\").unwrap();\n    wb.put(b\"e\", b\"\").unwrap();\n    wb.put(b\"f\", b\"\").unwrap();\n    wb.write().unwrap();\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"f\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(0);\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.put(b\"c\", b\"\").unwrap();\n    wb.put(b\"d\", b\"\").unwrap();\n    wb.put(b\"e\", b\"\").unwrap();\n    wb.put(b\"f\", b\"\").unwrap();\n    wb.write().unwrap();\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(\n        db.engine\n            .get_value(&123_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(\n        db.engine\n            .get_value(&255_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"f\").unwrap().is_some());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::cap_two", "test": "fn cap_two() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch_with_cap(2);\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.put(b\"c\", b\"\").unwrap();\n    wb.put(b\"d\", b\"\").unwrap();\n    wb.put(b\"e\", b\"\").unwrap();\n    wb.put(b\"f\", b\"\").unwrap();\n    wb.write().unwrap();\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"f\").unwrap().is_some());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(2);\n\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.put(b\"c\", b\"\").unwrap();\n    wb.put(b\"d\", b\"\").unwrap();\n    wb.put(b\"e\", b\"\").unwrap();\n    wb.put(b\"f\", b\"\").unwrap();\n    wb.write().unwrap();\n    assert!(\n        db.engine\n            .get_value(&0_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(\n        db.engine\n            .get_value(&123_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(\n        db.engine\n            .get_value(&255_usize.to_be_bytes())\n            .unwrap()\n            .is_some()\n    );\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    assert!(db.engine.get_value(b\"f\").unwrap().is_some());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::should_write_to_engine", "test": "fn should_write_to_engine() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;\n\n    let mut key = vec![];\n    loop {\n        key.push(b'a');\n        wb.put(&key, b\"\").unwrap();\n        if key.len() <= max_keys {\n            assert!(!wb.should_write_to_engine());\n        }\n        if key.len() == max_keys + 1 {\n            assert!(wb.should_write_to_engine());\n            wb.write().unwrap();\n            break;\n        }\n    }\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;\n\n    let mut key = vec![];\n    loop {\n        key.push(b'a');\n        wb.put(&key, b\"\").unwrap();\n        if key.len() <= max_keys {\n            assert!(!wb.should_write_to_engine());\n        }\n        if key.len() == max_keys + 1 {\n            assert!(wb.should_write_to_engine());\n            wb.write().unwrap();\n            break;\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/write_batch.rs::should_write_to_engine", "code": "fn should_write_to_engine(&self) -> bool {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::should_write_to_engine_but_whatever", "test": "fn should_write_to_engine_but_whatever() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;\n\n    let mut key = vec![];\n    loop {\n        key.push(b'a');\n        wb.put(&key, b\"\").unwrap();\n        if key.len() <= max_keys {\n            assert!(!wb.should_write_to_engine());\n        }\n        if key.len() > max_keys {\n            assert!(wb.should_write_to_engine());\n        }\n        if key.len() == max_keys * 2 {\n            assert!(wb.should_write_to_engine());\n            wb.write().unwrap();\n            break;\n        }\n    }\n\n    let mut key = vec![];\n    loop {\n        key.push(b'a');\n        assert!(db.engine.get_value(&key).unwrap().is_some());\n        if key.len() == max_keys * 2 {\n            break;\n        }\n    }\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;\n\n    let mut key = vec![];\n\n    loop {\n        key.push(b'a');\n        wb.put(&key, b\"\").unwrap();\n        if key.len() <= max_keys {\n            assert!(!wb.should_write_to_engine());\n        }\n        if key.len() > max_keys {\n            assert!(wb.should_write_to_engine());\n        }\n        if key.len() == max_keys * 2 {\n            assert!(wb.should_write_to_engine());\n            wb.write().unwrap();\n            break;\n        }\n    }\n\n    let mut key = vec![];\n    loop {\n        key.push(b'a');\n        assert!(db.engine.get_value(&key).unwrap().is_some());\n        if key.len() == max_keys * 2 {\n            break;\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/write_batch.rs::should_write_to_engine", "code": "fn should_write_to_engine(&self) -> bool {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_rollback_one", "test": "fn save_point_rollback_one() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n\n    let err = wb.rollback_to_save_point();\n    assert_engine_error(err);\n    let err = wb.pop_save_point();\n    assert_engine_error(err);\n    wb.write().unwrap();\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.set_save_point();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n\n    let err = wb.rollback_to_save_point();\n    assert_engine_error(err);\n    let err = wb.pop_save_point();\n    assert_engine_error(err);\n    wb.write().unwrap();\n    for i in 0..256_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_none(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_rollback_two", "test": "fn save_point_rollback_two() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.set_save_point();\n    wb.put(b\"b\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n\n    let err = wb.rollback_to_save_point();\n    assert_engine_error(err);\n    let err = wb.pop_save_point();\n    assert_engine_error(err);\n    wb.write().unwrap();\n    let a = db.engine.get_value(b\"a\").unwrap();\n    assert!(a.is_none());\n    let b = db.engine.get_value(b\"b\").unwrap();\n    assert!(b.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    wb.set_save_point();\n    for i in 0..max_keys {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.set_save_point();\n    for i in max_keys..2 * max_keys {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"b\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n\n    let err = wb.rollback_to_save_point();\n    assert_engine_error(err);\n    let err = wb.pop_save_point();\n    assert_engine_error(err);\n    wb.write().unwrap();\n    let a = db.engine.get_value(b\"a\").unwrap();\n    assert!(a.is_none());\n    let b = db.engine.get_value(b\"b\").unwrap();\n    assert!(b.is_none());\n    for i in 0..2 * max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        !self.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_rollback_partial", "test": "fn save_point_rollback_partial() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.set_save_point();\n    wb.put(b\"b\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n    wb.write().unwrap();\n    let a = db.engine.get_value(b\"a\").unwrap();\n    assert!(a.is_some());\n    let b = db.engine.get_value(b\"b\").unwrap();\n    assert!(b.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    for i in 0..max_keys {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.set_save_point();\n    wb.put(b\"b\", b\"\").unwrap();\n    for i in max_keys..2 * max_keys {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n\n    wb.rollback_to_save_point().unwrap();\n    wb.write().unwrap();\n    let a = db.engine.get_value(b\"a\").unwrap();\n    assert!(a.is_some());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n    let b = db.engine.get_value(b\"b\").unwrap();\n    assert!(b.is_none());\n    for i in max_keys..2 * max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_pop_rollback", "test": "fn save_point_pop_rollback() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.pop_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n\n    let err = wb.rollback_to_save_point();\n    assert_engine_error(err);\n    let err = wb.pop_save_point();\n    assert_engine_error(err);\n    wb.write().unwrap();\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n    let val = db.engine.get_value(b\"b\").unwrap();\n    assert!(val.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n\n    wb.set_save_point();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n    wb.set_save_point();\n    for i in 0..256_usize {\n        let x = i.to_be_bytes();\n        wb.put(&x, &x).unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.pop_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n\n    let err = wb.rollback_to_save_point();\n    assert_engine_error(err);\n    let err = wb.pop_save_point();\n    assert_engine_error(err);\n    wb.write().unwrap();\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n    let val = db.engine.get_value(b\"b\").unwrap();\n    assert!(val.is_none());\n    for i in 0..512_usize {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_none(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_rollback_after_write", "test": "fn save_point_rollback_after_write() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.write().unwrap();\n\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_some());\n\n    db.engine.delete(b\"a\").unwrap();\n\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n\n    wb.rollback_to_save_point().unwrap();\n    wb.write().unwrap();\n\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    wb.set_save_point();\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n\n    db.engine.delete(b\"a\").unwrap();\n    for i in 0..max_keys {\n        db.engine.delete(&i.to_be_bytes()).unwrap();\n    }\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n\n    wb.rollback_to_save_point().unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_same_rollback_one", "test": "fn save_point_same_rollback_one() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.set_save_point();\n    wb.set_save_point();\n    wb.set_save_point();\n\n    wb.put(b\"b\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n\n    wb.write().unwrap();\n\n    let a = db.engine.get_value(b\"a\").unwrap();\n    let b = db.engine.get_value(b\"b\").unwrap();\n\n    assert!(a.is_some());\n    assert!(b.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.set_save_point();\n    wb.set_save_point();\n    wb.set_save_point();\n\n    wb.put(b\"b\", b\"\").unwrap();\n    for i in max_keys..2 * max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    wb.rollback_to_save_point().unwrap();\n\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    for i in max_keys..2 * max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_same_rollback_all", "test": "fn save_point_same_rollback_all() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.set_save_point();\n    wb.set_save_point();\n    wb.set_save_point();\n\n    wb.put(b\"b\", b\"\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n\n    assert_engine_error(wb.pop_save_point());\n    assert_engine_error(wb.rollback_to_save_point());\n\n    wb.write().unwrap();\n\n    let a = db.engine.get_value(b\"a\").unwrap();\n    let b = db.engine.get_value(b\"b\").unwrap();\n\n    assert!(a.is_some());\n    assert!(b.is_none());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.set_save_point();\n    wb.set_save_point();\n    wb.set_save_point();\n\n    wb.put(b\"b\", b\"\").unwrap();\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    wb.rollback_to_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n    wb.rollback_to_save_point().unwrap();\n\n    assert_engine_error(wb.pop_save_point());\n    assert_engine_error(wb.rollback_to_save_point());\n\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n\n    assert!(db.engine.get_value(b\"b\").unwrap().is_none());\n    for i in max_keys..2 * max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_pop_after_write", "test": "fn save_point_pop_after_write() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n\n    wb.write().unwrap();\n\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_some());\n\n    db.engine.delete(b\"a\").unwrap();\n\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_none());\n\n    wb.pop_save_point().unwrap();\n    wb.write().unwrap();\n\n    let val = db.engine.get_value(b\"a\").unwrap();\n    assert!(val.is_some());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    wb.set_save_point();\n    wb.put(b\"a\", b\"\").unwrap();\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n\n    db.engine.delete(b\"a\").unwrap();\n    for i in 0..max_keys {\n        db.engine.delete(&i.to_be_bytes()).unwrap();\n    }\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_none());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());\n    }\n\n    wb.pop_save_point().unwrap();\n    wb.write().unwrap();\n\n    assert!(db.engine.get_value(b\"a\").unwrap().is_some());\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_point_all_commands", "test": "fn save_point_all_commands() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    db.engine.put(b\"a\", b\"\").unwrap();\n    db.engine.put(b\"d\", b\"\").unwrap();\n\n    wb.set_save_point();\n    wb.delete(b\"a\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.delete_range(b\"c\", b\"e\").unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n    wb.write().unwrap();\n\n    let a = db.engine.get_value(b\"a\").unwrap();\n    let b = db.engine.get_value(b\"b\").unwrap();\n    let d = db.engine.get_value(b\"d\").unwrap();\n    assert!(a.is_some());\n    assert!(b.is_none());\n    assert!(d.is_some());\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    for i in 0..max_keys / 2 {\n        db.engine.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n    db.engine.put(b\"a\", b\"\").unwrap();\n    for i in max_keys / 2..max_keys {\n        db.engine.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n    db.engine.put(b\"d\", b\"\").unwrap();\n\n    wb.set_save_point();\n    for i in 0..max_keys / 2 {\n        wb.delete(&i.to_be_bytes()).unwrap();\n    }\n    wb.delete(b\"a\").unwrap();\n    wb.put(b\"b\", b\"\").unwrap();\n    wb.delete_range(b\"c\", b\"e\").unwrap();\n    wb.delete_range(&(max_keys / 3).to_be_bytes(), &(2 * max_keys).to_be_bytes())\n        .unwrap();\n\n    wb.rollback_to_save_point().unwrap();\n    wb.write().unwrap();\n\n    let a = db.engine.get_value(b\"a\").unwrap();\n    let b = db.engine.get_value(b\"b\").unwrap();\n    let d = db.engine.get_value(b\"d\").unwrap();\n    for i in 0..max_keys {\n        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());\n    }\n    assert!(a.is_some());\n    assert!(b.is_none());\n    assert!(d.is_some());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits_tests/src/write_batch.rs::save_points_and_counts", "test": "fn save_points_and_counts() {\n    let db = default_engine();\n    let mut wb = db.engine.write_batch();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.put(b\"a\", b\"\").unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.rollback_to_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.put(b\"a\", b\"\").unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.pop_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.clear();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.put(b\"a\", b\"\").unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.write().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.rollback_to_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.put(b\"a\", b\"\").unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.write().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.pop_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), 1);\n\n    wb.clear();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    let db = multi_batch_write_engine();\n    let mut wb = db.engine.write_batch_with_cap(1024);\n    let max_keys = 256_usize;\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.rollback_to_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.pop_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.clear();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.write().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.rollback_to_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    wb.set_save_point();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n\n    for i in 0..max_keys {\n        wb.put(&i.to_be_bytes(), b\"\").unwrap();\n    }\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.write().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.pop_save_point().unwrap();\n\n    assert_eq!(wb.is_empty(), false);\n    assert_eq!(wb.count(), max_keys);\n\n    wb.clear();\n\n    assert_eq!(wb.is_empty(), true);\n    assert_eq!(wb.count(), 0);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.len() == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/failpoints/test_basic_write.rs::test_write_batch_rollback", "test": "fn test_write_batch_rollback() {\n    let mut cluster = Cluster::default();\n    let router = &mut cluster.routers[0];\n    let header = Box::new(router.new_request_for(2).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key\", b\"value\");\n\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n    // Make several entries to batch in apply thread.\n    fail::cfg(\"APPLY_COMMITTED_ENTRIES\", \"pause\").unwrap();\n\n    // Good proposal should be committed.\n    let (msg, mut sub0) = PeerMsg::simple_write(header.clone(), put.encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub0.wait_proposed()));\n    assert!(block_on(sub0.wait_committed()));\n\n    // If the write batch is correctly initialized, next write should not contain\n    // last result.\n    put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key1\", b\"value\");\n    let (msg, mut sub1) = PeerMsg::simple_write(header.clone(), put.encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub1.wait_proposed()));\n    assert!(block_on(sub1.wait_committed()));\n\n    fail::cfg(\"APPLY_PUT\", \"1*return()\").unwrap();\n    // Wake up and sleep in next committed entry.\n    fail::remove(\"APPLY_COMMITTED_ENTRIES\");\n    // First apply will fail due to aborted. If write batch is initialized\n    // correctly, correct response can be returned.\n    let resp = block_on(sub0.result()).unwrap();\n    assert!(\n        resp.get_header()\n            .get_error()\n            .get_message()\n            .contains(\"aborted\"),\n        \"{:?}\",\n        resp\n    );\n    let resp = block_on(sub1.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n\n    let snap = router.stale_snapshot(2);\n    assert_matches!(snap.get_value(b\"key\"), Ok(None));\n    assert_eq!(snap.get_value(b\"key1\").unwrap().unwrap(), b\"value\");\n\n    fail::cfg(\"APPLY_COMMITTED_ENTRIES\", \"pause\").unwrap();\n\n    // Trigger error again, so an initialized write batch should be rolled back.\n    put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key2\", b\"value\");\n    let (msg, mut sub0) = PeerMsg::simple_write(header.clone(), put.encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub0.wait_proposed()));\n    assert!(block_on(sub0.wait_committed()));\n\n    // If the write batch is correctly rollbacked, next write should not contain\n    // last result.\n    put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key3\", b\"value\");\n    let (msg, mut sub1) = PeerMsg::simple_write(header, put.encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub1.wait_proposed()));\n    assert!(block_on(sub1.wait_committed()));\n\n    fail::cfg(\"APPLY_PUT\", \"1*return()\").unwrap();\n    fail::remove(\"APPLY_COMMITTED_ENTRIES\");\n    let resp = block_on(sub0.result()).unwrap();\n    assert!(\n        resp.get_header()\n            .get_error()\n            .get_message()\n            .contains(\"aborted\"),\n        \"{:?}\",\n        resp\n    );\n    let resp = block_on(sub1.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    let snap = router.stale_snapshot(2);\n    assert_matches!(snap.get_value(b\"key2\"), Ok(None));\n    assert_eq!(snap.get_value(b\"key3\").unwrap().unwrap(), b\"value\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/router/response_channel.rs::wait_proposed", "code": "pub async fn wait_proposed(&mut self) -> bool {\n        WaitEvent {\n            event: CmdResChannel::PROPOSED_EVENT,\n            core: &self.core,\n        }\n        .await\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/failpoints/test_bootstrap.rs::test_bootstrap_half_way_failure", "test": "fn test_bootstrap_half_way_failure() {\n    let server = test_pd::Server::new(1);\n    let eps = server.bind_addrs();\n    let pd_client = test_pd::util::new_client(eps, None);\n    let path = TempDir::new().unwrap();\n    let engines = engine_test::new_temp_engine(&path);\n    let bootstrap = || {\n        let logger = slog_global::borrow_global().new(o!());\n        let mut bootstrap = Bootstrap::new(&engines.raft, 0, &pd_client, logger);\n        match bootstrap.bootstrap_store() {\n            Ok(store_id) => {\n                let mut store = Store::default();\n                store.set_id(store_id);\n                bootstrap.bootstrap_first_region(&store, store_id)\n            }\n            Err(e) => Err(e),\n        }\n    };\n\n    // Try to start this node, return after persisted some keys.\n    fail::cfg(\"node_after_bootstrap_store\", \"return\").unwrap();\n    let s = format!(\"{}\", bootstrap().unwrap_err());\n    assert!(s.contains(\"node_after_bootstrap_store\"), \"{}\", s);\n    assert_matches!(engines.raft.get_prepare_bootstrap_region(), Ok(None));\n\n    let ident = engines.raft.get_store_ident().unwrap().unwrap();\n    assert_ne!(ident.get_store_id(), 0);\n\n    // Check whether it can bootstrap cluster successfully.\n    fail::remove(\"node_after_bootstrap_store\");\n    fail::cfg(\"node_after_prepare_bootstrap_cluster\", \"return\").unwrap();\n    let s = format!(\"{}\", bootstrap().unwrap_err());\n    assert!(s.contains(\"node_after_prepare_bootstrap_cluster\"), \"{}\", s);\n    assert_matches!(engines.raft.get_prepare_bootstrap_region(), Ok(Some(_)));\n\n    fail::remove(\"node_after_prepare_bootstrap_cluster\");\n    fail::cfg(\"node_after_bootstrap_cluster\", \"return\").unwrap();\n    let s = format!(\"{}\", bootstrap().unwrap_err());\n    assert!(s.contains(\"node_after_bootstrap_cluster\"), \"{}\", s);\n    assert_matches!(engines.raft.get_prepare_bootstrap_region(), Ok(Some(_)));\n\n    // Although aborted by error, rebootstrap should continue.\n    bootstrap().unwrap().unwrap();\n    assert_matches!(engines.raft.get_prepare_bootstrap_region(), Ok(None));\n\n    // Second bootstrap should be noop.\n    assert_eq!(bootstrap().unwrap(), None);\n\n    assert_matches!(engines.raft.get_prepare_bootstrap_region(), Ok(None));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/failpoints/test_life.rs::test_destroy_by_larger_id_while_applying", "test": "fn test_destroy_by_larger_id_while_applying() {\n    let fp = \"APPLY_COMMITTED_ENTRIES\";\n    let mut cluster = Cluster::default();\n    let router = &cluster.routers[0];\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n\n    fail::cfg(fp, \"pause\").unwrap();\n\n    let header = Box::new(router.new_request_for(2).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key\", b\"value\");\n    let (msg, mut sub) = PeerMsg::simple_write(header.clone(), put.clone().encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub.wait_committed()));\n\n    let mut larger_id_msg = Box::<RaftMessage>::default();\n    larger_id_msg.set_region_id(2);\n    let mut target_peer = header.get_peer().clone();\n    target_peer.set_id(target_peer.get_id() + 1);\n    larger_id_msg.set_to_peer(target_peer.clone());\n    larger_id_msg.set_region_epoch(header.get_region_epoch().clone());\n    larger_id_msg\n        .mut_region_epoch()\n        .set_conf_ver(header.get_region_epoch().get_conf_ver() + 1);\n    larger_id_msg.set_from_peer(new_peer(2, 8));\n    let raft_message = larger_id_msg.mut_message();\n    raft_message.set_msg_type(MessageType::MsgHeartbeat);\n    raft_message.set_from(8);\n    raft_message.set_to(target_peer.get_id());\n    raft_message.set_term(10);\n\n    // Larger ID should trigger destroy.\n    router.send_raft_message(larger_id_msg).unwrap();\n    fail::remove(fp);\n    assert_peer_not_exist(2, header.get_peer().get_id(), router);\n    let meta = router\n        .must_query_debug_info(2, Duration::from_secs(3))\n        .unwrap();\n    assert_eq!(meta.raft_status.id, target_peer.get_id());\n    assert_eq!(meta.raft_status.hard_state.term, 10);\n\n    std::thread::sleep(Duration::from_millis(10));\n\n    // New peer should survive restart.\n    cluster.restart(0);\n    let router = &cluster.routers[0];\n    let meta = router\n        .must_query_debug_info(2, Duration::from_secs(3))\n        .unwrap();\n    assert_eq!(meta.raft_status.id, target_peer.get_id());\n    assert_eq!(meta.raft_status.hard_state.term, 10);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/router/response_channel.rs::wait_committed", "code": "pub async fn wait_committed(&mut self) -> bool {\n        WaitEvent {\n            event: CmdResChannel::COMMITTED_EVENT,\n            core: &self.core,\n        }\n        .await\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_basic_write.rs::test_basic_write", "test": "fn test_basic_write() {\n    let cluster = Cluster::default();\n    let router = &cluster.routers[0];\n    let header = Box::new(router.new_request_for(2).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key\", b\"value\");\n\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n\n    // Good proposal should be committed.\n    let (msg, mut sub) = PeerMsg::simple_write(header.clone(), put.clone().encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub.wait_proposed()));\n    assert!(block_on(sub.wait_committed()));\n    let resp = block_on(sub.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n\n    // Store id should be checked.\n    let mut invalid_header = header.clone();\n    invalid_header.set_peer(new_peer(3, 3));\n    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();\n    assert!(\n        resp.get_header().get_error().has_store_not_match(),\n        \"{:?}\",\n        resp\n    );\n\n    // Peer id should be checked.\n    invalid_header = header.clone();\n    invalid_header.set_peer(new_peer(1, 1));\n    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();\n    assert!(resp.get_header().has_error(), \"{:?}\", resp);\n\n    // Epoch should be checked.\n    invalid_header = header.clone();\n    invalid_header\n        .mut_region_epoch()\n        .set_version(INIT_EPOCH_VER - 1);\n    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();\n    assert!(\n        resp.get_header().get_error().has_epoch_not_match(),\n        \"{:?}\",\n        resp\n    );\n\n    // Term should be checked if set.\n    invalid_header = header.clone();\n    invalid_header.set_term(1);\n    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();\n    assert!(\n        resp.get_header().get_error().has_stale_command(),\n        \"{:?}\",\n        resp\n    );\n\n    // Too large message can cause regression and should be rejected.\n    let mut invalid_put = SimpleWriteEncoder::with_capacity(9 * 1024 * 1024);\n    invalid_put.put(CF_DEFAULT, b\"key\", &vec![0; 8 * 1024 * 1024]);\n    let resp = router.simple_write(2, header.clone(), invalid_put).unwrap();\n    assert!(\n        resp.get_header().get_error().has_raft_entry_too_large(),\n        \"{:?}\",\n        resp\n    );\n\n    // Make it step down and follower should reject write.\n    let mut msg = Box::<RaftMessage>::default();\n    msg.set_region_id(2);\n    msg.set_to_peer(new_peer(1, 3));\n    msg.mut_region_epoch().set_conf_ver(INIT_EPOCH_CONF_VER);\n    msg.set_from_peer(new_peer(2, 4));\n    let raft_message = msg.mut_message();\n    raft_message.set_msg_type(raft::prelude::MessageType::MsgHeartbeat);\n    raft_message.set_from(4);\n    raft_message.set_term(8);\n    router.send_raft_message(msg).unwrap();\n    let resp = router.simple_write(2, header, put).unwrap();\n    assert!(resp.get_header().get_error().has_not_leader(), \"{:?}\", resp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/router/response_channel.rs::wait_proposed", "code": "pub async fn wait_proposed(&mut self) -> bool {\n        WaitEvent {\n            event: CmdResChannel::PROPOSED_EVENT,\n            core: &self.core,\n        }\n        .await\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_basic_write.rs::test_put_delete", "test": "fn test_put_delete() {\n    let mut cluster = Cluster::default();\n    let router = &mut cluster.routers[0];\n    let header = Box::new(router.new_request_for(2).take_header());\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key\", b\"value\");\n\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n\n    let snap = router.stale_snapshot(2);\n    assert!(snap.get_value(b\"key\").unwrap().is_none());\n    let (msg, mut sub) = PeerMsg::simple_write(header.clone(), put.encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub.wait_proposed()));\n    assert!(block_on(sub.wait_committed()));\n    let resp = block_on(sub.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    let snap = router.stale_snapshot(2);\n    assert_eq!(snap.get_value(b\"key\").unwrap().unwrap(), b\"value\");\n\n    let mut delete = SimpleWriteEncoder::with_capacity(64);\n    delete.delete(CF_DEFAULT, b\"key\");\n    let (msg, mut sub) = PeerMsg::simple_write(header, delete.encode());\n    router.send(2, msg).unwrap();\n    assert!(block_on(sub.wait_proposed()));\n    assert!(block_on(sub.wait_committed()));\n    let resp = block_on(sub.result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    let snap = router.stale_snapshot(2);\n    assert_matches!(snap.get_value(b\"key\"), Ok(None));\n\n    // Check if WAL is skipped for basic writes.\n    let mut cached = cluster.node(0).tablet_registry().get(2).unwrap();\n    check_skip_wal(cached.latest().unwrap().as_inner().path());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        !self.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_pd_heartbeat.rs::test_region_heartbeat", "test": "fn test_region_heartbeat() {\n    let region_id = 2;\n    let cluster = Cluster::with_node_count(1, None);\n    let router = &cluster.routers[0];\n\n    // When there is only one peer, it should campaign immediately.\n    let mut req = RaftCmdRequest::default();\n    req.mut_header().set_peer(new_peer(1, 3));\n    req.mut_status_request()\n        .set_cmd_type(StatusCmdType::RegionLeader);\n    let res = router.query(region_id, req.clone()).unwrap();\n    let status_resp = res.response().unwrap().get_status_response();\n    assert_eq!(\n        *status_resp.get_region_leader().get_leader(),\n        new_peer(1, 3)\n    );\n\n    for _ in 0..5 {\n        let resp = block_on(\n            cluster\n                .node(0)\n                .pd_client()\n                .get_region_leader_by_id(region_id),\n        )\n        .unwrap();\n        if let Some((region, peer)) = resp {\n            assert_eq!(region.get_id(), region_id);\n            assert_eq!(peer.get_id(), 3);\n            assert_eq!(peer.get_store_id(), 1);\n            return;\n        }\n        std::thread::sleep(std::time::Duration::from_millis(50));\n    }\n    panic!(\"failed to get region leader\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client_v2.rs::get_leader", "code": "pub fn get_leader(&mut self) -> pdpb::Member {\n        block_on(self.raw_client.wait_for_ready()).unwrap();\n        self.raw_client.leader()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_pd_heartbeat.rs::test_report_buckets", "test": "fn test_report_buckets() {\n    let region_id = 2;\n    let mut cop_cfg = CopConfig::default();\n    cop_cfg.enable_region_bucket = Some(true);\n    cop_cfg.region_bucket_size = ReadableSize::kb(1);\n    let mut config = v2_default_config();\n    config.region_split_check_diff = Some(ReadableSize::kb(1));\n    let cluster = Cluster::with_cop_cfg(Some(config), cop_cfg);\n    let store_id = cluster.node(0).id();\n    let router = &cluster.routers[0];\n\n    // When there is only one peer, it should campaign immediately.\n    let mut req = RaftCmdRequest::default();\n    req.mut_header().set_peer(new_peer(store_id, 3));\n    req.mut_status_request()\n        .set_cmd_type(StatusCmdType::RegionLeader);\n    let res = router.query(region_id, req.clone()).unwrap();\n    let status_resp = res.response().unwrap().get_status_response();\n    assert_eq!(\n        *status_resp.get_region_leader().get_leader(),\n        new_peer(store_id, 3)\n    );\n    router.wait_applied_to_current_term(region_id, Duration::from_secs(3));\n\n    // load data to split bucket.\n    let mut suffix = String::from(\"\");\n    for _ in 0..200 {\n        suffix.push_str(\"fake \");\n    }\n\n    let repeat: u64 = 10;\n    let bytes = write_keys(&cluster, region_id, &suffix, repeat.try_into().unwrap());\n    // To find the split keys, it should flush memtable manually.\n    let mut cached = cluster.node(0).tablet_registry().get(region_id).unwrap();\n    cached.latest().unwrap().flush_cf(CF_DEFAULT, true).unwrap();\n    // send split region check to split bucket.\n    router\n        .send(region_id, PeerMsg::Tick(PeerTick::SplitRegionCheck))\n        .unwrap();\n    std::thread::sleep(std::time::Duration::from_millis(50));\n    // report buckets to pd.\n    router\n        .send(region_id, PeerMsg::Tick(PeerTick::ReportBuckets))\n        .unwrap();\n    std::thread::sleep(std::time::Duration::from_millis(50));\n\n    let resp = block_on(cluster.node(0).pd_client().get_buckets_by_id(region_id)).unwrap();\n    let mut buckets_tmp = vec![];\n    let mut bucket_ranges = vec![];\n    if let Some(buckets) = resp {\n        assert!(buckets.get_keys().len() > 2);\n        assert_eq!(buckets.get_region_id(), region_id);\n        let write_bytes = buckets.get_stats().get_write_bytes();\n        let write_keys = buckets.get_stats().get_write_keys();\n        for i in 0..buckets.keys.len() - 1 {\n            assert!(write_bytes[i] >= bytes);\n            assert!(write_keys[i] >= repeat);\n        }\n        for i in 0..buckets.keys.len() - 1 {\n            buckets_tmp.push(raftstore::store::Bucket::default());\n            let bucket_range =\n                raftstore::store::BucketRange(buckets.keys[i].clone(), buckets.keys[i + 1].clone());\n            bucket_ranges.push(bucket_range);\n        }\n    }\n\n    // report buckets to pd again, the write bytes and keys should be zero.\n    router\n        .send(region_id, PeerMsg::Tick(PeerTick::ReportBuckets))\n        .unwrap();\n    std::thread::sleep(std::time::Duration::from_millis(50));\n\n    let resp = block_on(cluster.node(0).pd_client().get_buckets_by_id(region_id)).unwrap();\n    if let Some(buckets) = resp {\n        assert_eq!(buckets.get_region_id(), region_id);\n        let write_bytes = buckets.get_stats().get_write_bytes();\n        let write_keys = buckets.get_stats().get_write_keys();\n        for i in 0..buckets.keys.len() - 1 {\n            assert!(write_bytes[i] == 0);\n            assert!(write_keys[i] == 0);\n        }\n    }\n\n    // send the same region buckets to refresh which needs to merge the last.\n    let resp = block_on(cluster.node(0).pd_client().get_region_by_id(region_id)).unwrap();\n    if let Some(region) = resp {\n        let region_epoch = region.get_region_epoch().clone();\n        for _ in 0..2 {\n            let msg = PeerMsg::RefreshRegionBuckets {\n                region_epoch: region_epoch.clone(),\n                buckets: buckets_tmp.clone(),\n                bucket_ranges: Some(bucket_ranges.clone()),\n            };\n            router.send(region_id, msg).unwrap();\n            std::thread::sleep(std::time::Duration::from_millis(50));\n        }\n    }\n    // report buckets to pd again, the write bytes and keys should be zero.\n    router\n        .send(region_id, PeerMsg::Tick(PeerTick::ReportBuckets))\n        .unwrap();\n    std::thread::sleep(std::time::Duration::from_millis(50));\n\n    let resp = block_on(cluster.node(0).pd_client().get_buckets_by_id(region_id)).unwrap();\n    if let Some(buckets) = resp {\n        assert_eq!(buckets.get_region_id(), region_id);\n        let write_bytes = buckets.get_stats().get_write_bytes();\n        let write_keys = buckets.get_stats().get_write_keys();\n        assert_eq!(write_bytes.len(), 1);\n        assert_eq!(write_keys.len(), 1);\n    }\n\n    fn write_keys(cluster: &Cluster, region_id: u64, suffix: &str, repeat: usize) -> u64 {\n        let router = &cluster.routers[0];\n        let header = Box::new(router.new_request_for(region_id).take_header());\n        for i in 0..repeat {\n            let mut put = SimpleWriteEncoder::with_capacity(64);\n            let mut key = format!(\"key-{}\", i);\n            key.push_str(suffix);\n            put.put(CF_DEFAULT, key.as_bytes(), b\"value\");\n            let (msg, sub) = PeerMsg::simple_write(header.clone(), put.clone().encode());\n            router.send(region_id, msg).unwrap();\n            let _resp = block_on(sub.result()).unwrap();\n        }\n        ((suffix.as_bytes().len() + 10) * repeat)\n            .try_into()\n            .unwrap()\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client_v2.rs::get_leader", "code": "pub fn get_leader(&mut self) -> pdpb::Member {\n        block_on(self.raw_client.wait_for_ready()).unwrap();\n        self.raw_client.leader()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_read.rs::test_query_with_write_cmd", "test": "fn test_query_with_write_cmd() {\n    let cluster = Cluster::default();\n    let router = &cluster.routers[0];\n    std::thread::sleep(std::time::Duration::from_millis(200));\n    let region_id = 2;\n    let mut req = router.new_request_for(2);\n\n    for write_cmd in [\n        CmdType::Prewrite,\n        CmdType::Delete,\n        CmdType::DeleteRange,\n        CmdType::Put,\n        CmdType::IngestSst,\n    ] {\n        let mut request_inner = Request::default();\n        request_inner.set_cmd_type(write_cmd);\n        req.mut_requests().push(request_inner);\n        let res = router.query(region_id, req.clone()).unwrap();\n        let resp = res.read();\n        assert!(resp.is_none());\n        let error_resp = res.response().unwrap();\n        assert!(error_resp.get_header().has_error());\n        req.clear_requests();\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/router/response_channel.rs::is_none", "code": "fn is_none(&self) -> bool {\n        false\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_status.rs::test_status", "test": "fn test_status() {\n    let cluster = Cluster::default();\n    let router = &cluster.routers[0];\n    // When there is only one peer, it should campaign immediately.\n    let mut req = RaftCmdRequest::default();\n    req.mut_header().set_peer(new_peer(1, 3));\n    req.mut_status_request()\n        .set_cmd_type(StatusCmdType::RegionLeader);\n    let res = router.query(2, req.clone()).unwrap();\n    let status_resp = res.response().unwrap().get_status_response();\n    assert_eq!(\n        *status_resp.get_region_leader().get_leader(),\n        new_peer(1, 3)\n    );\n\n    req.mut_status_request()\n        .set_cmd_type(StatusCmdType::RegionDetail);\n    let res = router.query(2, req.clone()).unwrap();\n    let status_resp = res.response().unwrap().get_status_response();\n    let detail = status_resp.get_region_detail();\n    assert_eq!(*detail.get_leader(), new_peer(1, 3));\n    let region = detail.get_region();\n    assert_eq!(region.get_id(), 2);\n    assert!(region.get_start_key().is_empty());\n    assert!(region.get_end_key().is_empty());\n    assert_eq!(*region.get_peers(), vec![new_peer(1, 3)]);\n    assert_eq!(region.get_region_epoch().get_version(), 1);\n    assert_eq!(region.get_region_epoch().get_conf_ver(), 1);\n\n    // Invalid store id should return error.\n    req.mut_header().mut_peer().set_store_id(4);\n    let res = router.query(2, req).unwrap();\n    let resp = res.response().unwrap();\n    assert!(\n        resp.get_header().get_error().has_store_not_match(),\n        \"{:?}\",\n        resp\n    );\n\n    // TODO: add a peer then check for region change and leadership change.\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client_v2.rs::get_leader", "code": "pub fn get_leader(&mut self) -> pdpb::Member {\n        block_on(self.raw_client.wait_for_ready()).unwrap();\n        self.raw_client.leader()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_trace_apply.rs::test_data_recovery", "test": "fn test_data_recovery() {\n    let mut cluster = Cluster::default();\n    let registry = cluster.node(0).tablet_registry();\n    let tablet_2_path = registry.tablet_path(2, RAFT_INIT_LOG_INDEX);\n    // The rocksdb is a bootstrapped tablet, so it will be opened and closed in\n    // bootstrap, and then open again in fsm initialization.\n    assert_eq!(count_info_log(&tablet_2_path), 2);\n    let router = &mut cluster.routers[0];\n    router.wait_applied_to_current_term(2, Duration::from_secs(3));\n\n    // Write 100 keys to default CF and not flush.\n    let header = Box::new(router.new_request_for(2).take_header());\n    for i in 0..100 {\n        let mut put = SimpleWriteEncoder::with_capacity(64);\n        put.put(\n            CF_DEFAULT,\n            format!(\"key{}\", i).as_bytes(),\n            format!(\"value{}\", i).as_bytes(),\n        );\n        router\n            .send(2, PeerMsg::simple_write(header.clone(), put.encode()).0)\n            .unwrap();\n    }\n\n    // Write 100 keys to write CF and flush half.\n    let mut sub = None;\n    for i in 0..50 {\n        let mut put = SimpleWriteEncoder::with_capacity(64);\n        put.put(\n            CF_WRITE,\n            format!(\"key{}\", i).as_bytes(),\n            format!(\"value{}\", i).as_bytes(),\n        );\n        let (msg, s) = PeerMsg::simple_write(header.clone(), put.encode());\n        router.send(2, msg).unwrap();\n        sub = Some(s);\n    }\n    let resp = block_on(sub.take().unwrap().result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n\n    let mut cached = cluster.node(0).tablet_registry().get(2).unwrap();\n    cached.latest().unwrap().flush_cf(CF_WRITE, true).unwrap();\n    let router = &mut cluster.routers[0];\n    for i in 50..100 {\n        let mut put = SimpleWriteEncoder::with_capacity(64);\n        put.put(\n            CF_WRITE,\n            format!(\"key{}\", i).as_bytes(),\n            format!(\"value{}\", i).as_bytes(),\n        );\n        router\n            .send(2, PeerMsg::simple_write(header.clone(), put.encode()).0)\n            .unwrap();\n    }\n\n    // Write 100 keys to lock CF and flush all.\n    for i in 0..100 {\n        let mut put = SimpleWriteEncoder::with_capacity(64);\n        put.put(\n            CF_LOCK,\n            format!(\"key{}\", i).as_bytes(),\n            format!(\"value{}\", i).as_bytes(),\n        );\n        let (msg, s) = PeerMsg::simple_write(header.clone(), put.encode());\n        router.send(2, msg).unwrap();\n        sub = Some(s);\n    }\n    let resp = block_on(sub.take().unwrap().result()).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n\n    cached = cluster.node(0).tablet_registry().get(2).unwrap();\n    cached.latest().unwrap().flush_cf(CF_LOCK, true).unwrap();\n\n    // Make sure all keys must be written.\n    let router = &mut cluster.routers[0];\n    let snap = router.stale_snapshot(2);\n    for cf in DATA_CFS {\n        for i in 0..100 {\n            let key = format!(\"key{}\", i);\n            let value = snap.get_value_cf(cf, key.as_bytes()).unwrap();\n            assert_eq!(\n                value.as_deref(),\n                Some(format!(\"value{}\", i).as_bytes()),\n                \"{} {}\",\n                cf,\n                key\n            );\n        }\n    }\n    let registry = cluster.node(0).tablet_registry();\n    cached = registry.get(2).unwrap();\n    cached\n        .latest()\n        .unwrap()\n        .set_db_options(&[(\"avoid_flush_during_shutdown\", \"true\")])\n        .unwrap();\n    drop((snap, cached));\n\n    cluster.restart(0);\n\n    let registry = cluster.node(0).tablet_registry();\n    cached = registry.get(2).unwrap();\n    cached\n        .latest()\n        .unwrap()\n        .set_db_options(&[(\"avoid_flush_during_shutdown\", \"true\")])\n        .unwrap();\n    let router = &mut cluster.routers[0];\n\n    // Write another key to ensure all data are recovered.\n    let mut put = SimpleWriteEncoder::with_capacity(64);\n    put.put(CF_DEFAULT, b\"key101\", b\"value101\");\n    let resp = router.simple_write(2, header, put).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n\n    // After being restarted, all unflushed logs should be applied again. So there\n    // should be no missing data.\n    let snap = router.stale_snapshot(2);\n    for cf in DATA_CFS {\n        for i in 0..100 {\n            let key = format!(\"key{}\", i);\n            let value = snap.get_value_cf(cf, key.as_bytes()).unwrap();\n            assert_eq!(\n                value.as_deref(),\n                Some(format!(\"value{}\", i).as_bytes()),\n                \"{} {}\",\n                cf,\n                key\n            );\n        }\n    }\n\n    // There is a restart, so LOG file should be rotate.\n    assert_eq!(count_info_log(&tablet_2_path), 3);\n    // We only trigger Flush twice, so there should be only 2 files. And because WAL\n    // is disabled, so when rocksdb is restarted, there should be no WAL to recover,\n    // so no additional flush will be triggered.\n    assert_eq!(count_sst(&tablet_2_path), 2);\n\n    cached = cluster.node(0).tablet_registry().get(2).unwrap();\n    cached.latest().unwrap().flush_cfs(DATA_CFS, true).unwrap();\n\n    // Although all CFs are triggered again, but recovery should only write:\n    // 1. [0, 101) to CF_DEFAULT\n    // 2. [50, 100) to CF_WRITE\n    //\n    // So there will be only 2 memtables to be flushed.\n    assert_eq!(count_sst(&tablet_2_path), 4);\n\n    drop((snap, cached));\n\n    cluster.restart(0);\n\n    let router = &mut cluster.routers[0];\n\n    assert_eq!(count_info_log(&tablet_2_path), 4);\n    // Because data is flushed before restarted, so all data can be read\n    // immediately.\n    let snap = router.stale_snapshot(2);\n    for cf in DATA_CFS {\n        for i in 0..100 {\n            let key = format!(\"key{}\", i);\n            let value = snap.get_value_cf(cf, key.as_bytes()).unwrap();\n            assert_eq!(\n                value.as_deref(),\n                Some(format!(\"value{}\", i).as_bytes()),\n                \"{} {}\",\n                cf,\n                key\n            );\n        }\n    }\n    // Trigger flush again.\n    cached = cluster.node(0).tablet_registry().get(2).unwrap();\n    cached.latest().unwrap().flush_cfs(DATA_CFS, true).unwrap();\n\n    // There is no recovery, so there should be nothing to flush.\n    assert_eq!(count_sst(&tablet_2_path), 4);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/tests/integrations/test_trace_apply.rs::count_info_log", "code": "fn count_info_log(path: &Path) -> usize {\n    count_file(path, |path| {\n        path.file_name()\n            .unwrap()\n            .to_string_lossy()\n            .starts_with(\"LOG\")\n    })\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/components/resource_metering/tests/summary_test.rs::test_summary", "test": "fn test_summary() {\n    let cfg = Config {\n        report_receiver_interval: ReadableDuration::millis(REPORT_INTERVAL_MS),\n        precision: ReadableDuration::millis(PRECISION_MS),\n        ..Default::default()\n    };\n\n    let (_, collector_reg_handle, resource_tag_factory, mut recorder_worker) =\n        init_recorder(cfg.precision.as_millis());\n    let (_, data_sink_reg_handle, mut reporter_worker) = init_reporter(cfg, collector_reg_handle);\n\n    let data_sink = MockDataSink::default();\n\n    // At this point we are ready for everything except turning on the switch.\n\n    // expect no data\n    {\n        let tf = resource_tag_factory.clone();\n        let data_sink = data_sink.clone();\n        thread::spawn(move || {\n            {\n                let mut ctx = Context::default();\n                ctx.set_resource_group_tag(b\"TAG-1\".to_vec());\n                let tag = tf.new_tag(&ctx);\n                let _g = tag.attach();\n                resource_metering::record_read_keys(123);\n                resource_metering::record_write_keys(456);\n            }\n            thread::sleep(Duration::from_millis(REPORT_INTERVAL_MS + 500)); // wait report\n            assert!(data_sink.get(b\"TAG-1\").is_none());\n            data_sink.clear();\n        })\n        .join()\n        .unwrap();\n    }\n\n    // turn on\n    let reg_guard = data_sink_reg_handle.register(Box::new(data_sink.clone()));\n\n    // expect can get data\n    {\n        let tf = resource_tag_factory.clone();\n        let data_sink = data_sink.clone();\n        thread::spawn(move || {\n            {\n                let mut ctx = Context::default();\n                ctx.set_resource_group_tag(b\"TAG-1\".to_vec());\n                let tag = tf.new_tag(&ctx);\n                let _g = tag.attach();\n                thread::sleep(Duration::from_millis(PRECISION_MS * 2)); // wait config apply\n                resource_metering::record_read_keys(123);\n                resource_metering::record_write_keys(456);\n            }\n            thread::sleep(Duration::from_millis(REPORT_INTERVAL_MS + 500)); // wait report\n\n            let r = data_sink.get(b\"TAG-1\").unwrap();\n            assert_eq!(\n                r.get_record()\n                    .get_items()\n                    .iter()\n                    .map(|item| item.read_keys)\n                    .sum::<u32>(),\n                123\n            );\n            assert_eq!(\n                r.get_record()\n                    .get_items()\n                    .iter()\n                    .map(|item| item.write_keys)\n                    .sum::<u32>(),\n                456\n            );\n            data_sink.clear();\n        })\n        .join()\n        .unwrap();\n    }\n\n    // turn off\n    drop(reg_guard);\n\n    // expect no data\n    thread::spawn(move || {\n        {\n            let mut ctx = Context::default();\n            ctx.set_resource_group_tag(b\"TAG-1\".to_vec());\n            let tag = resource_tag_factory.new_tag(&ctx);\n            let _g = tag.attach();\n            thread::sleep(Duration::from_millis(PRECISION_MS * 2)); // wait config apply\n            resource_metering::record_read_keys(123);\n            resource_metering::record_write_keys(456);\n        }\n        thread::sleep(Duration::from_millis(REPORT_INTERVAL_MS + 500)); // wait report\n        assert!(data_sink.get(b\"TAG-1\").is_none());\n        data_sink.clear();\n    })\n    .join()\n    .unwrap();\n\n    // stop worker\n    recorder_worker.stop();\n    reporter_worker.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_none(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_cmd_epoch_checker.rs::test_reject_proposal_during_leader_transfer", "test": "fn test_reject_proposal_during_leader_transfer() {\n    let mut cluster = new_node_cluster(0, 2);\n    let pd_client = cluster.pd_client.clone();\n    pd_client.disable_default_operator();\n    let r = cluster.run_conf_change();\n    pd_client.must_add_peer(r, new_peer(2, 2));\n\n    // Don't allow leader transfer succeed if it is actually triggered.\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(r, 2)\n            .msg_type(MessageType::MsgTimeoutNow)\n            .direction(Direction::Recv),\n    ));\n\n    cluster.must_put(b\"k\", b\"v\");\n    cluster.transfer_leader(r, new_peer(2, 2));\n    // The leader can't change to transferring state immediately due to\n    // pre-transfer-leader feature, so wait for a while.\n    sleep_ms(100);\n    assert_ne!(cluster.leader_of_region(r).unwrap(), new_peer(2, 2));\n\n    let force_delay_propose_batch_raft_command_fp = \"force_delay_propose_batch_raft_command\";\n    for i in 0..2 {\n        if i == 1 {\n            // Test another path of calling proposed callback.\n            fail::cfg(force_delay_propose_batch_raft_command_fp, \"2*return\").unwrap();\n        }\n        let write_req = make_write_req(&mut cluster, b\"k\");\n        let (cb, mut cb_receivers) = make_cb(&write_req);\n        cluster\n            .sim\n            .rl()\n            .async_command_on_node(1, write_req, cb)\n            .unwrap();\n        cb_receivers.assert_err();\n    }\n\n    cluster.clear_send_filters();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::leader_of_region", "code": "pub fn leader_of_region(&mut self, region_id: u64) -> Option<metapb::Peer> {\n        let timer = Instant::now_coarse();\n        let timeout = Duration::from_secs(5);\n        let mut store_ids = None;\n        while timer.saturating_elapsed() < timeout {\n            match self.voter_store_ids_of_region(region_id) {\n                None => thread::sleep(Duration::from_millis(10)),\n                Some(ids) => {\n                    store_ids = Some(ids);\n                    break;\n                }\n            }\n        }\n        let store_ids = store_ids?;\n        if let Some(l) = self.leaders.get(&region_id) {\n            // leader may be stopped in some tests.\n            if self.valid_leader_id(region_id, l.get_store_id()) {\n                return Some(l.clone());\n            }\n        }\n        self.reset_leader_of_region(region_id);\n        let mut leader = None;\n        let mut leaders = HashMap::default();\n\n        let node_ids = self.sim.rl().get_node_ids();\n        // For some tests, we stop the node but pd still has this information,\n        // and we must skip this.\n        let alive_store_ids: Vec<_> = store_ids\n            .iter()\n            .filter(|id| node_ids.contains(id))\n            .cloned()\n            .collect();\n        while timer.saturating_elapsed() < timeout {\n            for store_id in &alive_store_ids {\n                let l = match self.query_leader(*store_id, region_id, Duration::from_secs(1)) {\n                    None => continue,\n                    Some(l) => l,\n                };\n                leaders\n                    .entry(l.get_id())\n                    .or_insert((l, vec![]))\n                    .1\n                    .push(*store_id);\n            }\n            if let Some((_, (l, c))) = leaders.iter().max_by_key(|(_, (_, c))| c.len()) {\n                if c.contains(&l.get_store_id()) {\n                    leader = Some(l.clone());\n                    // Technically, correct calculation should use two quorum when in joint\n                    // state. Here just for simplicity.\n                    if c.len() > store_ids.len() / 2 {\n                        break;\n                    }\n                }\n            }\n            debug!(\"failed to detect leaders\"; \"leaders\" => ?leaders, \"store_ids\" => ?store_ids);\n            sleep_ms(10);\n            leaders.clear();\n        }\n\n        if let Some(l) = leader {\n            self.leaders.insert(region_id, l);\n        }\n\n        self.leaders.get(&region_id).cloned()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_conf_change.rs::test_write_after_destroy", "test": "fn test_write_after_destroy() {\n    // 3 nodes cluster.\n    let mut cluster = new_server_cluster(0, 3);\n\n    let pd_client = cluster.pd_client.clone();\n    // Disable default max peer count check.\n    pd_client.disable_default_operator();\n\n    let r1 = cluster.run_conf_change();\n\n    // Now region 1 only has peer (1, 1);\n    let (key, value) = (b\"k1\", b\"v1\");\n\n    cluster.must_put(key, value);\n    assert_eq!(cluster.get(key), Some(value.to_vec()));\n\n    // add peer (2,2) to region 1.\n    pd_client.must_add_peer(r1, new_peer(2, 2));\n\n    // add peer (3, 3) to region 1.\n    pd_client.must_add_peer(r1, new_peer(3, 3));\n    let engine_3 = cluster.get_engine(3);\n    must_get_equal(&engine_3, b\"k1\", b\"v1\");\n\n    let apply_fp = \"apply_on_conf_change_1_3_1\";\n    fail::cfg(apply_fp, \"pause\").unwrap();\n\n    cluster.must_transfer_leader(r1, new_peer(1, 1));\n    let conf_change = new_change_peer_request(ConfChangeType::RemoveNode, new_peer(3, 3));\n    let mut epoch = cluster.pd_client.get_region_epoch(r1);\n    let mut admin_req = new_admin_request(r1, &epoch, conf_change);\n    admin_req.mut_header().set_peer(new_peer(1, 1));\n    let (cb1, mut rx1) = make_cb(&admin_req);\n    let engines_3 = cluster.get_all_engines(3);\n    let region = block_on(cluster.pd_client.get_region_by_id(r1))\n        .unwrap()\n        .unwrap();\n    let reqs = vec![new_put_cmd(b\"k5\", b\"v5\")];\n    let new_version = epoch.get_conf_ver() + 1;\n    epoch.set_conf_ver(new_version);\n    let mut put = new_request(r1, epoch, reqs, false);\n    put.mut_header().set_peer(new_peer(1, 1));\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, admin_req, cb1)\n        .unwrap();\n    for _ in 0..100 {\n        let (cb2, _rx2) = make_cb(&put);\n        cluster\n            .sim\n            .rl()\n            .async_command_on_node(1, put.clone(), cb2)\n            .unwrap();\n    }\n    let engine_2 = cluster.get_engine(2);\n    must_get_equal(&engine_2, b\"k5\", b\"v5\");\n    fail::remove(apply_fp);\n    let resp = rx1.recv_timeout(Duration::from_secs(2)).unwrap();\n    assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    std::thread::sleep(Duration::from_secs(3));\n    must_get_none(&engine_3, b\"k5\");\n    must_region_cleared(&engines_3, &region);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::get", "code": "pub fn get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, false)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_deadline", "test": "fn test_deadline() {\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"deadline_check_fail\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_other_error().contains(\"exceeding the deadline\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_deadline_2", "test": "fn test_deadline_2() {\n    // It should not even take any snapshots when request is outdated from the\n    // beginning.\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"rockskv_async_snapshot\", \"panic\").unwrap();\n    fail::cfg(\"deadline_check_fail\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_other_error().contains(\"exceeding the deadline\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_deadline_3", "test": "fn test_deadline_3() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = {\n        let engine = tikv::storage::TestEngineBuilder::new().build().unwrap();\n        let cfg = tikv::server::Config {\n            end_point_request_max_handle_duration: tikv_util::config::ReadableDuration::secs(1),\n            ..Default::default()\n        };\n        init_data_with_details(Context::default(), engine, &product, &data, true, &cfg)\n    };\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"kv_cursor_seek\", \"sleep(2000)\").unwrap();\n    fail::cfg(\"copr_batch_initial_size\", \"return(1)\").unwrap();\n    let cop_resp = handle_request(&endpoint, req);\n    let mut resp = SelectResponse::default();\n    resp.merge_from_bytes(cop_resp.get_data()).unwrap();\n\n    assert!(\n        cop_resp.other_error.contains(\"exceeding the deadline\")\n            || resp\n                .get_error()\n                .get_msg()\n                .contains(\"exceeding the deadline\")\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_parse_request_failed", "test": "fn test_parse_request_failed() {\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"coprocessor_parse_request\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_other_error().contains(\"unsupported tp\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_parse_request_failed_2", "test": "fn test_parse_request_failed_2() {\n    // It should not even take any snapshots when parse failed.\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"rockskv_async_snapshot\", \"panic\").unwrap();\n    fail::cfg(\"coprocessor_parse_request\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_other_error().contains(\"unsupported tp\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_snapshot_failed", "test": "fn test_snapshot_failed() {\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &[]);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"rockskv_async_snapshot\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_other_error().contains(\"snapshot failed\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_storage_error", "test": "fn test_storage_error() {\n    let data = vec![(1, Some(\"name:0\"), 2), (2, Some(\"name:4\"), 3)];\n\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &data);\n    let req = DagSelect::from(&product).build();\n\n    fail::cfg(\"kv_cursor_seek\", \"return()\").unwrap();\n    let resp = handle_request(&endpoint, req);\n\n    assert!(resp.get_other_error().contains(\"kv cursor seek error\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_coprocessor.rs::test_region_error_in_scan", "test": "fn test_region_error_in_scan() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_cluster, raft_engine, mut ctx) = new_raft_engine(1, \"\");\n    ctx.set_isolation_level(IsolationLevel::Si);\n\n    let (_, endpoint, _) =\n        init_data_with_engine_and_commit(ctx.clone(), raft_engine, &product, &data, true);\n\n    fail::cfg(\"region_snapshot_seek\", \"return()\").unwrap();\n    let req = DagSelect::from(&product).build_with(ctx, &[0]);\n    let resp = handle_request(&endpoint, req);\n\n    assert!(\n        resp.get_region_error()\n            .get_message()\n            .contains(\"region seek error\")\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_disk_full.rs::test_majority_disk_full", "test": "fn test_majority_disk_full() {\n    let mut cluster = new_node_cluster(0, 3);\n    // To ensure the thread has full store disk usage infomation.\n    cluster.cfg.raft_store.store_batch_system.pool_size = 1;\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n\n    // To ensure all replicas are not pending.\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    let region = cluster.get_region(b\"k1\");\n    let epoch = region.get_region_epoch().clone();\n\n    // To ensure followers have reported disk usages to the leader.\n    for i in 1..3 {\n        fail::cfg(get_fp(DiskUsage::AlmostFull, i + 1), \"return\").unwrap();\n        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);\n    }\n\n    // Normal proposals will be rejected because of majority peers' disk full.\n    let mut ch = cluster.async_put(b\"k2\", b\"v2\").unwrap();\n    let resp = ch.recv_timeout(Duration::from_secs(1)).unwrap();\n    assert_eq!(disk_full_stores(&resp), vec![2, 3]);\n\n    // Proposals with special `DiskFullOpt`s can be accepted even if all peers are\n    // disk full.\n    fail::cfg(get_fp(DiskUsage::AlmostFull, 1), \"return\").unwrap();\n    let reqs = vec![new_put_cmd(b\"k3\", b\"v3\")];\n    let put = new_request(1, epoch.clone(), reqs, false);\n    let mut opts = RaftCmdExtraOpts::default();\n    opts.disk_full_opt = DiskFullOpt::AllowedOnAlmostFull;\n    let mut ch = cluster.async_request_with_opts(put, opts).unwrap();\n    let resp = ch.recv_timeout(Duration::from_secs(1)).unwrap();\n    assert!(!resp.get_header().has_error());\n\n    // Reset disk full status for peer 2 and 3. 2 follower reads must success\n    // because the leader will continue to append entries to followers after the\n    // new disk usages are reported.\n    for i in 1..3 {\n        fail::remove(get_fp(DiskUsage::AlmostFull, i + 1));\n        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);\n        must_get_equal(&cluster.get_engine(i + 1), b\"k3\", b\"v3\");\n    }\n\n    // To ensure followers have reported disk usages to the leader.\n    for i in 1..3 {\n        fail::cfg(get_fp(DiskUsage::AlreadyFull, i + 1), \"return\").unwrap();\n        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);\n    }\n\n    // Proposals with special `DiskFullOpt`s will still be rejected if majority\n    // peers are already disk full.\n    let reqs = vec![new_put_cmd(b\"k3\", b\"v3\")];\n    let put = new_request(1, epoch.clone(), reqs, false);\n    let mut opts = RaftCmdExtraOpts::default();\n    opts.disk_full_opt = DiskFullOpt::AllowedOnAlmostFull;\n    let mut ch = cluster.async_request_with_opts(put, opts).unwrap();\n    let resp = ch.recv_timeout(Duration::from_secs(10)).unwrap();\n    assert_eq!(disk_full_stores(&resp), vec![2, 3]);\n\n    // Peer 2 disk usage changes from already full to almost full.\n    fail::remove(get_fp(DiskUsage::AlreadyFull, 2));\n    fail::cfg(get_fp(DiskUsage::AlmostFull, 2), \"return\").unwrap();\n    ensure_disk_usage_is_reported(&mut cluster, 2, 2, &region);\n\n    // Configuration change should be alloed.\n    cluster.pd_client.must_remove_peer(1, new_peer(2, 2));\n\n    // After the last configuration change is applied, the raft group will be like\n    // `[(1, DiskUsage::AlmostFull), (3, DiskUsage::AlreadyFull)]`. So no more\n    // proposals should be allowed.\n    let reqs = vec![new_put_cmd(b\"k4\", b\"v4\")];\n    let put = new_request(1, epoch, reqs, false);\n    let mut opts = RaftCmdExtraOpts::default();\n    opts.disk_full_opt = DiskFullOpt::AllowedOnAlmostFull;\n    let mut ch = cluster.async_request_with_opts(put, opts).unwrap();\n    let resp = ch.recv_timeout(Duration::from_secs(1)).unwrap();\n    assert_eq!(disk_full_stores(&resp), vec![3]);\n\n    for i in 0..3 {\n        fail::remove(get_fp(DiskUsage::AlreadyFull, i + 1));\n        fail::remove(get_fp(DiskUsage::AlmostFull, i + 1));\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_disk_full.rs::disk_full_stores", "code": "fn disk_full_stores(resp: &RaftCmdResponse) -> Vec<u64> {\n    let region_error = resp.get_header().get_error();\n    assert!(region_error.has_disk_full());\n    let mut stores = region_error.get_disk_full().get_store_id().to_vec();\n    stores.sort_unstable();\n    stores\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_encryption.rs::test_file_dict_file_record_corrupted", "test": "fn test_file_dict_file_record_corrupted() {\n    let tempdir = tempfile::tempdir().unwrap();\n    let mut file_dict_file = FileDictionaryFile::new(\n        tempdir.path(),\n        \"test_file_dict_file_record_corrupted_1\",\n        true,\n        10, // file_rewrite_threshold\n    )\n    .unwrap();\n    let info1 = create_file_info(1, EncryptionMethod::Aes256Ctr);\n    let info2 = create_file_info(2, EncryptionMethod::Unknown);\n    // 9 represents that the first 9 bytes will be discarded.\n    // Crc32 (4 bytes) + File name length (2 bytes) + FileInfo length (2 bytes) +\n    // Log type (1 bytes)\n    fail::cfg(\"file_dict_log_append_incomplete\", \"return(9)\").unwrap();\n    file_dict_file.insert(\"info1\", &info1, true).unwrap();\n    fail::remove(\"file_dict_log_append_incomplete\");\n    file_dict_file.insert(\"info2\", &info2, true).unwrap();\n    // Intermediate record damage is not allowed.\n    file_dict_file.recovery().unwrap_err();\n\n    let mut file_dict_file = FileDictionaryFile::new(\n        tempdir.path(),\n        \"test_file_dict_file_record_corrupted_2\",\n        true,\n        10, // file_rewrite_threshold\n    )\n    .unwrap();\n    let info1 = create_file_info(1, EncryptionMethod::Aes256Ctr);\n    let info2 = create_file_info(2, EncryptionMethod::Unknown);\n    file_dict_file.insert(\"info1\", &info1, true).unwrap();\n    fail::cfg(\"file_dict_log_append_incomplete\", \"return(9)\").unwrap();\n    file_dict_file.insert(\"info2\", &info2, true).unwrap();\n    fail::remove(\"file_dict_log_append_incomplete\");\n    // The ending record can be discarded.\n    let file_dict = file_dict_file.recovery().unwrap();\n    assert_eq!(*file_dict.files.get(\"info1\").unwrap(), info1);\n    assert_eq!(file_dict.files.len(), 1);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/table_properties.rs::get", "code": "fn get(&self, _: &[u8]) -> Option<&[u8]> {\n        None\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_metrics.rs::test_txn_create_compaction_filter", "test": "fn test_txn_create_compaction_filter() {\n    GC_COMPACTION_FILTER_PERFORM.reset();\n    GC_COMPACTION_FILTER_SKIP.reset();\n\n    let mut cfg = DbConfig::default();\n    cfg.writecf.disable_auto_compactions = true;\n    cfg.writecf.dynamic_level_bytes = false;\n    let dir = tempfile::TempDir::new().unwrap();\n    let builder = TestEngineBuilder::new().path(dir.path());\n    let mut engine = builder.build_with_cfg(&cfg).unwrap();\n    let raw_engine = engine.get_rocksdb();\n\n    let mut gc_runner = TestGcRunner::new(0);\n    let value = vec![b'v'; 512];\n\n    must_prewrite_put(&mut engine, b\"zkey\", &value, b\"zkey\", 100);\n    must_commit(&mut engine, b\"zkey\", 100, 110);\n\n    gc_runner\n        .safe_point(TimeStamp::new(1).into_inner())\n        .gc(&raw_engine);\n    assert_eq!(\n        GC_COMPACTION_FILTER_PERFORM\n            .with_label_values(&[STAT_TXN_KEYMODE])\n            .get(),\n        1\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTER_SKIP\n            .with_label_values(&[STAT_TXN_KEYMODE])\n            .get(),\n        1\n    );\n\n    GC_COMPACTION_FILTER_PERFORM.reset();\n    GC_COMPACTION_FILTER_SKIP.reset();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/table_properties.rs::get", "code": "fn get(&self, _: &[u8]) -> Option<&[u8]> {\n        None\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_metrics.rs::test_txn_gc_keys_handled", "test": "fn test_txn_gc_keys_handled() {\n    let store_id = 1;\n    GC_COMPACTION_FILTER_MVCC_DELETION_MET.reset();\n    GC_COMPACTION_FILTER_MVCC_DELETION_HANDLED.reset();\n\n    let engine = TestEngineBuilder::new().build().unwrap();\n    let mut prefixed_engine = PrefixedEngine(engine.clone());\n\n    let (tx, _rx) = mpsc::channel();\n    let feature_gate = FeatureGate::default();\n    feature_gate.set_version(\"5.0.0\").unwrap();\n    let mut gc_worker = GcWorker::new(\n        prefixed_engine.clone(),\n        tx,\n        GcConfig::default(),\n        feature_gate,\n        Arc::new(MockRegionInfoProvider::new(vec![])),\n    );\n    gc_worker.start(store_id).unwrap();\n\n    let mut r1 = Region::default();\n    r1.set_id(1);\n    r1.mut_region_epoch().set_version(1);\n    r1.set_start_key(b\"\".to_vec());\n    r1.set_end_key(b\"\".to_vec());\n    r1.mut_peers().push(Peer::default());\n    r1.mut_peers()[0].set_store_id(store_id);\n\n    let sp_provider = MockSafePointProvider(200);\n    let mut host = CoprocessorHost::<RocksEngine>::default();\n    let ri_provider = RegionInfoAccessor::new(&mut host);\n    let auto_gc_cfg = AutoGcConfig::new(sp_provider, ri_provider, 1);\n    let safe_point = Arc::new(AtomicU64::new(500));\n\n    gc_worker.start_auto_gc(auto_gc_cfg, safe_point).unwrap();\n    host.on_region_changed(&r1, RegionChangeEvent::Create, StateRole::Leader);\n\n    let db = engine.kv_engine().unwrap().as_inner().clone();\n    let cf = get_cf_handle(&db, CF_WRITE).unwrap();\n\n    for i in 0..3 {\n        let k = format!(\"k{:02}\", i).into_bytes();\n        must_prewrite_put(&mut prefixed_engine, &k, b\"value\", &k, 101);\n        must_commit(&mut prefixed_engine, &k, 101, 102);\n        must_prewrite_delete(&mut prefixed_engine, &k, &k, 151);\n        must_commit(&mut prefixed_engine, &k, 151, 152);\n    }\n\n    db.flush_cf(cf, true, false).unwrap();\n\n    db.compact_range_cf(cf, None, None);\n\n    // This compaction can schedule gc task\n    db.compact_range_cf(cf, None, None);\n    thread::sleep(Duration::from_millis(100));\n\n    assert_eq!(\n        GC_COMPACTION_FILTER_MVCC_DELETION_MET\n            .with_label_values(&[STAT_TXN_KEYMODE])\n            .get(),\n        6\n    );\n\n    assert_eq!(\n        GC_COMPACTION_FILTER_MVCC_DELETION_HANDLED\n            .with_label_values(&[STAT_TXN_KEYMODE])\n            .get(),\n        3\n    );\n\n    GC_COMPACTION_FILTER_MVCC_DELETION_MET.reset();\n    GC_COMPACTION_FILTER_MVCC_DELETION_HANDLED.reset();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::get", "code": "fn get(&self, mut req: Get) -> PdFuture<GetResponse> {\n        let timer = Instant::now();\n        self.fill_cluster_id_for(req.inner.mut_header());\n        let executor = move |client: &Client, req: GetRequest| {\n            let handler = {\n                let inner = client.inner.rl();\n                let r = inner\n                    .meta_storage\n                    .get_async_opt(&req, call_option_inner(&inner));\n                futures::future::ready(r).err_into().try_flatten()\n            };\n            Box::pin(async move {\n                fail::fail_point!(\"meta_storage_get\", req.key.ends_with(b\"rejectme\"), |_| {\n                    Err(super::Error::Grpc(grpcio::Error::RemoteStopped))\n                });\n                let resp = handler.await?;\n                PD_REQUEST_HISTOGRAM_VEC\n                    .meta_storage_get\n                    .observe(timer.saturating_elapsed_secs());\n                Ok(resp)\n            }) as _\n        };\n\n        self.pd_client\n            .request(req.into(), executor, LEADER_CHANGE_RETRY)\n            .execute()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_metrics.rs::test_raw_gc_keys_handled", "test": "fn test_raw_gc_keys_handled() {\n    let store_id = 1;\n    GC_COMPACTION_FILTER_MVCC_DELETION_MET.reset();\n    GC_COMPACTION_FILTER_MVCC_DELETION_HANDLED.reset();\n\n    let engine = TestEngineBuilder::new()\n        .api_version(ApiVersion::V2)\n        .build()\n        .unwrap();\n    let prefixed_engine = PrefixedEngine(engine.clone());\n\n    let (tx, _rx) = mpsc::channel();\n    let feature_gate = FeatureGate::default();\n    let mut gc_worker = GcWorker::new(\n        prefixed_engine,\n        tx,\n        GcConfig::default(),\n        feature_gate,\n        Arc::new(MockRegionInfoProvider::new(vec![])),\n    );\n    gc_worker.start(store_id).unwrap();\n\n    let mut r1 = Region::default();\n    r1.set_id(1);\n    r1.mut_region_epoch().set_version(1);\n    r1.set_start_key(b\"\".to_vec());\n    r1.set_end_key(b\"\".to_vec());\n    r1.mut_peers().push(Peer::default());\n    r1.mut_peers()[0].set_store_id(store_id);\n\n    let sp_provider = MockSafePointProvider(200);\n    let mut host = CoprocessorHost::<RocksEngine>::default();\n    let ri_provider = RegionInfoAccessor::new(&mut host);\n    let auto_gc_cfg = AutoGcConfig::new(sp_provider, ri_provider, store_id);\n    let safe_point = Arc::new(AtomicU64::new(500));\n\n    gc_worker.start_auto_gc(auto_gc_cfg, safe_point).unwrap();\n    host.on_region_changed(&r1, RegionChangeEvent::Create, StateRole::Leader);\n\n    let db = engine.kv_engine().unwrap().as_inner().clone();\n\n    let user_key_del = b\"r\\0aaaaaaaaaaa\";\n\n    // If it's deleted, it will call async scheduler GcTask.\n    let test_raws = vec![\n        (user_key_del, 9, true),\n        (user_key_del, 5, false),\n        (user_key_del, 1, false),\n    ];\n\n    let modifies = test_raws\n        .into_iter()\n        .map(|(key, ts, is_delete)| {\n            (\n                make_key(key, ts),\n                ApiV2::encode_raw_value(RawValue {\n                    user_value: &[0; 10][..],\n                    expire_ts: Some(TimeStamp::max().into_inner()),\n                    is_delete,\n                }),\n            )\n        })\n        .map(|(k, v)| Modify::Put(CF_DEFAULT, Key::from_encoded_slice(k.as_slice()), v))\n        .collect();\n\n    let ctx = Context {\n        api_version: ApiVersion::V2,\n        ..Default::default()\n    };\n\n    let batch = WriteData::from_modifies(modifies);\n\n    engine.write(&ctx, batch).unwrap();\n\n    let cf = get_cf_handle(&db, CF_DEFAULT).unwrap();\n    db.flush_cf(cf, true, false).unwrap();\n\n    db.compact_range_cf(cf, None, None);\n\n    thread::sleep(Duration::from_millis(100));\n\n    assert_eq!(\n        GC_COMPACTION_FILTER_MVCC_DELETION_MET\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        1\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTER_MVCC_DELETION_HANDLED\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        1\n    );\n\n    GC_COMPACTION_FILTER_MVCC_DELETION_MET.reset();\n    GC_COMPACTION_FILTER_MVCC_DELETION_HANDLED.reset();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::get", "code": "fn get(&self, mut req: Get) -> PdFuture<GetResponse> {\n        let timer = Instant::now();\n        self.fill_cluster_id_for(req.inner.mut_header());\n        let executor = move |client: &Client, req: GetRequest| {\n            let handler = {\n                let inner = client.inner.rl();\n                let r = inner\n                    .meta_storage\n                    .get_async_opt(&req, call_option_inner(&inner));\n                futures::future::ready(r).err_into().try_flatten()\n            };\n            Box::pin(async move {\n                fail::fail_point!(\"meta_storage_get\", req.key.ends_with(b\"rejectme\"), |_| {\n                    Err(super::Error::Grpc(grpcio::Error::RemoteStopped))\n                });\n                let resp = handler.await?;\n                PD_REQUEST_HISTOGRAM_VEC\n                    .meta_storage_get\n                    .observe(timer.saturating_elapsed_secs());\n                Ok(resp)\n            }) as _\n        };\n\n        self.pd_client\n            .request(req.into(), executor, LEADER_CHANGE_RETRY)\n            .execute()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_worker.rs::test_error_in_compaction_filter", "test": "fn test_error_in_compaction_filter() {\n    let mut engine = TestEngineBuilder::new().build().unwrap();\n    let raw_engine = engine.get_rocksdb();\n\n    let large_value = vec![b'x'; 300];\n    must_prewrite_put(&mut engine, b\"zkey\", &large_value, b\"zkey\", 101);\n    must_commit(&mut engine, b\"zkey\", 101, 102);\n    must_prewrite_put(&mut engine, b\"zkey\", &large_value, b\"zkey\", 103);\n    must_commit(&mut engine, b\"zkey\", 103, 104);\n    must_prewrite_delete(&mut engine, b\"zkey\", b\"zkey\", 105);\n    must_commit(&mut engine, b\"zkey\", 105, 106);\n\n    let fp = \"write_compaction_filter_flush_write_batch\";\n    fail::cfg(fp, \"return\").unwrap();\n\n    let mut gc_runner = TestGcRunner::new(200);\n    gc_runner.gc(&raw_engine);\n\n    match gc_runner.gc_receiver.recv().unwrap() {\n        GcTask::OrphanVersions { wb, .. } => assert_eq!(wb.count(), 2),\n        GcTask::GcKeys { .. } => {}\n        _ => unreachable!(),\n    }\n\n    // Although versions on default CF is not cleaned, write CF is GCed correctly.\n    must_get_none(&mut engine, b\"zkey\", 102);\n    must_get_none(&mut engine, b\"zkey\", 104);\n\n    fail::remove(fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/write_batch.rs::count", "code": "fn count(&self) -> usize {\n        panic!()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_gc_worker.rs::test_orphan_versions_from_compaction_filter", "test": "fn test_orphan_versions_from_compaction_filter() {\n    let (cluster, leader, ctx) = must_new_and_configure_cluster(|cluster| {\n        cluster.cfg.gc.enable_compaction_filter = true;\n        cluster.cfg.gc.compaction_filter_skip_version_check = true;\n        cluster.pd_client.disable_default_operator();\n    });\n\n    let env = Arc::new(Environment::new(1));\n    let leader_store = leader.get_store_id();\n    let channel = ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader_store));\n    let client = TikvClient::new(channel);\n\n    init_compaction_filter(&cluster, leader_store);\n    let engine = cluster.engines.get(&leader_store).unwrap();\n\n    let pk = b\"k1\".to_vec();\n    let large_value = vec![b'x'; 300];\n    for &start_ts in &[10, 20, 30, 40] {\n        let commit_ts = start_ts + 5;\n        let op = if start_ts < 40 { Op::Put } else { Op::Del };\n        let muts = vec![new_mutation(op, b\"k1\", &large_value)];\n        must_kv_prewrite(&client, ctx.clone(), muts, pk.clone(), start_ts);\n        let keys = vec![pk.clone()];\n        must_kv_commit(&client, ctx.clone(), keys, start_ts, commit_ts, commit_ts);\n        if start_ts < 40 {\n            let key = Key::from_raw(b\"k1\").append_ts(start_ts.into());\n            let key = data_key(key.as_encoded());\n            assert!(engine.kv.get_value(&key).unwrap().is_some());\n        }\n    }\n\n    let fp = \"write_compaction_filter_flush_write_batch\";\n    fail::cfg(fp, \"return\").unwrap();\n\n    let mut gc_runner = TestGcRunner::new(100);\n    gc_runner.gc_scheduler = cluster.sim.rl().get_gc_worker(1).scheduler();\n    gc_runner.gc(&engine.kv);\n\n    'IterKeys: for &start_ts in &[10, 20, 30] {\n        let key = Key::from_raw(b\"k1\").append_ts(start_ts.into());\n        let key = data_key(key.as_encoded());\n        for _ in 0..100 {\n            if engine.kv.get_value(&key).unwrap().is_some() {\n                thread::sleep(Duration::from_millis(20));\n                continue;\n            }\n            continue 'IterKeys;\n        }\n        panic!(\"orphan versions should already been cleaned by GC worker\");\n    }\n\n    fail::remove(fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_kv_service.rs::test_batch_get_memory_lock", "test": "fn test_batch_get_memory_lock() {\n    let (_cluster, client, ctx) = must_new_cluster_and_kv_client();\n\n    let mut req = BatchGetRequest::default();\n    req.set_context(ctx);\n    req.set_keys(vec![b\"a\".to_vec(), b\"b\".to_vec()].into());\n    req.version = 50;\n\n    fail::cfg(\"raftkv_async_snapshot_err\", \"return\").unwrap();\n    let resp = client.kv_batch_get(&req).unwrap();\n    // the injected error should be returned at both places for backward\n    // compatibility.\n    assert!(!resp.pairs[0].get_error().get_abort().is_empty());\n    assert!(!resp.get_error().get_abort().is_empty());\n    fail::remove(\"raftkv_async_snapshot_err\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_kv_service.rs::test_kv_scan_memory_lock", "test": "fn test_kv_scan_memory_lock() {\n    let (_cluster, client, ctx) = must_new_cluster_and_kv_client();\n\n    let mut req = ScanRequest::default();\n    req.set_context(ctx);\n    req.set_start_key(b\"a\".to_vec());\n    req.version = 50;\n\n    fail::cfg(\"raftkv_async_snapshot_err\", \"return\").unwrap();\n    let resp = client.kv_scan(&req).unwrap();\n    // the injected error should be returned at both places for backward\n    // compatibility.\n    assert!(!resp.pairs[0].get_error().get_abort().is_empty());\n    assert!(!resp.get_error().get_abort().is_empty());\n    fail::remove(\"raftkv_async_snapshot_err\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_kv_service.rs::test_undetermined_write_err", "test": "fn test_undetermined_write_err() {\n    let (cluster, leader, ctx) = must_new_cluster_mul(1);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env)\n        .keepalive_time(Duration::from_millis(500))\n        .keepalive_timeout(Duration::from_millis(500))\n        .connect(&cluster.sim.read().unwrap().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.set_key(b\"k\".to_vec());\n    mutation.set_value(b\"v\".to_vec());\n    fail::cfg(\"applied_cb_return_undetermined_err\", \"return()\").unwrap();\n    let err = try_kv_prewrite_with_impl(\n        &client,\n        ctx,\n        vec![mutation],\n        b\"k\".to_vec(),\n        10,\n        0,\n        false,\n        false,\n    )\n    .unwrap_err();\n    assert_eq!(err.to_string(), \"RpcFailure: 1-CANCELLED CANCELLED\",);\n    fail::remove(\"applied_cb_return_undetermined_err\");\n    // The previous panic hasn't been captured.\n    assert!(std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| drop(cluster))).is_err());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/datum.rs::to_string", "code": "pub fn to_string(&self) -> Result<String> {\n        let s = match *self {\n            Datum::I64(i) => format!(\"{}\", i),\n            Datum::U64(u) => format!(\"{}\", u),\n            Datum::F64(f) => format!(\"{}\", f),\n            Datum::Bytes(ref bs) => String::from_utf8(bs.to_vec())?,\n            Datum::Time(t) => format!(\"{}\", t),\n            Datum::Dur(ref d) => format!(\"{}\", d),\n            Datum::Dec(ref d) => format!(\"{}\", d),\n            Datum::Json(ref d) => d.to_string(),\n            Datum::Enum(ref e) => e.to_string(),\n            Datum::Set(ref s) => s.to_string(),\n            ref d => return Err(invalid_type!(\"can't convert {} to string\", d)),\n        };\n        Ok(s)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_local_read.rs::test_consistency_after_lease_pass", "test": "fn test_consistency_after_lease_pass() {\n    let mut cluster = new_server_cluster(0, 3);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    cluster.run();\n    let leader = new_peer(1, 1);\n    cluster.must_transfer_leader(1, leader);\n\n    // Create clients.\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(Arc::clone(&env)).connect(&cluster.sim.rl().get_addr(1));\n    let client = TikvClient::new(channel);\n\n    let region = cluster.get_region(&b\"key1\"[..]);\n    let region_id = region.id;\n    let leader = cluster.leader_of_region(region_id).unwrap();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region_id);\n    ctx.set_peer(leader.clone());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n\n    must_raw_put(&client, ctx.clone(), b\"key1\".to_vec(), b\"value1\".to_vec());\n    must_get_equal(&cluster.get_engine(1), b\"key1\", b\"value1\");\n\n    // Ensure the request is executed by the local reader\n    fail::cfg(\"localreader_before_redirect\", \"panic\").unwrap();\n\n    // Lease read works correctly\n    assert_eq!(\n        must_raw_get(&client, ctx.clone(), b\"key1\".to_vec()).unwrap(),\n        b\"value1\".to_vec()\n    );\n\n    // we pause just after pass the lease check, and then remove the peer. We can\n    // still read the relevant value as we should have already got the snapshot when\n    // passing the lease check.\n    fail::cfg(\"after_pass_lease_check\", \"pause\").unwrap();\n\n    let mut get_req = RawGetRequest::default();\n    get_req.set_context(ctx);\n    get_req.key = b\"key1\".to_vec();\n    let mut receiver = client.raw_get_async(&get_req).unwrap();\n\n    thread::sleep(Duration::from_millis(200));\n\n    let mut peer = leader.clone();\n    cluster.must_transfer_leader(1, new_peer(2, 2));\n    pd_client.must_remove_peer(region_id, leader);\n    peer.id = 1000;\n    // After we pass the lease check, we should have got the snapshot, so the data\n    // that the region contains cannot be deleted.\n    // So we need to add the new peer for this region and stop before applying the\n    // snapshot so that the old data will be deleted and the snapshot data has not\n    // been written.\n    fail::cfg(\"apply_snap_cleanup_range\", \"pause\").unwrap();\n    pd_client.must_add_peer(region_id, peer);\n\n    // Wait for data to be cleaned\n    must_get_none(&cluster.get_engine(1), b\"key1\");\n    fail::cfg(\"after_pass_lease_check\", \"off\").unwrap();\n\n    assert_eq!(b\"value1\", receiver.receive_sync().unwrap().1.get_value());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/peer.rs::to_vec", "code": "pub fn to_vec(self) -> Vec<u8> {\n        if self.is_empty() {\n            return vec![];\n        }\n        let ctx = self.bits();\n        vec![ctx]\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_merge_rollback", "test": "fn test_node_merge_rollback() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run_conf_change();\n\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    pd_client.must_add_peer(left.get_id(), new_peer(2, 2));\n    pd_client.must_add_peer(right.get_id(), new_peer(2, 4));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    let target_region = pd_client.get_region(b\"k3\").unwrap();\n\n    let schedule_merge_fp = \"on_schedule_merge\";\n    fail::cfg(schedule_merge_fp, \"return()\").unwrap();\n\n    // The call is finished when prepare_merge is applied.\n    cluster.must_try_merge(region.get_id(), target_region.get_id());\n\n    // Add a peer to trigger rollback.\n    pd_client.must_add_peer(right.get_id(), new_peer(3, 5));\n    cluster.must_put(b\"k4\", b\"v4\");\n    must_get_equal(&cluster.get_engine(3), b\"k4\", b\"v4\");\n\n    let mut region = pd_client.get_region(b\"k1\").unwrap();\n    // After split and prepare_merge, version becomes 1 + 2 = 3;\n    assert_eq!(region.get_region_epoch().get_version(), 3);\n    // After ConfChange and prepare_merge, conf version becomes 1 + 2 = 3;\n    assert_eq!(region.get_region_epoch().get_conf_ver(), 3);\n    fail::remove(schedule_merge_fp);\n    // Wait till rollback.\n    cluster.must_put(b\"k11\", b\"v11\");\n\n    // After rollback, version becomes 3 + 1 = 4;\n    region.mut_region_epoch().set_version(4);\n    for i in 1..3 {\n        must_get_equal(&cluster.get_engine(i), b\"k11\", b\"v11\");\n        let state_key = keys::region_state_key(region.get_id());\n        let state: RegionLocalState = cluster\n            .get_engine(i)\n            .get_msg_cf(CF_RAFT, &state_key)\n            .unwrap()\n            .unwrap();\n        assert_eq!(state.get_state(), PeerState::Normal);\n        assert_eq!(*state.get_region(), region);\n    }\n\n    pd_client.must_remove_peer(right.get_id(), new_peer(3, 5));\n    fail::cfg(schedule_merge_fp, \"return()\").unwrap();\n\n    let target_region = pd_client.get_region(b\"k3\").unwrap();\n    cluster.must_try_merge(region.get_id(), target_region.get_id());\n    let mut region = pd_client.get_region(b\"k1\").unwrap();\n\n    // Split to trigger rollback.\n    cluster.must_split(&right, b\"k3\");\n    fail::remove(schedule_merge_fp);\n    // Wait till rollback.\n    cluster.must_put(b\"k12\", b\"v12\");\n\n    // After premerge and rollback, conf_ver becomes 3 + 1 = 4, version becomes 4 +\n    // 2 = 6;\n    region.mut_region_epoch().set_conf_ver(4);\n    region.mut_region_epoch().set_version(6);\n    for i in 1..3 {\n        must_get_equal(&cluster.get_engine(i), b\"k12\", b\"v12\");\n        let state_key = keys::region_state_key(region.get_id());\n        let state: RegionLocalState = cluster\n            .get_engine(i)\n            .get_msg_cf(CF_RAFT, &state_key)\n            .unwrap()\n            .unwrap();\n        assert_eq!(state.get_state(), PeerState::Normal);\n        assert_eq!(*state.get_region(), region);\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/util.rs::get_version", "code": "pub fn get_version(&self) -> u64 {\n        self.memtable_version\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_merge_restart", "test": "fn test_node_merge_restart() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.run();\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let schedule_merge_fp = \"on_schedule_merge\";\n    fail::cfg(schedule_merge_fp, \"return()\").unwrap();\n\n    cluster.must_try_merge(left.get_id(), right.get_id());\n    let leader = cluster.leader_of_region(left.get_id()).unwrap();\n\n    cluster.shutdown();\n    let engine = cluster.get_engine(leader.get_store_id());\n    let state_key = keys::region_state_key(left.get_id());\n    let state: RegionLocalState = engine.get_msg_cf(CF_RAFT, &state_key).unwrap().unwrap();\n    assert_eq!(state.get_state(), PeerState::Merging, \"{:?}\", state);\n    let state_key = keys::region_state_key(right.get_id());\n    let state: RegionLocalState = engine.get_msg_cf(CF_RAFT, &state_key).unwrap().unwrap();\n    assert_eq!(state.get_state(), PeerState::Normal, \"{:?}\", state);\n    fail::remove(schedule_merge_fp);\n    cluster.start().unwrap();\n\n    // Wait till merge is finished.\n    pd_client.check_merged_timeout(left.get_id(), Duration::from_secs(5));\n\n    cluster.must_put(b\"k4\", b\"v4\");\n\n    for i in 1..4 {\n        must_get_equal(&cluster.get_engine(i), b\"k4\", b\"v4\");\n        let state_key = keys::region_state_key(left.get_id());\n        let state: RegionLocalState = cluster\n            .get_engine(i)\n            .get_msg_cf(CF_RAFT, &state_key)\n            .unwrap()\n            .unwrap();\n        assert_eq!(state.get_state(), PeerState::Tombstone, \"{:?}\", state);\n        let state_key = keys::region_state_key(right.get_id());\n        let state: RegionLocalState = cluster\n            .get_engine(i)\n            .get_msg_cf(CF_RAFT, &state_key)\n            .unwrap()\n            .unwrap();\n        assert_eq!(state.get_state(), PeerState::Normal, \"{:?}\", state);\n        assert!(state.get_region().get_start_key().is_empty());\n        assert!(state.get_region().get_end_key().is_empty());\n    }\n\n    // Now test if cluster works fine when it crash after merge is applied\n    // but before notifying raftstore thread.\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    let peer_on_store1 = find_peer(&region, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    cluster.must_split(&region, b\"k2\");\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n    let peer_on_store1 = find_peer(&left, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(left.get_id(), peer_on_store1);\n    cluster.must_put(b\"k11\", b\"v11\");\n    must_get_equal(&cluster.get_engine(3), b\"k11\", b\"v11\");\n    let skip_destroy_fp = \"raft_store_skip_destroy_peer\";\n    fail::cfg(skip_destroy_fp, \"return()\").unwrap();\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    pd_client.must_merge(left.get_id(), right.get_id());\n    let peer = find_peer(&right, 3).unwrap().to_owned();\n    pd_client.must_remove_peer(right.get_id(), peer);\n    cluster.shutdown();\n    fail::remove(skip_destroy_fp);\n    cluster.clear_send_filters();\n    cluster.start().unwrap();\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    must_get_none(&cluster.get_engine(3), b\"k3\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_state", "code": "pub fn get_state(&self) -> Arc<AtomicCell<DownstreamState>> {\n        self.state.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_merge_catch_up_logs_no_need", "test": "fn test_node_merge_catch_up_logs_no_need() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(10);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 25;\n    cluster.cfg.raft_store.raft_log_gc_threshold = 12;\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(12);\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(100);\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    let peer_on_store1 = find_peer(&region, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    cluster.must_split(&region, b\"k2\");\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    // put some keys to trigger compact raft log\n    for i in 2..20 {\n        cluster.must_put(format!(\"k1{}\", i).as_bytes(), b\"v\");\n    }\n\n    // let the peer of left region on store 3 falls behind.\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(left.get_id(), 3)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend),\n    ));\n\n    // make sure the peer is isolated.\n    cluster.must_put(b\"k11\", b\"v11\");\n    must_get_none(&cluster.get_engine(3), b\"k11\");\n\n    // propose merge but not let apply index make progress.\n    fail::cfg(\"apply_after_prepare_merge\", \"pause\").unwrap();\n    pd_client.merge_region(left.get_id(), right.get_id());\n    must_get_none(&cluster.get_engine(3), b\"k11\");\n\n    // wait to trigger compact raft log\n    thread::sleep(Duration::from_millis(100));\n\n    // let source region not merged\n    fail::cfg(\"before_handle_catch_up_logs_for_merge\", \"pause\").unwrap();\n    fail::cfg(\"after_handle_catch_up_logs_for_merge\", \"pause\").unwrap();\n    // due to `before_handle_catch_up_logs_for_merge` failpoint, we already pass\n    // `apply_index < catch_up_logs.merge.get_commit()` so now can let apply\n    // index make progress.\n    fail::remove(\"apply_after_prepare_merge\");\n\n    // make sure all the logs are committed, including the compact command\n    cluster.clear_send_filters();\n    thread::sleep(Duration::from_millis(50));\n\n    // let merge process continue\n    fail::remove(\"before_handle_catch_up_logs_for_merge\");\n    fail::remove(\"after_handle_catch_up_logs_for_merge\");\n    thread::sleep(Duration::from_millis(50));\n\n    // the source region should be merged and the peer should be destroyed.\n    assert!(pd_client.check_merged(left.get_id()));\n    must_get_equal(&cluster.get_engine(3), b\"k11\", b\"v11\");\n    cluster.must_region_not_exist(left.get_id(), 3);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_failed_merge_before_succeed_merge", "test": "fn test_node_failed_merge_before_succeed_merge() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.raft_store.merge_max_log_gap = 30;\n    cluster.cfg.raft_store.store_batch_system.max_batch_size = Some(1);\n    cluster.cfg.raft_store.store_batch_system.pool_size = 2;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    for i in 0..10 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), b\"v1\");\n    }\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k5\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let mut right = pd_client.get_region(b\"k5\").unwrap();\n    let left_peer_1 = find_peer(&left, 1).cloned().unwrap();\n    cluster.must_transfer_leader(left.get_id(), left_peer_1);\n\n    let left_peer_3 = find_peer(&left, 3).cloned().unwrap();\n    assert_eq!(left_peer_3.get_id(), 1003);\n\n    // Prevent sched_merge_tick to propose CommitMerge\n    let schedule_merge_fp = \"on_schedule_merge\";\n    fail::cfg(schedule_merge_fp, \"return\").unwrap();\n\n    // To minimize peers log gap for merging\n    cluster.must_put(b\"k11\", b\"v2\");\n    must_get_equal(&cluster.get_engine(2), b\"k11\", b\"v2\");\n    must_get_equal(&cluster.get_engine(3), b\"k11\", b\"v2\");\n    // Make peer 1003 can't receive PrepareMerge and RollbackMerge log\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n\n    cluster.must_try_merge(left.get_id(), right.get_id());\n\n    // Change right region's epoch to make this merge failed\n    cluster.must_split(&right, b\"k8\");\n    fail::remove(schedule_merge_fp);\n    // Wait for left region to rollback merge\n    cluster.must_put(b\"k12\", b\"v2\");\n    // Prevent apply fsm applying the `PrepareMerge` and `RollbackMerge` log after\n    // cleaning send filter.\n    let before_handle_normal_1003_fp = \"before_handle_normal_1003\";\n    fail::cfg(before_handle_normal_1003_fp, \"return\").unwrap();\n    cluster.clear_send_filters();\n\n    right = pd_client.get_region(b\"k5\").unwrap();\n    let right_peer_1 = find_peer(&right, 1).cloned().unwrap();\n    cluster.must_transfer_leader(right.get_id(), right_peer_1);\n    // Add some data for checking data integrity check at a later time\n    for i in 0..5 {\n        cluster.must_put(format!(\"k2{}\", i).as_bytes(), b\"v3\");\n    }\n    // Do a really succeed merge\n    pd_client.must_merge(left.get_id(), right.get_id());\n    // Wait right region to send CatchUpLogs to left region.\n    sleep_ms(100);\n    // After executing CatchUpLogs in source peer fsm, the committed log will send\n    // to apply fsm in the end of this batch. So even the first\n    // `on_ready_prepare_merge` is executed after CatchUplogs, the latter\n    // committed logs is still sent to apply fsm if CatchUpLogs and\n    // `on_ready_prepare_merge` is in different batch.\n    //\n    // In this case, the data is complete because the wrong up-to-date msg from the\n    // first `on_ready_prepare_merge` is sent after all committed log.\n    // Sleep a while to wait apply fsm to send `on_ready_prepare_merge` to peer fsm.\n    let after_send_to_apply_1003_fp = \"after_send_to_apply_1003\";\n    fail::cfg(after_send_to_apply_1003_fp, \"sleep(300)\").unwrap();\n\n    fail::remove(before_handle_normal_1003_fp);\n    // Wait `after_send_to_apply_1003` timeout\n    sleep_ms(300);\n    fail::remove(after_send_to_apply_1003_fp);\n    // Check the data integrity\n    for i in 0..5 {\n        must_get_equal(&cluster.get_engine(3), format!(\"k2{}\", i).as_bytes(), b\"v3\");\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_merge_transfer_leader", "test": "fn test_node_merge_transfer_leader() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.raft_store.store_batch_system.max_batch_size = Some(1);\n    cluster.cfg.raft_store.store_batch_system.pool_size = 2;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    // To ensure the region has applied to its current term so that later `split`\n    // can success without any retries. Then, `left_peer_3` will must be `1003`.\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    let peer_1 = find_peer(&region, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), peer_1);\n    let k = b\"k1_for_apply_to_current_term\";\n    cluster.must_put(k, b\"value\");\n    must_get_equal(&cluster.get_engine(1), k, b\"value\");\n\n    cluster.must_split(&region, b\"k2\");\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    let left_peer_1 = find_peer(&left, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(left.get_id(), left_peer_1.clone());\n\n    let left_peer_3 = find_peer(&left, 3).unwrap().to_owned();\n    assert_eq!(left_peer_3.get_id(), 1003);\n\n    let schedule_merge_fp = \"on_schedule_merge\";\n    fail::cfg(schedule_merge_fp, \"return()\").unwrap();\n\n    cluster.must_try_merge(left.get_id(), right.get_id());\n\n    // Prevent peer 1003 to handle ready when it's leader\n    let before_handle_raft_ready_1003 = \"before_handle_raft_ready_1003\";\n    fail::cfg(before_handle_raft_ready_1003, \"pause\").unwrap();\n\n    let epoch = cluster.get_region_epoch(left.get_id());\n    let mut transfer_leader_req =\n        new_admin_request(left.get_id(), &epoch, new_transfer_leader_cmd(left_peer_3));\n    transfer_leader_req.mut_header().set_peer(left_peer_1);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, transfer_leader_req, Callback::None)\n        .unwrap();\n    fail::remove(schedule_merge_fp);\n\n    pd_client.check_merged_timeout(left.get_id(), Duration::from_secs(5));\n\n    fail::remove(before_handle_raft_ready_1003);\n    sleep_ms(100);\n    cluster.must_put(b\"k4\", b\"v4\");\n    must_get_equal(&cluster.get_engine(3), b\"k4\", b\"v4\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_multiple_rollback_merge", "test": "fn test_node_multiple_rollback_merge() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.raft_store.right_derive_when_split = true;\n    cluster.cfg.raft_store.merge_check_tick_interval = ReadableDuration::millis(20);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    for i in 0..10 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), b\"v\");\n    }\n\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    let left_peer_1 = find_peer(&left, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(left.get_id(), left_peer_1.clone());\n    assert_eq!(left_peer_1.get_id(), 1001);\n\n    let on_schedule_merge_fp = \"on_schedule_merge\";\n    let on_check_merge_not_1001_fp = \"on_check_merge_not_1001\";\n\n    let mut right_peer_1_id = find_peer(&right, 1).unwrap().get_id();\n\n    for i in 0..3 {\n        fail::cfg(on_schedule_merge_fp, \"return()\").unwrap();\n        cluster.must_try_merge(left.get_id(), right.get_id());\n        // Change the epoch of target region and the merge will fail\n        pd_client.must_remove_peer(right.get_id(), new_peer(1, right_peer_1_id));\n        right_peer_1_id += 100;\n        pd_client.must_add_peer(right.get_id(), new_peer(1, right_peer_1_id));\n        // Only the source leader is running `on_check_merge`\n        fail::cfg(on_check_merge_not_1001_fp, \"return()\").unwrap();\n        fail::remove(on_schedule_merge_fp);\n        // In previous implementation, rollback merge proposal can be proposed by leader\n        // itself So wait for the leader propose rollback merge if possible\n        sleep_ms(100);\n        // Check if the source region is still in merging mode.\n        let mut l_r = pd_client.get_region(b\"k1\").unwrap();\n        let req = new_request(\n            l_r.get_id(),\n            l_r.take_region_epoch(),\n            vec![new_put_cf_cmd(\n                \"default\",\n                format!(\"k1{}\", i).as_bytes(),\n                b\"vv\",\n            )],\n            false,\n        );\n        let resp = cluster\n            .call_command_on_leader(req, Duration::from_millis(100))\n            .unwrap();\n        if !resp\n            .get_header()\n            .get_error()\n            .get_message()\n            .contains(\"merging mode\")\n        {\n            panic!(\"resp {:?} does not contain merging mode error\", resp);\n        }\n\n        fail::remove(on_check_merge_not_1001_fp);\n        // Write data for waiting the merge to rollback easily\n        cluster.must_put(format!(\"k1{}\", i).as_bytes(), b\"vv\");\n        // Make sure source region is not merged to target region\n        assert_eq!(pd_client.get_region(b\"k1\").unwrap().get_id(), left.get_id());\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_merge_write_data_to_source_region_after_merging", "test": "fn test_node_merge_write_data_to_source_region_after_merging() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.merge_check_tick_interval = ReadableDuration::millis(100);\n    // For snapshot after merging\n    cluster.cfg.raft_store.merge_max_log_gap = 10;\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(12);\n    cluster.cfg.raft_store.apply_batch_system.max_batch_size = Some(1);\n    cluster.cfg.raft_store.apply_batch_system.pool_size = 2;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k2\", b\"v2\");\n\n    let mut region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    let right_peer_2 = find_peer(&right, 2).cloned().unwrap();\n    assert_eq!(right_peer_2.get_id(), 2);\n\n    // Make sure peer 2 finish split before pause\n    cluster.must_put(b\"k2pause\", b\"vpause\");\n    must_get_equal(&cluster.get_engine(2), b\"k2pause\", b\"vpause\");\n\n    let on_handle_apply_2_fp = \"on_handle_apply_2\";\n    fail::cfg(on_handle_apply_2_fp, \"pause\").unwrap();\n\n    let right_peer_1 = find_peer(&right, 1).cloned().unwrap();\n    cluster.must_transfer_leader(right.get_id(), right_peer_1);\n\n    let left_peer_3 = find_peer(&left, 3).cloned().unwrap();\n    cluster.must_transfer_leader(left.get_id(), left_peer_3.clone());\n\n    let schedule_merge_fp = \"on_schedule_merge\";\n    fail::cfg(schedule_merge_fp, \"return()\").unwrap();\n\n    cluster.must_try_merge(left.get_id(), right.get_id());\n\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n\n    fail::remove(schedule_merge_fp);\n\n    pd_client.check_merged_timeout(left.get_id(), Duration::from_secs(5));\n\n    region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n    let state1 = cluster.apply_state(region.get_id(), 1);\n    for i in 0..15 {\n        cluster.must_put(format!(\"k2{}\", i).as_bytes(), b\"v2\");\n    }\n    cluster.wait_log_truncated(region.get_id(), 1, state1.get_applied_index());\n    // Ignore this msg to make left region exist.\n    let on_has_merge_target_fp = \"on_has_merge_target\";\n    fail::cfg(on_has_merge_target_fp, \"return\").unwrap();\n\n    cluster.clear_send_filters();\n    // On store 3, now the right region is updated by snapshot not applying logs\n    // so the left region still exist.\n    // Wait for left region to rollback merge (in previous wrong implementation)\n    sleep_ms(200);\n    // Write data to left region\n    let mut new_left = left;\n    let mut epoch = new_left.take_region_epoch();\n    // prepareMerge => conf_ver + 1, version + 1\n    // rollbackMerge => version + 1\n    epoch.set_conf_ver(epoch.get_conf_ver() + 1);\n    epoch.set_version(epoch.get_version() + 2);\n    let mut req = new_request(\n        new_left.get_id(),\n        epoch,\n        vec![new_put_cf_cmd(\"default\", b\"k11\", b\"v11\")],\n        false,\n    );\n    req.mut_header().set_peer(left_peer_3);\n    if let Ok(()) = cluster\n        .sim\n        .rl()\n        .async_command_on_node(3, req, Callback::None)\n    {\n        sleep_ms(200);\n        // The write must not succeed\n        must_get_none(&cluster.get_engine(2), b\"k11\");\n        must_get_none(&cluster.get_engine(3), b\"k11\");\n    }\n\n    fail::remove(on_handle_apply_2_fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_node_merge_crash_before_snapshot_then_catch_up_logs", "test": "fn test_node_merge_crash_before_snapshot_then_catch_up_logs() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.merge_max_log_gap = 10;\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(11);\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(50);\n    // Make merge check resume quickly.\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(10);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 10;\n    // election timeout must be greater than lease\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(90);\n    cluster.cfg.raft_store.merge_check_tick_interval = ReadableDuration::millis(100);\n    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::millis(500);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let on_raft_gc_log_tick_fp = \"on_raft_gc_log_tick\";\n    fail::cfg(on_raft_gc_log_tick_fp, \"return()\").unwrap();\n\n    cluster.run();\n\n    let mut region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    let left_on_store1 = find_peer(&left, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(left.get_id(), left_on_store1);\n    let right_on_store1 = find_peer(&right, 1).unwrap().to_owned();\n    cluster.must_transfer_leader(right.get_id(), right_on_store1);\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n\n    pd_client.must_merge(left.get_id(), right.get_id());\n\n    region = pd_client.get_region(b\"k1\").unwrap();\n    // Write some logs and the logs' number is greater than\n    // `raft_log_gc_count_limit` for latter log compaction\n    for i in 2..15 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), b\"v\");\n    }\n\n    // Aim at making peer 2 only know the compact log but do not know it is\n    // committed\n    let condition = Arc::new(AtomicBool::new(false));\n    let recv_filter = Box::new(\n        RegionPacketFilter::new(region.get_id(), 2)\n            .direction(Direction::Recv)\n            .when(condition.clone())\n            .set_msg_callback(Arc::new(move |msg: &RaftMessage| {\n                if !condition.load(Ordering::Acquire)\n                    && msg.get_message().get_msg_type() == MessageType::MsgAppend\n                    && !msg.get_message().get_entries().is_empty()\n                {\n                    condition.store(true, Ordering::Release);\n                }\n            })),\n    );\n    cluster.sim.wl().add_recv_filter(2, recv_filter);\n\n    let state1 = cluster.truncated_state(region.get_id(), 1);\n    // Remove log compaction failpoint\n    fail::remove(on_raft_gc_log_tick_fp);\n    // Wait to trigger compact raft log\n    cluster.wait_log_truncated(region.get_id(), 1, state1.get_index() + 1);\n\n    let peer_on_store3 = find_peer(&region, 3).unwrap().to_owned();\n    assert_eq!(peer_on_store3.get_id(), 3);\n    // Make peer 3 do not handle snapshot ready\n    // In previous implementation, destroying its source peer and applying snapshot\n    // is not atomic. So making its source peer be destroyed and do not apply\n    // snapshot to reproduce the problem\n    let before_handle_snapshot_ready_3_fp = \"before_handle_snapshot_ready_3\";\n    fail::cfg(before_handle_snapshot_ready_3_fp, \"return()\").unwrap();\n\n    cluster.clear_send_filters();\n    // Peer 1 will send snapshot to peer 3\n    // Source peer sends msg to others to get target region info until the election\n    // timeout. The max election timeout is 2 * 10 * 10 = 200ms\n    let election_timeout = 2\n        * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n        * cluster.cfg.raft_store.raft_election_timeout_ticks as u64;\n    sleep_ms(election_timeout + 100);\n\n    cluster.stop_node(1);\n    cluster.stop_node(3);\n\n    cluster.sim.wl().clear_recv_filters(2);\n    fail::remove(before_handle_snapshot_ready_3_fp);\n    cluster.run_node(3).unwrap();\n    // Peer 2 will become leader and it don't know the compact log is committed.\n    // So it will send logs not snapshot to peer 3\n    for i in 20..30 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), b\"v\");\n    }\n    must_get_equal(&cluster.get_engine(3), b\"k29\", b\"v\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/service.rs::get_id", "code": "pub fn get_id(&self) -> ConnId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_source_peer_read_delegate_after_apply", "test": "fn test_source_peer_read_delegate_after_apply() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_split(&cluster.get_region(b\"\"), b\"k2\");\n    let target = cluster.get_region(b\"k1\");\n    let source = cluster.get_region(b\"k3\");\n\n    cluster.must_transfer_leader(target.get_id(), find_peer(&target, 1).unwrap().to_owned());\n\n    let on_destroy_peer_fp = \"destroy_peer\";\n    fail::cfg(on_destroy_peer_fp, \"pause\").unwrap();\n\n    // Merge finish means the leader of the target region have call\n    // `on_ready_commit_merge`\n    pd_client.must_merge(source.get_id(), target.get_id());\n\n    // The source peer's `ReadDelegate` should not be removed yet and mark as\n    // `pending_remove`\n    assert!(\n        cluster.store_metas[&1]\n            .lock()\n            .unwrap()\n            .readers\n            .get(&source.get_id())\n            .unwrap()\n            .pending_remove\n    );\n\n    fail::remove(on_destroy_peer_fp);\n    // Wait for source peer is destroyed\n    sleep_ms(100);\n\n    assert!(\n        cluster.store_metas[&1]\n            .lock()\n            .unwrap()\n            .readers\n            .get(&source.get_id())\n            .is_none()\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/service.rs::get_id", "code": "pub fn get_id(&self) -> ConnId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_merge_with_concurrent_pessimistic_locking", "test": "fn test_merge_with_concurrent_pessimistic_locking() {\n    let mut cluster = new_server_cluster(0, 2);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k2\");\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k3\");\n\n    // Transfer the leader of the right region to store 2. The leaders of source and\n    // target regions don't need to be on the same store.\n    cluster.must_transfer_leader(right.id, new_peer(2, 2));\n\n    let snapshot = cluster.must_get_snapshot_of_region(left.id);\n    let txn_ext = snapshot.txn_ext.unwrap();\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(\n            Key::from_raw(b\"k0\"),\n            PessimisticLock {\n                primary: b\"k0\".to_vec().into_boxed_slice(),\n                start_ts: 10.into(),\n                ttl: 3000,\n                for_update_ts: 20.into(),\n                min_commit_ts: 30.into(),\n                last_change_ts: 15.into(),\n                versions_to_last_change: 3,\n            },\n        )])\n        .unwrap();\n\n    let addr = cluster.sim.rl().get_addr(1);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env).connect(&addr);\n    let client = TikvClient::new(channel);\n\n    fail::cfg(\"before_propose_locks_on_region_merge\", \"pause\").unwrap();\n\n    // 1. Locking before proposing pessimistic locks in the source region can\n    // succeed.\n    let client2 = client.clone();\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::PessimisticLock);\n    mutation.key = b\"k1\".to_vec();\n    let mut req = PessimisticLockRequest::default();\n    req.set_context(cluster.get_ctx(b\"k1\"));\n    req.set_mutations(vec![mutation].into());\n    req.set_start_version(10);\n    req.set_for_update_ts(10);\n    req.set_primary_lock(b\"k1\".to_vec());\n    fail::cfg(\"txn_before_process_write\", \"pause\").unwrap();\n    let res = thread::spawn(move || client2.kv_pessimistic_lock(&req).unwrap());\n    thread::sleep(Duration::from_millis(150));\n    cluster.merge_region(left.id, right.id, Callback::None);\n    thread::sleep(Duration::from_millis(150));\n    fail::remove(\"txn_before_process_write\");\n    let resp = res.join().unwrap();\n    assert!(!resp.has_region_error());\n    fail::remove(\"before_propose_locks_on_region_merge\");\n\n    // 2. After locks are proposed, later pessimistic lock request should fail.\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::PessimisticLock);\n    mutation.key = b\"k11\".to_vec();\n    let mut req = PessimisticLockRequest::default();\n    req.set_context(cluster.get_ctx(b\"k11\"));\n    req.set_mutations(vec![mutation].into());\n    req.set_start_version(10);\n    req.set_for_update_ts(10);\n    req.set_primary_lock(b\"k11\".to_vec());\n    fail::cfg(\"txn_before_process_write\", \"pause\").unwrap();\n    let res = thread::spawn(move || client.kv_pessimistic_lock(&req).unwrap());\n    thread::sleep(Duration::from_millis(200));\n    fail::remove(\"txn_before_process_write\");\n    let resp = res.join().unwrap();\n    assert!(resp.has_region_error());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_merge_pessimistic_locks_with_concurrent_prewrite", "test": "fn test_merge_pessimistic_locks_with_concurrent_prewrite() {\n    let mut cluster = new_server_cluster(0, 2);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k2\");\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k3\");\n\n    cluster.must_transfer_leader(right.id, new_peer(2, 2));\n\n    let addr = cluster.sim.rl().get_addr(1);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env).connect(&addr);\n    let client = TikvClient::new(channel);\n\n    let snapshot = cluster.must_get_snapshot_of_region(left.id);\n    let txn_ext = snapshot.txn_ext.unwrap();\n    let lock = PessimisticLock {\n        primary: b\"k0\".to_vec().into_boxed_slice(),\n        start_ts: 10.into(),\n        ttl: 3000,\n        for_update_ts: 20.into(),\n        min_commit_ts: 30.into(),\n        last_change_ts: 15.into(),\n        versions_to_last_change: 3,\n    };\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![\n            (Key::from_raw(b\"k0\"), lock.clone()),\n            (Key::from_raw(b\"k1\"), lock),\n        ])\n        .unwrap();\n\n    let mut mutation = Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.set_key(b\"k0\".to_vec());\n    mutation.set_value(b\"v\".to_vec());\n    let mut req = PrewriteRequest::default();\n    req.set_context(cluster.get_ctx(b\"k0\"));\n    req.set_mutations(vec![mutation].into());\n    req.set_pessimistic_actions(vec![DoPessimisticCheck]);\n    req.set_start_version(10);\n    req.set_for_update_ts(40);\n    req.set_primary_lock(b\"k0\".to_vec());\n\n    // First, pause apply and prewrite.\n    fail::cfg(\"on_handle_apply\", \"pause\").unwrap();\n    let req2 = req.clone();\n    let client2 = client.clone();\n    let resp = thread::spawn(move || client2.kv_prewrite(&req2).unwrap());\n    thread::sleep(Duration::from_millis(500));\n\n    // Then, start merging. PrepareMerge should wait until prewrite is done.\n    cluster.merge_region(left.id, right.id, Callback::None);\n    thread::sleep(Duration::from_millis(500));\n    assert!(txn_ext.pessimistic_locks.read().is_writable());\n\n    // But a later prewrite request should fail because we have already banned all\n    // later proposals.\n    req.mut_mutations()[0].set_key(b\"k1\".to_vec());\n    let resp2 = thread::spawn(move || client.kv_prewrite(&req).unwrap());\n\n    fail::remove(\"on_handle_apply\");\n    let resp = resp.join().unwrap();\n    assert!(!resp.has_region_error(), \"{:?}\", resp);\n\n    let resp2 = resp2.join().unwrap();\n    assert!(resp2.has_region_error());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/txn_ext.rs::is_writable", "code": "pub fn is_writable(&self) -> bool {\n        self.status == LocksStatus::Normal\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_retry_pending_prepare_merge_fail", "test": "fn test_retry_pending_prepare_merge_fail() {\n    let mut cluster = new_server_cluster(0, 2);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k2\");\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k3\");\n\n    cluster.must_transfer_leader(right.id, new_peer(2, 2));\n\n    // Insert lock l1 into the left region\n    let snapshot = cluster.must_get_snapshot_of_region(left.id);\n    let txn_ext = snapshot.txn_ext.unwrap();\n    let l1 = PessimisticLock {\n        primary: b\"k1\".to_vec().into_boxed_slice(),\n        start_ts: 10.into(),\n        ttl: 3000,\n        for_update_ts: 20.into(),\n        min_commit_ts: 30.into(),\n        last_change_ts: 15.into(),\n        versions_to_last_change: 3,\n    };\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(Key::from_raw(b\"k1\"), l1)])\n        .unwrap();\n\n    // Pause apply and write some data to the left region\n    fail::cfg(\"on_handle_apply\", \"pause\").unwrap();\n    let (propose_tx, propose_rx) = mpsc::sync_channel(10);\n    fail::cfg_callback(\"after_propose\", move || propose_tx.send(()).unwrap()).unwrap();\n\n    let mut rx = cluster.async_put(b\"k1\", b\"v11\").unwrap();\n    propose_rx.recv_timeout(Duration::from_secs(2)).unwrap();\n    rx.recv_timeout(Duration::from_millis(200)).unwrap_err();\n\n    // Then, start merging. PrepareMerge should become pending because applied_index\n    // is smaller than proposed_index.\n    cluster.merge_region(left.id, right.id, Callback::None);\n    propose_rx.recv_timeout(Duration::from_secs(2)).unwrap();\n    thread::sleep(Duration::from_millis(200));\n    assert!(txn_ext.pessimistic_locks.read().is_writable());\n\n    // Set disk full error to let PrepareMerge fail. (Set both peer to full to avoid\n    // transferring leader)\n    fail::cfg(\"disk_already_full_peer_1\", \"return\").unwrap();\n    fail::cfg(\"disk_already_full_peer_2\", \"return\").unwrap();\n    fail::remove(\"on_handle_apply\");\n    let res = rx.recv_timeout(Duration::from_secs(1)).unwrap();\n    assert!(!res.get_header().has_error(), \"{:?}\", res);\n\n    propose_rx.recv_timeout(Duration::from_secs(2)).unwrap();\n    fail::remove(\"disk_already_full_peer_1\");\n    fail::remove(\"disk_already_full_peer_2\");\n\n    // Merge should not succeed because the disk is full.\n    thread::sleep(Duration::from_millis(300));\n    cluster.reset_leader_of_region(left.id);\n    assert_eq!(cluster.get_region(b\"k1\"), left);\n\n    cluster.must_put(b\"k1\", b\"v12\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/txn_ext.rs::is_writable", "code": "pub fn is_writable(&self) -> bool {\n        self.status == LocksStatus::Normal\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_merge_pessimistic_locks_propose_fail", "test": "fn test_merge_pessimistic_locks_propose_fail() {\n    let mut cluster = new_server_cluster(0, 2);\n    configure_for_merge(&mut cluster.cfg);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k2\");\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k3\");\n\n    // Sending a TransferLeaeder message to make left region fail to propose.\n\n    let snapshot = cluster.must_get_snapshot_of_region(left.id);\n    let txn_ext = snapshot.ext().get_txn_ext().unwrap().clone();\n    let lock = PessimisticLock {\n        primary: b\"k1\".to_vec().into_boxed_slice(),\n        start_ts: 10.into(),\n        ttl: 3000,\n        for_update_ts: 20.into(),\n        min_commit_ts: 30.into(),\n        last_change_ts: 15.into(),\n        versions_to_last_change: 3,\n    };\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(Key::from_raw(b\"k1\"), lock)])\n        .unwrap();\n\n    fail::cfg(\"raft_propose\", \"pause\").unwrap();\n\n    cluster.merge_region(left.id, right.id, Callback::None);\n    thread::sleep(Duration::from_millis(500));\n    assert_eq!(\n        txn_ext.pessimistic_locks.read().status,\n        LocksStatus::MergingRegion\n    );\n\n    // With the fail point set, we will fail to propose the locks or the\n    // PrepareMerge request.\n    fail::cfg(\"raft_propose\", \"return()\").unwrap();\n\n    // But after that, the pessimistic locks status should remain unchanged.\n    for _ in 0..5 {\n        thread::sleep(Duration::from_millis(500));\n        if txn_ext.pessimistic_locks.read().status == LocksStatus::Normal {\n            return;\n        }\n    }\n    panic!(\n        \"pessimistic locks status should return to Normal, but got {:?}\",\n        txn_ext.pessimistic_locks.read().status\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/external_storage/export/src/dylib.rs::read", "code": "fn read(&self, _name: &str) -> crate::ExternalData<'_> {\n            unimplemented!(\"use restore instead of read\")\n        }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_merge.rs::test_destroy_source_peer_while_merging", "test": "fn test_destroy_source_peer_while_merging() {\n    let mut cluster = new_node_cluster(0, 5);\n    configure_for_merge(&mut cluster.cfg);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n    for i in 1..=5 {\n        must_get_equal(&cluster.get_engine(i), b\"k1\", b\"v1\");\n        must_get_equal(&cluster.get_engine(i), b\"k3\", b\"v3\");\n    }\n\n    cluster.must_split(&pd_client.get_region(b\"k1\").unwrap(), b\"k2\");\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k3\").unwrap();\n    cluster.must_transfer_leader(right.get_id(), new_peer(1, 1));\n\n    let schedule_merge_fp = \"on_schedule_merge\";\n    fail::cfg(schedule_merge_fp, \"return()\").unwrap();\n\n    // Start merge and wait until peer 5 apply prepare merge\n    cluster.must_try_merge(right.get_id(), left.get_id());\n    cluster.must_peer_state(right.get_id(), 5, PeerState::Merging);\n\n    // filter heartbeat and append message for peer 5\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(right.get_id(), 5)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgHeartbeat)\n            .msg_type(MessageType::MsgAppend),\n    ));\n\n    // remove peer from target region to trigger merge rollback.\n    pd_client.must_remove_peer(left.get_id(), find_peer(&left, 2).unwrap().clone());\n    must_get_none(&cluster.get_engine(2), b\"k1\");\n\n    // Merge must rollbacked if we can put more data to the source region\n    fail::remove(schedule_merge_fp);\n    cluster.must_put(b\"k4\", b\"v4\");\n    for i in 1..=4 {\n        must_get_equal(&cluster.get_engine(i), b\"k4\", b\"v4\");\n    }\n\n    // remove peer 5 from peer list so it will destroy itself by tombstone message\n    // and should not persist the `merge_state`\n    pd_client.must_remove_peer(right.get_id(), new_peer(5, 5));\n    must_get_none(&cluster.get_engine(5), b\"k3\");\n\n    // so that other peers will send message to store 5\n    pd_client.must_add_peer(right.get_id(), new_peer(5, 6));\n    // but it is still in tombstone state due to the message filter\n    let state = cluster.region_local_state(right.get_id(), 5);\n    assert_eq!(state.get_state(), PeerState::Tombstone);\n\n    // let the peer on store 4 have a larger peer id\n    pd_client.must_remove_peer(right.get_id(), new_peer(4, 4));\n    pd_client.must_add_peer(right.get_id(), new_peer(4, 7));\n    must_get_equal(&cluster.get_engine(4), b\"k4\", b\"v4\");\n\n    // if store 5 have persist the merge state, peer 2 and peer 3 will be destroyed\n    // because store 5 will response their request vote message with a gc\n    // message, and peer 7 will cause store 5 panic because peer 7 have larger\n    // peer id than the peer in the merge state\n    cluster.clear_send_filters();\n    cluster.add_send_filter(IsolationFilterFactory::new(1));\n\n    cluster.must_put(b\"k5\", b\"v5\");\n    assert!(!state.has_merge_state(), \"{:?}\", state);\n    for i in 2..=5 {\n        must_get_equal(&cluster.get_engine(i), b\"k5\", b\"v5\");\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_state", "code": "pub fn get_state(&self) -> Arc<AtomicCell<DownstreamState>> {\n        self.state.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_pd_client_legacy.rs::test_load_global_config", "test": "fn test_load_global_config() {\n    let (mut _server, client) = new_test_server_and_client(ReadableDuration::millis(100));\n    let global_items = vec![(\"test1\", \"val1\"), (\"test2\", \"val2\"), (\"test3\", \"val3\")];\n    let check_items = global_items.clone();\n    if let Err(err) = futures::executor::block_on(\n        client.store_global_config(\n            String::from(\"global\"),\n            global_items\n                .iter()\n                .map(|(name, value)| {\n                    let mut item = GlobalConfigItem::default();\n                    item.set_name(name.to_string());\n                    item.set_payload(value.as_bytes().into());\n                    item\n                })\n                .collect::<Vec<GlobalConfigItem>>(),\n        ),\n    ) {\n        panic!(\"error occur {:?}\", err);\n    }\n\n    let (res, revision) =\n        futures::executor::block_on(client.load_global_config(String::from(\"global\"))).unwrap();\n    assert!(\n        res.iter()\n            .zip(check_items)\n            .all(|(item1, item2)| item1.name == item2.0 && item1.payload == item2.1.as_bytes())\n    );\n    assert_eq!(revision, 3);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::as_bytes", "code": "pub fn as_bytes(&self) -> Option<BytesRef<'_>> {\n        EvaluableRef::borrow_scalar_value(self)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_pd_client_legacy.rs::test_watch_global_config_on_closed_server", "test": "fn test_watch_global_config_on_closed_server() {\n    let (mut server, client) = new_test_server_and_client(ReadableDuration::millis(100));\n    let global_items = vec![(\"test1\", \"val1\"), (\"test2\", \"val2\"), (\"test3\", \"val3\")];\n    let items_clone = global_items.clone();\n\n    let client = Arc::new(client);\n    let cli_clone = client.clone();\n    use futures::StreamExt;\n    let background_worker = Builder::new(\"background\").thread_count(1).create();\n    background_worker.spawn_async_task(async move {\n        match cli_clone.watch_global_config(\"global\".into(), 0) {\n            Ok(mut stream) => {\n                let mut i: usize = 0;\n                while let Some(grpc_response) = stream.next().await {\n                    match grpc_response {\n                        Ok(r) => {\n                            for item in r.get_changes() {\n                                assert_eq!(item.get_name(), items_clone[i].0);\n                                assert_eq!(\n                                    from_utf8(item.get_payload()).unwrap(),\n                                    items_clone[i].1\n                                );\n                                i += 1;\n                            }\n                        }\n                        Err(err) => panic!(\"failed to get stream, err: {:?}\", err),\n                    }\n                }\n            }\n            Err(err) => {\n                if !err.to_string().contains(\"UNAVAILABLE\") {\n                    // Not 14-UNAVAILABLE\n                    panic!(\"other error occur {:?}\", err)\n                }\n            }\n        }\n    });\n\n    if let Err(err) = futures::executor::block_on(\n        client.store_global_config(\n            \"global\".into(),\n            global_items\n                .iter()\n                .map(|(name, value)| {\n                    let mut item = GlobalConfigItem::default();\n                    item.set_name(name.to_string());\n                    item.set_payload(value.as_bytes().into());\n                    item\n                })\n                .collect::<Vec<GlobalConfigItem>>(),\n        ),\n    ) {\n        panic!(\"error occur {:?}\", err);\n    }\n\n    thread::sleep(Duration::from_millis(100));\n    server.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/tests/benches/coprocessor_executors/util/mod.rs::get_name", "code": "fn get_name(&self) -> &'static str {\n        self.name\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_pd_client_legacy.rs::test_reconnect_limit", "test": "fn test_reconnect_limit() {\n    let pd_client_reconnect_fp = \"pd_client_reconnect\";\n    let (_server, client) = new_test_server_and_client(ReadableDuration::secs(100));\n\n    // The GLOBAL_RECONNECT_INTERVAL is 0.1s so sleeps 0.2s here.\n    thread::sleep(Duration::from_millis(200));\n\n    // The first reconnection will succeed, and the last_update will not be updated.\n    fail::cfg(pd_client_reconnect_fp, \"return\").unwrap();\n    client.reconnect().unwrap();\n    // The subsequent reconnection will be cancelled.\n    for _ in 0..10 {\n        let ret = client.reconnect();\n        assert!(format!(\"{:?}\", ret.unwrap_err()).contains(\"cancel reconnection\"));\n    }\n\n    fail::remove(pd_client_reconnect_fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_pending_peers.rs::test_pending_peers", "test": "fn test_pending_peers() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.pd_heartbeat_tick_interval = ReadableDuration::millis(100);\n\n    let region_worker_fp = \"region_apply_snap\";\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer count check.\n    pd_client.disable_default_operator();\n\n    let region_id = cluster.run_conf_change();\n    pd_client.must_add_peer(region_id, new_peer(2, 2));\n\n    // To ensure peer 2 is not pending.\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n\n    fail::cfg(region_worker_fp, \"sleep(2000)\").unwrap();\n    pd_client.must_add_peer(region_id, new_peer(3, 3));\n    sleep_ms(1000);\n    let pending_peers = pd_client.get_pending_peers();\n    // Region worker is not started, snapshot should not be applied yet.\n    assert_eq!(pending_peers[&3], new_peer(3, 3));\n    // But it will be applied finally.\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    sleep_ms(100);\n    let pending_peers = pd_client.get_pending_peers();\n    assert!(pending_peers.is_empty());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tikv_util/src/store/peer.rs::new_peer", "code": "pub fn new_peer(store_id: u64, peer_id: u64) -> Peer {\n    let mut peer = Peer::default();\n    peer.set_store_id(store_id);\n    peer.set_id(peer_id);\n    peer.set_role(PeerRole::Voter);\n    peer\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_pending_peers.rs::test_pending_snapshot", "test": "fn test_pending_snapshot() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_snapshot(&mut cluster.cfg);\n    let election_timeout = configure_for_lease_read(&mut cluster.cfg, None, Some(15));\n    let gc_limit = cluster.cfg.raft_store.raft_log_gc_count_limit();\n    cluster.cfg.raft_store.pd_heartbeat_tick_interval = ReadableDuration::millis(100);\n\n    let handle_snapshot_fp = \"apply_on_handle_snapshot_1_1\";\n    let handle_snapshot_finish_fp = \"apply_on_handle_snapshot_finish_1_1\";\n    fail::cfg(\"apply_on_handle_snapshot_sync\", \"return\").unwrap();\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer count check.\n    pd_client.disable_default_operator();\n\n    let region_id = cluster.run_conf_change();\n    pd_client.must_add_peer(region_id, new_peer(2, 2));\n    cluster.must_transfer_leader(region_id, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    fail::cfg(handle_snapshot_fp, \"pause\").unwrap();\n    pd_client.must_add_peer(region_id, new_peer(3, 3));\n    // Give some time for peer 3 to request snapshot.\n    sleep_ms(100);\n\n    // Isolate peer 1 from rest of the cluster.\n    cluster.add_send_filter(IsolationFilterFactory::new(1));\n\n    sleep_ms((election_timeout.as_millis() * 2) as _);\n    cluster.reset_leader_of_region(region_id);\n    // Compact logs to force requesting snapshot after clearing send filters.\n    let state2 = cluster.truncated_state(1, 2);\n    for i in 1..gc_limit * 10 {\n        let k = i.to_string().into_bytes();\n        cluster.must_put(&k, &k.clone());\n    }\n    cluster.wait_log_truncated(1, 2, state2.get_index() + 5 * gc_limit);\n\n    // Make sure peer 1 has applied snapshot.\n    cluster.clear_send_filters();\n    let start = Instant::now();\n    loop {\n        if cluster.pd_client.get_pending_peers().get(&1).is_none()\n            || start.saturating_elapsed() > election_timeout * 10\n        {\n            break;\n        }\n        sleep_ms(50);\n    }\n    let state1 = cluster.truncated_state(1, 1);\n\n    // Peer 2 continues to handle snapshot.\n    fail::cfg(handle_snapshot_finish_fp, \"pause\").unwrap();\n    fail::remove(handle_snapshot_fp);\n    sleep_ms(200);\n    let state2 = cluster.truncated_state(1, 1);\n    fail::remove(handle_snapshot_finish_fp);\n    assert!(\n        state1.get_term() <= state2.get_term(),\n        \"{:?} {:?}\",\n        state1,\n        state2\n    );\n    assert!(\n        state1.get_index() <= state2.get_index(),\n        \"{:?} {:?}\",\n        state1,\n        state2\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tikv_kv/src/raftstore_impls.rs::get_term", "code": "fn get_term(&self) -> Option<NonZeroU64> {\n        self.snapshot.term\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_rawkv.rs::test_leader_transfer", "test": "fn test_leader_transfer() {\n    let mut suite = TestSuite::new(3, ApiVersion::V2);\n    let key1 = b\"rk1\";\n    let region = suite.cluster.get_region(key1);\n\n    // Transfer leader and write to store 1.\n    {\n        suite.must_transfer_leader(&region, 1);\n        let leader1 = suite.must_leader_on_store(key1, 1);\n\n        suite.must_raw_put(key1, b\"v1\");\n        suite.must_raw_put(key1, b\"v2\");\n        suite.must_raw_put(key1, b\"v3\");\n        suite.flush_timestamp(leader1.get_store_id()); // Flush to make ts bigger than other stores.\n        suite.must_raw_put(key1, b\"v4\");\n        assert_eq!(suite.must_raw_get(key1), Some(b\"v4\".to_vec()));\n    }\n\n    // Make causal_ts_provider.async_flush() & handle_update_max_timestamp fail.\n    fail::cfg(FP_GET_TSO, \"return(50)\").unwrap();\n\n    // Transfer leader and write to store 2.\n    {\n        suite.must_transfer_leader(&region, 2);\n        suite.must_leader_on_store(key1, 2);\n\n        // Store 2 has a TSO batch smaller than store 1.\n        suite.raw_put_err_by_timestamp_not_synced(key1, b\"v5\");\n        assert_eq!(suite.must_raw_get(key1), Some(b\"v4\".to_vec()));\n        suite.raw_put_err_by_timestamp_not_synced(key1, b\"v6\");\n        assert_eq!(suite.must_raw_get(key1), Some(b\"v4\".to_vec()));\n    }\n\n    // Transfer leader back.\n    suite.must_transfer_leader(&region, 1);\n    suite.must_leader_on_store(key1, 1);\n    // Make handle_update_max_timestamp succeed.\n    fail::cfg(FP_GET_TSO, \"off\").unwrap();\n    // Transfer leader and write to store 2 again.\n    {\n        suite.must_transfer_leader(&region, 2);\n        suite.must_leader_on_store(key1, 2);\n\n        suite.must_raw_put(key1, b\"v7\");\n        assert_eq!(suite.must_raw_get(key1), Some(b\"v7\".to_vec()));\n        suite.must_raw_put(key1, b\"v8\");\n        assert_eq!(suite.must_raw_get(key1), Some(b\"v8\".to_vec()));\n    }\n\n    fail::remove(FP_GET_TSO);\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_backup/src/lib.rs::must_raw_get", "code": "pub fn must_raw_get(&self, k: Vec<u8>, cf: String) -> Vec<u8> {\n        let mut request = RawGetRequest::default();\n        let mut context = self.context.clone();\n        if context.api_version == ApiVersion::V1ttl {\n            context.api_version = ApiVersion::V1;\n        }\n        request.set_context(context);\n        request.set_key(k);\n        request.set_cf(cf);\n        let mut response = self.tikv_cli.raw_get(&request).unwrap();\n        retry_req!(\n            self.tikv_cli.raw_get(&request).unwrap(),\n            !response.has_region_error() && response.error.is_empty(),\n            response,\n            10,   // retry 10 times\n            1000  // 1s timeout\n        );\n        assert!(response.error.is_empty(), \"{:?}\", response.get_error());\n        response.take_value()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_rawkv.rs::test_region_merge", "test": "fn test_region_merge() {\n    let mut suite = TestSuite::new(3, ApiVersion::V2);\n    let keys = vec![b\"rk0\", b\"rk1\", b\"rk2\", b\"rk3\", b\"rk4\", b\"rk5\"];\n\n    suite.must_raw_put(keys[1], b\"v1\");\n    suite.must_raw_put(keys[3], b\"v3\");\n    suite.must_raw_put(keys[5], b\"v5\");\n\n    // Split to: region1: (-, 2), region3: [2, 4), region5: [4, +)\n    let region1 = suite.cluster.get_region(keys[1]);\n    suite.cluster.must_split(&region1, keys[2]);\n    let region1 = suite.cluster.get_region(keys[1]);\n    let region3 = suite.cluster.get_region(keys[3]);\n    suite.cluster.must_split(&region3, keys[4]);\n    let region3 = suite.cluster.get_region(keys[3]);\n    let region5 = suite.cluster.get_region(keys[5]);\n    assert_eq!(region1.get_end_key(), region3.get_start_key());\n    assert_eq!(region3.get_end_key(), region5.get_start_key());\n\n    // Transfer leaders: region 1 -> store 1, region 3 -> store 2, region 5 -> store\n    // 3.\n    suite.must_transfer_leader(&region1, 1);\n    suite.must_transfer_leader(&region3, 2);\n    suite.must_transfer_leader(&region5, 3);\n\n    // Write to region 1.\n    {\n        let leader1 = suite.must_leader_on_store(keys[1], 1);\n\n        suite.must_raw_put(keys[1], b\"v2\");\n        suite.must_raw_put(keys[1], b\"v3\");\n        suite.flush_timestamp(leader1.get_store_id()); // Flush to make ts of store 1 larger than others.\n        suite.must_raw_put(keys[1], b\"v4\");\n        assert_eq!(suite.must_raw_get(keys[1]), Some(b\"v4\".to_vec()));\n    }\n\n    // Make causal_ts_provider.async_flush() & handle_update_max_timestamp fail.\n    fail::cfg(FP_GET_TSO, \"return(50)\").unwrap();\n\n    // Merge region 1 to 3.\n    {\n        suite.must_merge_region_by_key(keys[1], keys[3]);\n        suite.must_leader_on_store(keys[1], 2);\n\n        // Write to store 2. Store 2 has a TSO batch smaller than store 1.\n        suite.raw_put_err_by_timestamp_not_synced(keys[1], b\"v5\");\n        assert_eq!(suite.must_raw_get(keys[1]), Some(b\"v4\".to_vec()));\n        suite.raw_put_err_by_timestamp_not_synced(keys[1], b\"v6\");\n        assert_eq!(suite.must_raw_get(keys[1]), Some(b\"v4\".to_vec()));\n    }\n\n    // Make handle_update_max_timestamp succeed.\n    fail::cfg(FP_GET_TSO, \"off\").unwrap();\n\n    // Merge region 3 to 5.\n    {\n        suite.must_merge_region_by_key(keys[3], keys[5]);\n        suite.must_leader_on_store(keys[1], 3);\n\n        // Write to store 3.\n        suite.must_raw_put(keys[1], b\"v7\");\n        assert_eq!(suite.must_raw_get(keys[1]), Some(b\"v7\".to_vec()));\n        suite.must_raw_put(keys[1], b\"v8\");\n        assert_eq!(suite.must_raw_get(keys[1]), Some(b\"v8\".to_vec()));\n    }\n\n    fail::remove(FP_GET_TSO);\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/region_snapshot.rs::get_end_key", "code": "pub fn get_end_key(&self) -> &[u8] {\n        self.region.get_end_key()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_rawkv.rs::test_raw_put_key_guard", "test": "fn test_raw_put_key_guard() {\n    let mut suite = TestSuite::new(3, ApiVersion::V2);\n    let pause_write_fp = \"raftkv_async_write\";\n\n    let test_key = b\"rk3\".to_vec();\n    let test_value = b\"v3\".to_vec();\n\n    let region = suite.cluster.get_region(&test_key);\n    let region_id = region.get_id();\n    let client = suite.get_client(region_id);\n    let ctx = suite.get_context(region_id);\n    let node_id = region.get_peers()[0].get_id();\n    let leader_cm = suite.cluster.sim.rl().get_concurrency_manager(node_id);\n    let ts_provider = suite.get_causal_ts_provider(node_id).unwrap();\n    let ts = block_on(ts_provider.async_get_ts()).unwrap();\n\n    let copy_test_key = test_key.clone();\n    let copy_test_value = test_value.clone();\n    fail::cfg(pause_write_fp, \"pause\").unwrap();\n    let handle = thread::spawn(move || {\n        must_raw_put(&client, ctx, copy_test_key, copy_test_value);\n    });\n\n    // Wait for global_min_lock_ts.\n    sleep_ms(500);\n    let start = Instant::now();\n    while leader_cm.global_min_lock_ts().is_none()\n        && start.saturating_elapsed() < Duration::from_secs(5)\n    {\n        sleep_ms(200);\n    }\n\n    // Before raw_put finish, min_ts should be the ts of \"key guard\" of the raw_put\n    // request.\n    assert_eq!(suite.must_raw_get(&test_key), None);\n    let min_ts = leader_cm.global_min_lock_ts();\n    assert_eq!(min_ts.unwrap(), ts.next());\n\n    fail::remove(pause_write_fp);\n    handle.join().unwrap();\n\n    // After raw_put is finished, \"key guard\" is released.\n    assert_eq!(suite.must_raw_get(&test_key), Some(test_value));\n    let min_ts = leader_cm.global_min_lock_ts();\n    assert!(min_ts.is_none());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_backup/src/lib.rs::must_raw_get", "code": "pub fn must_raw_get(&self, k: Vec<u8>, cf: String) -> Vec<u8> {\n        let mut request = RawGetRequest::default();\n        let mut context = self.context.clone();\n        if context.api_version == ApiVersion::V1ttl {\n            context.api_version = ApiVersion::V1;\n        }\n        request.set_context(context);\n        request.set_key(k);\n        request.set_cf(cf);\n        let mut response = self.tikv_cli.raw_get(&request).unwrap();\n        retry_req!(\n            self.tikv_cli.raw_get(&request).unwrap(),\n            !response.has_region_error() && response.error.is_empty(),\n            response,\n            10,   // retry 10 times\n            1000  // 1s timeout\n        );\n        assert!(response.error.is_empty(), \"{:?}\", response.get_error());\n        response.take_value()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_replica_read.rs::test_read_after_cleanup_range_for_snap", "test": "fn test_read_after_cleanup_range_for_snap() {\n    let mut cluster = new_server_cluster(1, 3);\n    configure_for_snapshot(&mut cluster.cfg);\n    configure_for_lease_read(&mut cluster.cfg, Some(100), Some(10));\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    // Set region and peers\n    let r1 = cluster.run_conf_change();\n    let p1 = new_peer(1, 1);\n    let p2 = new_peer(2, 2);\n    cluster.pd_client.must_add_peer(r1, p2.clone());\n    let p3 = new_peer(3, 3);\n    cluster.pd_client.must_add_peer(r1, p3.clone());\n    cluster.must_put(b\"k0\", b\"v0\");\n    cluster.pd_client.must_none_pending_peer(p2);\n    cluster.pd_client.must_none_pending_peer(p3.clone());\n    let region = cluster.get_region(b\"k0\");\n    assert_eq!(cluster.leader_of_region(region.get_id()).unwrap(), p1);\n    must_get_equal(&cluster.get_engine(3), b\"k0\", b\"v0\");\n    cluster.stop_node(3);\n    let last_index = cluster.raft_local_state(r1, 1).last_index;\n    (0..10).for_each(|_| cluster.must_put(b\"k1\", b\"v1\"));\n    // Ensure logs are compacted, then node 1 will send a snapshot to node 3 later\n    cluster.wait_log_truncated(r1, 1, last_index + 1);\n\n    fail::cfg(\"send_snapshot\", \"pause\").unwrap();\n    cluster.run_node(3).unwrap();\n    // Sleep for a while to ensure peer 3 receives a HeartBeat\n    thread::sleep(Duration::from_millis(500));\n\n    // Add filter for delaying ReadIndexResp and MsgSnapshot\n    let (read_index_sx, read_index_rx) = channel::unbounded::<RaftMessage>();\n    let (snap_sx, snap_rx) = channel::unbounded::<RaftMessage>();\n    let recv_filter = Box::new(\n        RegionPacketFilter::new(region.get_id(), 3)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgSnapshot)\n            .set_msg_callback(Arc::new(move |msg: &RaftMessage| {\n                snap_sx.send(msg.clone()).unwrap();\n            })),\n    );\n    let send_read_index_filter = RegionPacketFilter::new(region.get_id(), 3)\n        .direction(Direction::Recv)\n        .msg_type(MessageType::MsgReadIndexResp)\n        .set_msg_callback(Arc::new(move |msg: &RaftMessage| {\n            read_index_sx.send(msg.clone()).unwrap();\n        }));\n    cluster.sim.wl().add_recv_filter(3, recv_filter);\n    cluster.add_send_filter(CloneFilterFactory(send_read_index_filter));\n    fail::remove(\"send_snapshot\");\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cf_cmd(\"default\", b\"k0\")],\n        false,\n    );\n    request.mut_header().set_peer(p3);\n    request.mut_header().set_replica_read(true);\n    // Send follower read request to peer 3\n    let (cb1, mut rx1) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(3, request, cb1)\n        .unwrap();\n    let read_index_msg = read_index_rx.recv_timeout(Duration::from_secs(5)).unwrap();\n    let snap_msg = snap_rx.recv_timeout(Duration::from_secs(5)).unwrap();\n\n    fail::cfg(\"apply_snap_cleanup_range\", \"pause\").unwrap();\n\n    let router = cluster.sim.wl().get_router(3).unwrap();\n    fail::cfg(\"pause_on_peer_collect_message\", \"pause\").unwrap();\n    cluster.sim.wl().clear_recv_filters(3);\n    cluster.clear_send_filters();\n    router.send_raft_message(snap_msg).unwrap();\n    router.send_raft_message(read_index_msg).unwrap();\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    fail::remove(\"pause_on_peer_collect_message\");\n    must_get_none(&cluster.get_engine(3), b\"k0\");\n    // Should not receive resp\n    rx1.recv_timeout(Duration::from_millis(500)).unwrap_err();\n    fail::remove(\"apply_snap_cleanup_range\");\n    rx1.recv_timeout(Duration::from_secs(5)).unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/service.rs::get_id", "code": "pub fn get_id(&self) -> ConnId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_server.rs::test_mismatch_store_node", "test": "fn test_mismatch_store_node() {\n    let count = 3;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n    let node_ids = cluster.get_node_ids();\n    let mut iter = node_ids.iter();\n    let node1_id = *iter.next().unwrap();\n    let node2_id = *iter.next().unwrap();\n    let node3_id = *iter.next().unwrap();\n    let pd_client = cluster.pd_client.clone();\n    must_get_equal(&cluster.get_engine(node1_id), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(node2_id), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(node3_id), b\"k1\", b\"v1\");\n    let node1_addr = pd_client\n        .get_store(node1_id)\n        .unwrap()\n        .get_address()\n        .to_string();\n    let node2_addr = pd_client\n        .get_store(node2_id)\n        .unwrap()\n        .get_address()\n        .to_string();\n    let node3_addr = cluster\n        .pd_client\n        .get_store(node3_id)\n        .unwrap()\n        .get_address()\n        .to_string();\n    cluster.stop_node(node2_id);\n    cluster.stop_node(node3_id);\n    // run node2\n    cluster.cfg.server.addr = node3_addr.clone();\n    cluster.run_node(node2_id).unwrap();\n    let filter = RegionPacketFilter::new(1, node2_id)\n        .direction(Direction::Send)\n        .msg_type(MessageType::MsgRequestPreVote);\n    cluster.add_send_filter(CloneFilterFactory(filter));\n    // run node3\n    cluster.cfg.server.addr = node2_addr.clone();\n    cluster.run_node(node3_id).unwrap();\n    let filter = RegionPacketFilter::new(1, node3_id)\n        .direction(Direction::Send)\n        .msg_type(MessageType::MsgRequestPreVote);\n    cluster.add_send_filter(CloneFilterFactory(filter));\n    sleep_ms(600);\n    fail::cfg(\"mock_store_refresh_interval_secs\", \"return(0)\").unwrap();\n    cluster.must_put(b\"k2\", b\"v2\");\n    assert_eq!(\n        node1_addr,\n        pd_client.get_store(node1_id).unwrap().get_address()\n    );\n    assert_eq!(\n        node3_addr,\n        pd_client.get_store(node2_id).unwrap().get_address()\n    );\n    assert_eq!(\n        node2_addr,\n        cluster.pd_client.get_store(node3_id).unwrap().get_address()\n    );\n    must_get_equal(&cluster.get_engine(node3_id), b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(node2_id), b\"k2\", b\"v2\");\n    fail::remove(\"mock_store_refresh_interval_secs\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/server/resolve.rs::get_address", "code": "fn get_address(&self, store_id: u64) -> Result<String> {\n        let pd_client = Arc::clone(&self.pd_client);\n        let mut s = match pd_client.get_store(store_id) {\n            Ok(s) => s,\n            // `get_store` will filter tombstone store, so here needs to handle\n            // it explicitly.\n            Err(pd_client::Error::StoreTombstone(_)) => {\n                RESOLVE_STORE_COUNTER_STATIC.tombstone.inc();\n                return Err(box_err!(\"store {} has been removed\", store_id));\n            }\n            Err(e) => return Err(box_err!(e)),\n        };\n        let mut group_id = None;\n        let mut state = self.state.lock().unwrap();\n        if state.status().get_mode() == ReplicationMode::DrAutoSync {\n            let state_id = state.status().get_dr_auto_sync().state_id;\n            if state.group.group_id(state_id, store_id).is_none() {\n                group_id = state.group.register_store(store_id, s.take_labels().into());\n            }\n        } else {\n            state.group.backup_store_labels(&mut s);\n        }\n        drop(state);\n        if let Some(group_id) = group_id {\n            self.router.report_resolved(store_id, group_id);\n        }\n        let addr = take_peer_address(&mut s);\n        // In some tests, we use empty address for store first,\n        // so we should ignore here.\n        // TODO: we may remove this check after we refactor the test.\n        if addr.is_empty() {\n            return Err(box_err!(\"invalid empty address for store {}\", store_id));\n        }\n        Ok(addr)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_server.rs::test_serving_status", "test": "fn test_serving_status() {\n    let mut cluster = new_server_cluster(0, 3);\n    // A round is 30 ticks, set inspect interval to 20ms, so one round is 0.3s.\n    cluster.cfg.raft_store.inspect_interval = ReadableDuration::millis(10);\n    cluster.run();\n\n    let service = cluster.sim.rl().health_services.get(&1).unwrap().clone();\n    let builder =\n        ServerBuilder::new(Arc::new(Environment::new(1))).register_service(create_health(service));\n    let mut server = builder.bind(\"127.0.0.1\", 0).build().unwrap();\n    server.start();\n\n    let (addr, port) = server.bind_addrs().next().unwrap();\n    let ch =\n        ChannelBuilder::new(Arc::new(Environment::new(1))).connect(&format!(\"{}:{}\", addr, port));\n    let client = HealthClient::new(ch);\n\n    let check = || {\n        let req = HealthCheckRequest {\n            service: \"\".to_string(),\n            ..Default::default()\n        };\n        let resp = client.check(&req).unwrap();\n        resp.status\n    };\n\n    thread::sleep(Duration::from_millis(500));\n    assert_eq!(check(), ServingStatus::Serving);\n\n    fail::cfg(\"pause_on_peer_collect_message\", \"pause\").unwrap();\n\n    thread::sleep(Duration::from_secs(1));\n    assert_eq!(check(), ServingStatus::ServiceUnknown);\n\n    fail::remove(\"pause_on_peer_collect_message\");\n\n    // It should recover within one round.\n    thread::sleep(Duration::from_millis(200));\n    assert_eq!(check(), ServingStatus::Serving);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/security/src/lib.rs::check", "code": "fn check(&mut self, ctx: &RpcContext<'_>) -> CheckResult {\n        match check_common_name(&self.allowed_cn, ctx) {\n            Ok(()) => CheckResult::Continue,\n            Err(reason) => CheckResult::Abort(RpcStatus::with_message(\n                RpcStatusCode::UNAUTHENTICATED,\n                format!(\n                    \"Common name check fail, reason: {}, cert_allowed_cn: {:?}\",\n                    reason, self.allowed_cn\n                ),\n            )),\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_sst_recovery.rs::test_sst_recovery_basic", "test": "fn test_sst_recovery_basic() {\n    let (mut cluster, pd_client, engine1) = create_tikv_cluster_with_one_node_damaged();\n\n    // Test that only sst recovery can delete the sst file, remove peer don't delete\n    // it.\n    fail::cfg(\"sst_recovery_before_delete_files\", \"pause\").unwrap();\n\n    let store_meta = cluster.store_metas.get(&1).unwrap().clone();\n    std::thread::sleep(CHECK_DURATION);\n    assert_eq!(\n        store_meta\n            .lock()\n            .unwrap()\n            .get_all_damaged_region_ids()\n            .len(),\n        2\n    );\n\n    // Remove peers for safe deletion of files in sst recovery.\n    let region = cluster.get_region(b\"2\");\n    let peer = find_peer(&region, 1).unwrap();\n    pd_client.must_remove_peer(region.id, peer.clone());\n    let region = cluster.get_region(b\"4\");\n    let peer = find_peer(&region, 1).unwrap();\n    pd_client.must_remove_peer(region.id, peer.clone());\n\n    // Read from other store must success.\n    assert_eq!(cluster.must_get(b\"4\").unwrap(), b\"val\");\n\n    std::thread::sleep(CHECK_DURATION);\n\n    must_get_equal(&engine1, b\"1\", b\"val\");\n    must_get_equal(&engine1, b\"7\", b\"val\");\n    assert_corruption(engine1.get_value(b\"z4\"));\n\n    fail::remove(\"sst_recovery_before_delete_files\");\n    std::thread::sleep(CHECK_DURATION);\n\n    must_get_equal(&engine1, b\"1\", b\"val\");\n    must_get_equal(&engine1, b\"7\", b\"val\");\n    assert!(engine1.get_value(b\"z4\").unwrap().is_none());\n\n    // Damaged file has been deleted.\n    let files = engine1.as_inner().get_live_files();\n    assert_eq!(files.get_files_count(), 2);\n    assert_eq!(store_meta.lock().unwrap().damaged_ranges.len(), 0);\n\n    // only store 1 remove peer so key \"4\" should be accessed by cluster.\n    assert_eq!(cluster.must_get(b\"4\").unwrap(), b\"val\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/txn_ext.rs::len", "code": "pub fn len(&self) -> usize {\n        self.map.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_stale_peer.rs::test_node_update_localreader_after_removed", "test": "fn test_node_update_localreader_after_removed() {\n    let mut cluster = new_node_cluster(0, 6);\n    let pd_client = cluster.pd_client.clone();\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n    let r1 = cluster.run_conf_change();\n\n    // Add 4 peers.\n    for i in 2..6 {\n        pd_client.must_add_peer(r1, new_peer(i, i));\n    }\n\n    // Make sure peer 1 leads the region.\n    cluster.must_transfer_leader(r1, new_peer(1, 1));\n    let (key, value) = (b\"k1\", b\"v1\");\n    cluster.must_put(key, value);\n    assert_eq!(cluster.get(key), Some(value.to_vec()));\n\n    // Make sure peer 2 is initialized.\n    let engine_2 = cluster.get_engine(2);\n    must_get_equal(&engine_2, key, value);\n\n    // Pause peer 2 apply worker if it executes AddNode.\n    let add_node_fp = \"apply_on_add_node_1_2\";\n    fail::cfg(add_node_fp, \"pause\").unwrap();\n\n    // Add peer 6.\n    pd_client.must_add_peer(r1, new_peer(6, 6));\n\n    // Isolate peer 2 from rest of the cluster.\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n\n    // Remove peer 2, so it will receive a gc msssage\n    // after max_leader_missing_duration timeout.\n    pd_client.must_remove_peer(r1, new_peer(2, 2));\n    thread::sleep(cluster.cfg.raft_store.max_leader_missing_duration.0 * 2);\n\n    // Continue peer 2 apply worker, so that peer 2 tries to\n    // update region to its read delegate.\n    fail::remove(add_node_fp);\n\n    // Make sure peer 2 is removed in node 2.\n    cluster.must_region_not_exist(r1, 2);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::get", "code": "pub fn get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, false)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_stale_peer.rs::test_destroy_uninitialized_peer_when_there_exists_old_peer", "test": "fn test_destroy_uninitialized_peer_when_there_exists_old_peer() {\n    // 4 stores cluster.\n    let mut cluster = new_node_cluster(0, 4);\n    cluster.cfg.raft_store.pd_store_heartbeat_tick_interval = ReadableDuration::millis(10);\n    cluster.cfg.raft_store.hibernate_regions = false;\n\n    let pd_client = cluster.pd_client.clone();\n    // Disable default max peer count check.\n    pd_client.disable_default_operator();\n\n    let r1 = cluster.run_conf_change();\n\n    // Now region 1 only has peer (1, 1);\n    let (key, value) = (b\"k1\", b\"v1\");\n\n    cluster.must_put(key, value);\n    assert_eq!(cluster.get(key), Some(value.to_vec()));\n\n    // add peer (2,2) to region 1.\n    pd_client.must_add_peer(r1, new_peer(2, 2));\n\n    // add peer (3, 3) to region 1.\n    pd_client.must_add_peer(r1, new_peer(3, 3));\n\n    let epoch = pd_client.get_region_epoch(r1);\n\n    // Conf version must change.\n    assert!(epoch.get_conf_ver() > 2);\n\n    // Transfer leader to peer (2, 2).\n    cluster.must_transfer_leader(r1, new_peer(2, 2));\n\n    // Isolate node 1\n    cluster.add_send_filter(IsolationFilterFactory::new(1));\n\n    cluster.must_put(format!(\"k{}\", 2).as_bytes(), b\"v1\");\n\n    // Remove 3 and add 4\n    pd_client.must_add_peer(r1, new_learner_peer(4, 4));\n    pd_client.must_add_peer(r1, new_peer(4, 4));\n    pd_client.must_remove_peer(r1, new_peer(3, 3));\n\n    cluster.must_put(format!(\"k{}\", 3).as_bytes(), b\"v1\");\n\n    // Ensure 5 drops all snapshot\n    let (notify_tx, _notify_rx) = mpsc::channel();\n    cluster\n        .sim\n        .wl()\n        .add_recv_filter(3, Box::new(DropSnapshotFilter::new(notify_tx)));\n\n    // Add learner 5 on store 3\n    pd_client.must_add_peer(r1, new_learner_peer(3, 5));\n\n    cluster.must_put(format!(\"k{}\", 4).as_bytes(), b\"v1\");\n\n    // Remove and destroy the uninitialized 5\n    let peer_5 = new_learner_peer(3, 5);\n    pd_client.must_remove_peer(r1, peer_5.clone());\n    cluster.must_gc_peer(r1, 3, peer_5);\n\n    let region = block_on(pd_client.get_region_by_id(r1)).unwrap();\n    must_region_cleared(&cluster.get_all_engines(3), &region.unwrap());\n\n    // Unisolate 1 and try wakeup 3\n    cluster.clear_send_filters();\n    cluster.sim.wl().clear_recv_filters(3);\n    cluster.partition(vec![1, 3], vec![2, 4]);\n\n    sleep_until_election_triggered(&cluster.cfg);\n\n    let region = block_on(pd_client.get_region_by_id(r1)).unwrap();\n    must_region_cleared(&cluster.get_all_engines(3), &region.unwrap());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::get", "code": "pub fn get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, false)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_stale_peer.rs::test_destroy_clean_up_logs_with_unfinished_log_gc", "test": "fn test_destroy_clean_up_logs_with_unfinished_log_gc() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(15);\n    cluster.cfg.raft_store.raft_log_gc_threshold = 15;\n    let pd_client = cluster.pd_client.clone();\n\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n    cluster.run();\n    // Simulate raft log gc tasks are lost during shutdown.\n    let fp = \"worker_gc_raft_log\";\n    fail::cfg(fp, \"return\").unwrap();\n\n    let state = cluster.truncated_state(1, 3);\n    for i in 0..30 {\n        let b = format!(\"k{}\", i).into_bytes();\n        cluster.must_put(&b, &b);\n    }\n    must_get_equal(&cluster.get_engine(3), b\"k29\", b\"k29\");\n    cluster.wait_log_truncated(1, 3, state.get_index() + 1);\n    cluster.stop_node(3);\n    let truncated_index = cluster.truncated_state(1, 3).get_index();\n    let raft_engine = cluster.engines[&3].raft.clone();\n    // Make sure there are stale logs.\n    raft_engine.get_entry(1, truncated_index).unwrap().unwrap();\n\n    pd_client.must_remove_peer(1, new_peer(3, 3));\n    cluster.must_put(b\"k30\", b\"v30\");\n    must_get_equal(&cluster.get_engine(1), b\"k30\", b\"v30\");\n\n    fail::remove(fp);\n    // So peer (3, 3) will be destroyed by gc message. And all stale logs before\n    // first index should be cleaned up.\n    cluster.run_node(3).unwrap();\n    must_get_none(&cluster.get_engine(3), b\"k29\");\n\n    let mut dest = vec![];\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    // All logs should be deleted.\n    assert!(dest.is_empty(), \"{:?}\", dest);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/txn/store.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.entries.len() == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_stale_read.rs::test_read_after_peer_destroyed", "test": "fn test_read_after_peer_destroyed() {\n    let mut cluster = new_node_cluster(0, 3);\n    let pd_client = cluster.pd_client.clone();\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n    let r1 = cluster.run_conf_change();\n\n    // Add 2 peers.\n    for i in 2..4 {\n        pd_client.must_add_peer(r1, new_peer(i, i));\n    }\n\n    // Make sure peer 1 leads the region.\n    cluster.must_transfer_leader(r1, new_peer(1, 1));\n    let (key, value) = (b\"k1\", b\"v1\");\n    cluster.must_put(key, value);\n    assert_eq!(cluster.get(key), Some(value.to_vec()));\n\n    let destroy_peer_fp = \"destroy_peer\";\n    fail::cfg(destroy_peer_fp, \"pause\").unwrap();\n    pd_client.must_remove_peer(r1, new_peer(1, 1));\n    sleep_ms(300);\n\n    // Try writing k2 to peer3\n    let mut request = new_request(\n        r1,\n        cluster.pd_client.get_region_epoch(r1),\n        vec![new_get_cmd(b\"k1\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    // Wait for raftstore receives the read request.\n    sleep_ms(200);\n    fail::remove(destroy_peer_fp);\n\n    let resp = rx.recv_timeout(Duration::from_millis(200)).unwrap();\n    assert!(\n        resp.get_header().get_error().has_region_not_found(),\n        \"{:?}\",\n        resp\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::get", "code": "pub fn get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, false)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_stats.rs::test_bucket_stats", "test": "fn test_bucket_stats() {\n    let (mut cluster, client, ctx) = must_new_and_configure_cluster_and_kv_client(|cluster| {\n        cluster.cfg.coprocessor.enable_region_bucket = Some(true);\n        cluster.cfg.raft_store.split_region_check_tick_interval = ReadableDuration::days(1);\n        cluster.cfg.raft_store.report_region_buckets_tick_interval = ReadableDuration::millis(100);\n    });\n\n    let fp = \"mock_tick_interval\";\n    fail::cfg(fp, \"return(0)\").unwrap();\n\n    sleep_ms(200);\n    let mut keys = Vec::with_capacity(50);\n    for i in 0..50u8 {\n        let key = vec![b'k', i];\n        cluster.must_put(&key, &[b' '; 4]);\n        cluster.must_get(&[b'k', i]);\n        keys.push(key);\n    }\n    let mut req = RawBatchGetRequest::default();\n    req.set_context(ctx);\n    req.set_keys(protobuf::RepeatedField::from(keys));\n    client.raw_batch_get(&req).unwrap();\n    sleep_ms(600);\n    let buckets = cluster.must_get_buckets(1);\n    assert_eq!(buckets.meta.keys.len(), 2);\n    assert_eq!(buckets.stats.get_write_keys(), [50]);\n    assert_eq!(buckets.stats.get_write_bytes(), [50 * (4 + 2)]);\n    assert_eq!(buckets.stats.get_read_keys(), [50]);\n    assert_eq!(buckets.stats.get_read_bytes(), [50 * (4 + 2)]);\n    fail::remove(fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_server_catching_api_error", "test": "fn test_server_catching_api_error() {\n    let raftkv_fp = \"raftkv_early_error_report\";\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n    let region = cluster.get_region(b\"\");\n    let leader = region.get_peers()[0].clone();\n\n    fail::cfg(raftkv_fp, \"return\").unwrap();\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n\n    let mut prewrite_req = PrewriteRequest::default();\n    prewrite_req.set_context(ctx.clone());\n    let mutation = kvrpcpb::Mutation {\n        op: Op::Put,\n        key: b\"k3\".to_vec(),\n        value: b\"v3\".to_vec(),\n        ..Default::default()\n    };\n    prewrite_req.set_mutations(vec![mutation].into_iter().collect());\n    prewrite_req.primary_lock = b\"k3\".to_vec();\n    prewrite_req.start_version = 1;\n    prewrite_req.lock_ttl = prewrite_req.start_version + 1;\n    let prewrite_resp = client.kv_prewrite(&prewrite_req).unwrap();\n    assert!(prewrite_resp.has_region_error(), \"{:?}\", prewrite_resp);\n    assert!(\n        prewrite_resp.get_region_error().has_region_not_found(),\n        \"{:?}\",\n        prewrite_resp\n    );\n    must_get_none(&cluster.get_engine(1), b\"k3\");\n\n    let mut put_req = RawPutRequest::default();\n    put_req.set_context(ctx);\n    put_req.key = b\"k3\".to_vec();\n    put_req.value = b\"v3\".to_vec();\n    let put_resp = client.raw_put(&put_req).unwrap();\n    assert!(put_resp.has_region_error(), \"{:?}\", put_resp);\n    assert!(\n        put_resp.get_region_error().has_region_not_found(),\n        \"{:?}\",\n        put_resp\n    );\n    must_get_none(&cluster.get_engine(1), b\"k3\");\n\n    fail::remove(raftkv_fp);\n    let put_resp = client.raw_put(&put_req).unwrap();\n    assert!(!put_resp.has_region_error(), \"{:?}\", put_resp);\n    must_get_equal(&cluster.get_engine(1), b\"k3\", b\"v3\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_raftkv_early_error_report", "test": "fn test_raftkv_early_error_report() {\n    let raftkv_fp = \"raftkv_early_error_report\";\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n    cluster.must_split(&cluster.get_region(b\"k0\"), b\"k1\");\n\n    let env = Arc::new(Environment::new(1));\n    let mut clients: HashMap<&[u8], (Context, TikvClient)> = HashMap::default();\n    for &k in &[b\"k0\", b\"k1\"] {\n        let region = cluster.get_region(k);\n        let leader = region.get_peers()[0].clone();\n        let mut ctx = Context::default();\n        let channel = ChannelBuilder::new(env.clone())\n            .connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n        let client = TikvClient::new(channel);\n        ctx.set_region_id(region.get_id());\n        ctx.set_region_epoch(region.get_region_epoch().clone());\n        ctx.set_peer(leader);\n        clients.insert(k, (ctx, client));\n    }\n\n    // Inject error to all regions.\n    fail::cfg(raftkv_fp, \"return\").unwrap();\n    for (k, (ctx, client)) in &clients {\n        let mut put_req = RawPutRequest::default();\n        put_req.set_context(ctx.clone());\n        put_req.key = k.to_vec();\n        put_req.value = b\"v\".to_vec();\n        let put_resp = client.raw_put(&put_req).unwrap();\n        assert!(put_resp.has_region_error(), \"{:?}\", put_resp);\n        assert!(\n            put_resp.get_region_error().has_region_not_found(),\n            \"{:?}\",\n            put_resp\n        );\n        must_get_none(&cluster.get_engine(1), k);\n    }\n    fail::remove(raftkv_fp);\n\n    // Inject only one region\n    let injected_region_id = clients[b\"k0\".as_ref()].0.get_region_id();\n    fail::cfg(raftkv_fp, &format!(\"return({})\", injected_region_id)).unwrap();\n    for (k, (ctx, client)) in &clients {\n        let mut put_req = RawPutRequest::default();\n        put_req.set_context(ctx.clone());\n        put_req.key = k.to_vec();\n        put_req.value = b\"v\".to_vec();\n        let put_resp = client.raw_put(&put_req).unwrap();\n        if ctx.get_region_id() == injected_region_id {\n            assert!(put_resp.has_region_error(), \"{:?}\", put_resp);\n            assert!(\n                put_resp.get_region_error().has_region_not_found(),\n                \"{:?}\",\n                put_resp\n            );\n            must_get_none(&cluster.get_engine(1), k);\n        } else {\n            assert!(!put_resp.has_region_error(), \"{:?}\", put_resp);\n            must_get_equal(&cluster.get_engine(1), k, b\"v\");\n        }\n    }\n    fail::remove(raftkv_fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_scale_scheduler_pool", "test": "fn test_scale_scheduler_pool() {\n    let snapshot_fp = \"scheduler_start_execute\";\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n    let origin_pool_size = cluster.cfg.storage.scheduler_worker_pool_size;\n\n    let engine = cluster\n        .sim\n        .read()\n        .unwrap()\n        .storages\n        .get(&1)\n        .unwrap()\n        .clone();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .config(cluster.cfg.tikv.storage.clone())\n        .build()\n        .unwrap();\n\n    let cfg = new_tikv_config(1);\n    let kv_engine = storage.get_engine().kv_engine().unwrap();\n    let (_tx, rx) = std::sync::mpsc::channel();\n    let flow_controller = Arc::new(FlowController::Singleton(EngineFlowController::new(\n        &cfg.storage.flow_control,\n        kv_engine.clone(),\n        rx,\n    )));\n\n    let cfg_controller = ConfigController::new(cfg);\n    let (scheduler, _receiver) = dummy_scheduler();\n    cfg_controller.register(\n        Module::Storage,\n        Box::new(StorageConfigManger::new(\n            kv_engine,\n            scheduler,\n            flow_controller,\n            storage.get_scheduler(),\n        )),\n    );\n    let scheduler = storage.get_scheduler();\n\n    let region = cluster.get_region(b\"k1\");\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.id);\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(cluster.leader_of_region(region.id).unwrap());\n    let do_prewrite = |key: &[u8], val: &[u8]| {\n        // prewrite\n        let (prewrite_tx, prewrite_rx) = channel();\n        storage\n            .sched_txn_command(\n                commands::Prewrite::new(\n                    vec![Mutation::make_put(Key::from_raw(key), val.to_vec())],\n                    key.to_vec(),\n                    10.into(),\n                    100,\n                    false,\n                    2,\n                    TimeStamp::default(),\n                    TimeStamp::default(),\n                    None,\n                    false,\n                    AssertionLevel::Off,\n                    ctx.clone(),\n                ),\n                Box::new(move |res: storage::Result<_>| {\n                    let _ = prewrite_tx.send(res);\n                }),\n            )\n            .unwrap();\n        prewrite_rx.recv_timeout(Duration::from_secs(2))\n    };\n\n    let scale_pool = |size: usize| {\n        cfg_controller\n            .update_config(\"storage.scheduler-worker-pool-size\", &format!(\"{}\", size))\n            .unwrap();\n        assert_eq!(\n            scheduler.get_sched_pool().get_pool_size(CommandPri::Normal),\n            size\n        );\n    };\n\n    scale_pool(1);\n    fail::cfg(snapshot_fp, \"1*pause\").unwrap();\n    // propose one prewrite to block the only worker\n    do_prewrite(b\"k1\", b\"v1\").unwrap_err();\n\n    scale_pool(2);\n\n    // do prewrite again, as we scale another worker, this request should success\n    do_prewrite(b\"k2\", b\"v2\").unwrap().unwrap();\n\n    // restore to original config.\n    scale_pool(origin_pool_size);\n    fail::remove(snapshot_fp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/txn/sched_pool.rs::get_pool_size", "code": "fn get_pool_size(&self, priority_level: CommandPri) -> usize {\n        if priority_level == CommandPri::High {\n            self.high_worker_pool.get_pool_size()\n        } else {\n            self.worker_pool.get_pool_size()\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_scheduler_pool_auto_switch_for_resource_ctl", "test": "fn test_scheduler_pool_auto_switch_for_resource_ctl() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n\n    let engine = cluster\n        .sim\n        .read()\n        .unwrap()\n        .storages\n        .get(&1)\n        .unwrap()\n        .clone();\n    let resource_manager = ResourceGroupManager::default();\n    let resource_ctl = resource_manager.derive_controller(\"test\".to_string(), true);\n\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .config(cluster.cfg.tikv.storage.clone())\n        .build_for_resource_controller(resource_ctl)\n        .unwrap();\n\n    let region = cluster.get_region(b\"k1\");\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.id);\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(cluster.leader_of_region(region.id).unwrap());\n\n    let do_prewrite = |key: &[u8], val: &[u8]| {\n        // prewrite\n        let (prewrite_tx, prewrite_rx) = channel();\n        storage\n            .sched_txn_command(\n                commands::Prewrite::new(\n                    vec![Mutation::make_put(Key::from_raw(key), val.to_vec())],\n                    key.to_vec(),\n                    10.into(),\n                    100,\n                    false,\n                    2,\n                    TimeStamp::default(),\n                    TimeStamp::default(),\n                    None,\n                    false,\n                    AssertionLevel::Off,\n                    ctx.clone(),\n                ),\n                Box::new(move |res: storage::Result<_>| {\n                    let _ = prewrite_tx.send(res);\n                }),\n            )\n            .unwrap();\n        prewrite_rx.recv_timeout(Duration::from_secs(2))\n    };\n\n    let (sender, receiver) = channel();\n    let priority_queue_sender = Mutex::new(sender.clone());\n    let single_queue_sender = Mutex::new(sender);\n    fail::cfg_callback(\"priority_pool_task\", move || {\n        let sender = priority_queue_sender.lock().unwrap();\n        sender.send(\"priority_queue\").unwrap();\n    })\n    .unwrap();\n    fail::cfg_callback(\"single_queue_pool_task\", move || {\n        let sender = single_queue_sender.lock().unwrap();\n        sender.send(\"single_queue\").unwrap();\n    })\n    .unwrap();\n\n    // Default is use single queue\n    assert_eq!(do_prewrite(b\"k1\", b\"v1\").is_ok(), true);\n    assert_eq!(\n        receiver.recv_timeout(Duration::from_millis(500)).unwrap(),\n        \"single_queue\"\n    );\n\n    // Add group use priority queue\n    use kvproto::resource_manager::{GroupMode, GroupRequestUnitSettings, ResourceGroup};\n    let mut group = ResourceGroup::new();\n    group.set_name(\"rg1\".to_string());\n    group.set_mode(GroupMode::RuMode);\n    let mut ru_setting = GroupRequestUnitSettings::new();\n    ru_setting.mut_r_u().mut_settings().set_fill_rate(100000);\n    group.set_r_u_settings(ru_setting);\n    resource_manager.add_resource_group(group);\n    thread::sleep(Duration::from_millis(200));\n    assert_eq!(do_prewrite(b\"k2\", b\"v2\").is_ok(), true);\n    assert_eq!(\n        receiver.recv_timeout(Duration::from_millis(500)).unwrap(),\n        \"priority_queue\"\n    );\n\n    // Delete group use single queue\n    resource_manager.remove_resource_group(\"rg1\");\n    thread::sleep(Duration::from_millis(200));\n    assert_eq!(do_prewrite(b\"k3\", b\"v3\").is_ok(), true);\n    assert_eq!(\n        receiver.recv_timeout(Duration::from_millis(500)).unwrap(),\n        \"single_queue\"\n    );\n\n    // Scale pool size\n    let scheduler = storage.get_scheduler();\n    let pool = scheduler.get_sched_pool();\n    assert_eq!(pool.get_pool_size(CommandPri::Normal), 1);\n    pool.scale_pool_size(2);\n    assert_eq!(pool.get_pool_size(CommandPri::Normal), 2);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_pd/src/mocker/retry.rs::is_ok", "code": "fn is_ok(&self) -> bool {\n        let count = self.count.fetch_add(1, Ordering::SeqCst);\n        if count != 0 && count % self.retry == 0 {\n            // it's ok.\n            return true;\n        }\n        // let's sleep awhile, so that client will update its connection.\n        thread::sleep(REQUEST_RECONNECT_INTERVAL);\n        false\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_async_apply_prewrite_fallback", "test": "fn test_async_apply_prewrite_fallback() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n\n    let engine = cluster\n        .sim\n        .read()\n        .unwrap()\n        .storages\n        .get(&1)\n        .unwrap()\n        .clone();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .async_apply_prewrite(true)\n        .build()\n        .unwrap();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(1);\n    ctx.set_region_epoch(cluster.get_region_epoch(1));\n    ctx.set_peer(cluster.leader_of_region(1).unwrap());\n\n    let before_async_apply_prewrite_finish = \"before_async_apply_prewrite_finish\";\n    let on_handle_apply = \"on_handle_apply\";\n\n    fail::cfg(before_async_apply_prewrite_finish, \"return()\").unwrap();\n    fail::cfg(on_handle_apply, \"pause\").unwrap();\n\n    let (key, value) = (b\"k1\", b\"v1\");\n    let (tx, rx) = channel();\n    storage\n        .sched_txn_command(\n            commands::Prewrite::new(\n                vec![Mutation::make_put(Key::from_raw(key), value.to_vec())],\n                key.to_vec(),\n                10.into(),\n                0,\n                false,\n                1,\n                0.into(),\n                0.into(),\n                Some(vec![]),\n                false,\n                AssertionLevel::Off,\n                ctx.clone(),\n            ),\n            Box::new(move |r| tx.send(r).unwrap()),\n        )\n        .unwrap();\n\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(200)).unwrap_err(),\n        RecvTimeoutError::Timeout\n    );\n\n    fail::remove(on_handle_apply);\n\n    let res = rx.recv().unwrap().unwrap();\n    assert!(res.min_commit_ts > 10.into());\n\n    fail::remove(before_async_apply_prewrite_finish);\n\n    let (tx, rx) = channel();\n    storage\n        .sched_txn_command(\n            commands::Commit::new(vec![Key::from_raw(key)], 10.into(), res.min_commit_ts, ctx),\n            Box::new(move |r| tx.send(r).unwrap()),\n        )\n        .unwrap();\n\n    rx.recv_timeout(Duration::from_secs(5)).unwrap().unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_atomic_cas_lock_by_latch", "test": "fn test_atomic_cas_lock_by_latch() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n\n    let engine = cluster\n        .sim\n        .read()\n        .unwrap()\n        .storages\n        .get(&1)\n        .unwrap()\n        .clone();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .build()\n        .unwrap();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(1);\n    ctx.set_region_epoch(cluster.get_region_epoch(1));\n    ctx.set_peer(cluster.leader_of_region(1).unwrap());\n\n    let latch_acquire_success_fp = \"txn_scheduler_acquire_success\";\n    let latch_acquire_fail_fp = \"txn_scheduler_acquire_fail\";\n    let pending_cas_fp = \"txn_commands_compare_and_swap\";\n    let wakeup_latch_fp = \"txn_scheduler_try_to_wake_up\";\n    let acquire_flag = Arc::new(AtomicBool::new(false));\n    let acquire_flag1 = acquire_flag.clone();\n    let acquire_flag_fail = Arc::new(AtomicBool::new(false));\n    let acquire_flag_fail1 = acquire_flag_fail.clone();\n    let wakeup_latch_flag = Arc::new(AtomicBool::new(false));\n    let wakeup1 = wakeup_latch_flag.clone();\n\n    fail::cfg(pending_cas_fp, \"pause\").unwrap();\n    fail::cfg_callback(latch_acquire_success_fp, move || {\n        acquire_flag1.store(true, Ordering::Release);\n    })\n    .unwrap();\n    fail::cfg_callback(latch_acquire_fail_fp, move || {\n        acquire_flag_fail1.store(true, Ordering::Release);\n    })\n    .unwrap();\n    fail::cfg_callback(wakeup_latch_fp, move || {\n        wakeup1.store(true, Ordering::Release);\n    })\n    .unwrap();\n    let (cb, f1) = paired_future_callback();\n    storage\n        .raw_compare_and_swap_atomic(\n            ctx.clone(),\n            \"\".to_string(),\n            b\"key\".to_vec(),\n            None,\n            b\"v1\".to_vec(),\n            0,\n            cb,\n        )\n        .unwrap();\n    thread::sleep(Duration::from_secs(1));\n    assert!(acquire_flag.load(Ordering::Acquire));\n    assert!(!acquire_flag_fail.load(Ordering::Acquire));\n    acquire_flag.store(false, Ordering::Release);\n    let (cb, f2) = paired_future_callback();\n    storage\n        .raw_compare_and_swap_atomic(\n            ctx.clone(),\n            \"\".to_string(),\n            b\"key\".to_vec(),\n            Some(b\"v1\".to_vec()),\n            b\"v2\".to_vec(),\n            0,\n            cb,\n        )\n        .unwrap();\n    thread::sleep(Duration::from_secs(1));\n    assert!(acquire_flag_fail.load(Ordering::Acquire));\n    assert!(!acquire_flag.load(Ordering::Acquire));\n    fail::remove(pending_cas_fp);\n    let _ = block_on(f1).unwrap();\n    let (prev_val, succeed) = block_on(f2).unwrap().unwrap();\n    assert!(wakeup_latch_flag.load(Ordering::Acquire));\n    assert!(succeed);\n    assert_eq!(prev_val, Some(b\"v1\".to_vec()));\n    let f = storage.raw_get(ctx, \"\".to_string(), b\"key\".to_vec());\n    let ret = block_on(f).unwrap().unwrap();\n    assert_eq!(b\"v2\".to_vec(), ret);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_mvcc_concurrent_commit_and_rollback_at_shutdown", "test": "fn test_mvcc_concurrent_commit_and_rollback_at_shutdown() {\n    let (mut cluster, mut client, mut ctx) = must_new_cluster_and_kv_client_mul(3);\n    let k = b\"key\".to_vec();\n    // Use big value to force it in default cf.\n    let v = vec![0; 10240];\n\n    let mut ts = 0;\n\n    // Prewrite\n    ts += 1;\n    let prewrite_start_version = ts;\n    let mut mutation = kvrpcpb::Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.set_key(k.clone());\n    mutation.set_value(v.clone());\n    must_kv_prewrite(\n        &client,\n        ctx.clone(),\n        vec![mutation],\n        k.clone(),\n        prewrite_start_version,\n    );\n\n    // So all following operation will not be committed by this leader.\n    let leader_fp = \"before_leader_handle_committed_entries\";\n    fail::cfg(leader_fp, \"pause\").unwrap();\n\n    // Commit\n    ts += 1;\n    let commit_version = ts;\n    let mut commit_req = CommitRequest::default();\n    commit_req.set_context(ctx.clone());\n    commit_req.start_version = prewrite_start_version;\n    commit_req.mut_keys().push(k.clone());\n    commit_req.commit_version = commit_version;\n    let _commit_resp = client.kv_commit_async(&commit_req).unwrap();\n\n    // Rollback\n    let rollback_start_version = prewrite_start_version;\n    let mut rollback_req = BatchRollbackRequest::default();\n    rollback_req.set_context(ctx.clone());\n    rollback_req.start_version = rollback_start_version;\n    rollback_req.mut_keys().push(k.clone());\n    let _rollback_resp = client.kv_batch_rollback_async(&rollback_req).unwrap();\n\n    // Sleep some time to make sure both commit and rollback are queued in latch.\n    thread::sleep(Duration::from_millis(100));\n    let shutdown_fp = \"after_shutdown_apply\";\n    fail::cfg_callback(shutdown_fp, move || {\n        fail::remove(leader_fp);\n        // Sleep some time to ensure all logs can be replicated.\n        thread::sleep(Duration::from_millis(300));\n    })\n    .unwrap();\n    let mut leader = cluster.leader_of_region(1).unwrap();\n    cluster.stop_node(leader.get_store_id());\n\n    // So a new leader should be elected.\n    cluster.must_put(b\"k2\", b\"v2\");\n    leader = cluster.leader_of_region(1).unwrap();\n    ctx.set_peer(leader.clone());\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    client = TikvClient::new(channel);\n\n    // The first request is commit, the second is rollback, the first one should\n    // succeed.\n    ts += 1;\n    let get_version = ts;\n    let mut get_req = GetRequest::default();\n    get_req.set_context(ctx);\n    get_req.key = k;\n    get_req.version = get_version;\n    let get_resp = client.kv_get(&get_req).unwrap();\n    assert!(\n        !get_resp.has_region_error() && !get_resp.has_error(),\n        \"{:?}\",\n        get_resp\n    );\n    assert_eq!(get_resp.value, v);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_storage.rs::test_raw_put_deadline", "test": "fn test_raw_put_deadline() {\n    let deadline_fp = \"deadline_check_fail\";\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n    let region = cluster.get_region(b\"\");\n    let leader = region.get_peers()[0].clone();\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n\n    let mut put_req = RawPutRequest::default();\n    put_req.set_context(ctx);\n    put_req.key = b\"k3\".to_vec();\n    put_req.value = b\"v3\".to_vec();\n    fail::cfg(deadline_fp, \"return()\").unwrap();\n    let put_resp = client.raw_put(&put_req).unwrap();\n    assert!(put_resp.has_region_error(), \"{:?}\", put_resp);\n    must_get_none(&cluster.get_engine(1), b\"k3\");\n\n    fail::remove(deadline_fp);\n    let put_resp = client.raw_put(&put_req).unwrap();\n    assert!(!put_resp.has_region_error(), \"{:?}\", put_resp);\n    must_get_equal(&cluster.get_engine(1), b\"k3\", b\"v3\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_table_properties.rs::test_check_need_gc", "test": "fn test_check_need_gc() {\n    GC_COMPACTION_FILTER_PERFORM.reset();\n    GC_COMPACTION_FILTER_SKIP.reset();\n\n    let mut cfg = DbConfig::default();\n    cfg.defaultcf.disable_auto_compactions = true;\n    cfg.defaultcf.dynamic_level_bytes = false;\n    let dir = tempfile::TempDir::new().unwrap();\n    let builder = TestEngineBuilder::new().path(dir.path());\n    let engine = builder\n        .api_version(ApiVersion::V2)\n        .build_with_cfg(&cfg)\n        .unwrap();\n    let raw_engine = engine.get_rocksdb();\n    let mut gc_runner = TestGcRunner::new(0);\n\n    do_write(&engine, false, 5);\n\n    // Check init value\n    assert_eq!(\n        GC_COMPACTION_FILTER_PERFORM\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        0\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTER_SKIP\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        0\n    );\n\n    // TEST 1: If ratio_threshold < 1.0 || context.is_bottommost_level() is true,\n    // check_need_gc return true, call dofilter\n    gc_runner\n        .safe_point(TimeStamp::max().into_inner())\n        .gc_raw(&raw_engine);\n\n    assert_eq!(\n        GC_COMPACTION_FILTER_PERFORM\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        1\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTER_SKIP\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        0\n    );\n\n    // TEST 2: props.num_versions as f64 > props.num_rows as f64 * ratio_threshold\n    // return true.\n    do_write(&engine, false, 5);\n    engine.get_rocksdb().flush_cfs(&[], true).unwrap();\n\n    do_gc(&raw_engine, 2, &mut gc_runner, &dir);\n\n    do_write(&engine, false, 5);\n    engine.get_rocksdb().flush_cfs(&[], true).unwrap();\n\n    // Set ratio_threshold, let (props.num_versions as f64 > props.num_rows as\n    // f64 * ratio_threshold) return true\n    gc_runner.ratio_threshold = Option::Some(0.0f64);\n\n    // is_bottommost_level = false\n    do_gc(&raw_engine, 1, &mut gc_runner, &dir);\n\n    assert_eq!(\n        GC_COMPACTION_FILTER_PERFORM\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        3\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTER_SKIP\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        0\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/table_properties.rs::get", "code": "fn get(&self, _: &[u8]) -> Option<&[u8]> {\n        None\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_table_properties.rs::test_skip_gc_by_check", "test": "fn test_skip_gc_by_check() {\n    GC_COMPACTION_FILTER_PERFORM.reset();\n    GC_COMPACTION_FILTER_SKIP.reset();\n\n    let mut cfg = DbConfig::default();\n    cfg.defaultcf.disable_auto_compactions = true;\n    cfg.defaultcf.dynamic_level_bytes = false;\n    cfg.defaultcf.num_levels = 7;\n    let dir = tempfile::TempDir::new().unwrap();\n    let builder = TestEngineBuilder::new().path(dir.path());\n    let engine = builder\n        .api_version(ApiVersion::V2)\n        .build_with_cfg(&cfg)\n        .unwrap();\n    let raw_engine = engine.get_rocksdb();\n    let mut gc_runner = TestGcRunner::new(0);\n\n    do_write(&engine, false, 5);\n    engine.get_rocksdb().flush_cfs(&[], true).unwrap();\n\n    // The min_mvcc_ts ts > gc safepoint, check_need_gc return false, don't call\n    // dofilter\n    gc_runner\n        .safe_point(TimeStamp::new(1).into_inner())\n        .gc_raw(&raw_engine);\n    assert_eq!(\n        GC_COMPACTION_FILTER_PERFORM\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        1\n    );\n    assert_eq!(\n        GC_COMPACTION_FILTER_SKIP\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        1\n    );\n\n    // TEST 2:When is_bottommost_level = false,\n    // write data to level2\n    do_write(&engine, false, 5);\n    engine.get_rocksdb().flush_cfs(&[], true).unwrap();\n\n    do_gc(&raw_engine, 2, &mut gc_runner, &dir);\n\n    do_write(&engine, false, 5);\n    engine.get_rocksdb().flush_cfs(&[], true).unwrap();\n\n    // Set ratio_threshold, let (props.num_versions as f64 > props.num_rows as\n    // f64 * ratio_threshold) return false\n    gc_runner.ratio_threshold = Option::Some(f64::MAX);\n\n    // is_bottommost_level = false\n    do_gc(&raw_engine, 1, &mut gc_runner, &dir);\n\n    assert_eq!(\n        GC_COMPACTION_FILTER_PERFORM\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        3\n    );\n\n    // The check_need_gc return false, GC_COMPACTION_FILTER_SKIP will add 1.\n    assert_eq!(\n        GC_COMPACTION_FILTER_SKIP\n            .with_label_values(&[STAT_RAW_KEYMODE])\n            .get(),\n        2\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_panic/src/table_properties.rs::get", "code": "fn get(&self, _: &[u8]) -> Option<&[u8]> {\n        None\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_max_commit_ts_error", "test": "fn test_max_commit_ts_error() {\n    let engine = TestEngineBuilder::new().build().unwrap();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .build()\n        .unwrap();\n    let cm = storage.get_concurrency_manager();\n\n    fail::cfg(\"after_prewrite_one_key\", \"sleep(500)\").unwrap();\n    let (prewrite_tx, prewrite_rx) = channel();\n    storage\n        .sched_txn_command(\n            commands::Prewrite::new(\n                vec![\n                    Mutation::make_put(Key::from_raw(b\"k1\"), b\"v\".to_vec()),\n                    Mutation::make_put(Key::from_raw(b\"k2\"), b\"v\".to_vec()),\n                ],\n                b\"k1\".to_vec(),\n                10.into(),\n                20000,\n                false,\n                2,\n                TimeStamp::default(),\n                100.into(),\n                Some(vec![b\"k2\".to_vec()]),\n                false,\n                AssertionLevel::Off,\n                Context::default(),\n            ),\n            Box::new(move |res| {\n                prewrite_tx.send(res).unwrap();\n            }),\n        )\n        .unwrap();\n    thread::sleep(Duration::from_millis(200));\n    cm.read_key_check(&Key::from_raw(b\"k1\"), |_| Err(()))\n        .unwrap_err();\n    cm.update_max_ts(200.into());\n\n    let res = prewrite_rx.recv().unwrap().unwrap();\n    assert!(res.min_commit_ts.is_zero());\n    assert!(res.one_pc_commit_ts.is_zero());\n\n    // There should not be any memory lock left.\n    cm.read_range_check(None, None, |_, _| Err(())).unwrap();\n\n    // Two locks should be written, the second one does not async commit.\n    let l1 = must_locked(&mut storage.get_engine(), b\"k1\", 10);\n    let l2 = must_locked(&mut storage.get_engine(), b\"k2\", 10);\n    assert!(l1.use_async_commit);\n    assert!(!l2.use_async_commit);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::is_zero", "code": "pub fn is_zero(&self) -> bool {\n        let len = word_cnt!(self.int_cnt) + word_cnt!(self.frac_cnt);\n        self.word_buf[0..len as usize].iter().all(|&x| x == 0)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_exceed_max_commit_ts_in_the_middle_of_prewrite", "test": "fn test_exceed_max_commit_ts_in_the_middle_of_prewrite() {\n    let engine = TestEngineBuilder::new().build().unwrap();\n    let storage = TestStorageBuilderApiV1::from_engine_and_lock_mgr(engine, MockLockManager::new())\n        .build()\n        .unwrap();\n    let cm = storage.get_concurrency_manager();\n\n    let (prewrite_tx, prewrite_rx) = channel();\n    // Pause between getting max ts and store the lock in memory\n    fail::cfg(\"before-set-lock-in-memory\", \"pause\").unwrap();\n\n    cm.update_max_ts(40.into());\n    let mutations = vec![\n        Mutation::make_put(Key::from_raw(b\"k1\"), b\"v\".to_vec()),\n        Mutation::make_put(Key::from_raw(b\"k2\"), b\"v\".to_vec()),\n    ];\n    storage\n        .sched_txn_command(\n            commands::Prewrite::new(\n                mutations.clone(),\n                b\"k1\".to_vec(),\n                10.into(),\n                20000,\n                false,\n                2,\n                11.into(),\n                50.into(),\n                Some(vec![]),\n                false,\n                AssertionLevel::Off,\n                Context::default(),\n            ),\n            Box::new(move |res| {\n                prewrite_tx.send(res).unwrap();\n            }),\n        )\n        .unwrap();\n    // sleep a while so the first key gets max ts.\n    thread::sleep(Duration::from_millis(200));\n\n    cm.update_max_ts(51.into());\n    fail::remove(\"before-set-lock-in-memory\");\n    let res = prewrite_rx.recv().unwrap().unwrap();\n    assert!(res.min_commit_ts.is_zero());\n    assert!(res.one_pc_commit_ts.is_zero());\n\n    let locks = block_on(storage.scan_lock(\n        Context::default(),\n        20.into(),\n        Some(Key::from_raw(b\"k1\")),\n        None,\n        2,\n    ))\n    .unwrap();\n    assert_eq!(locks.len(), 2);\n    assert_eq!(locks[0].get_key(), b\"k1\");\n    assert!(locks[0].get_use_async_commit());\n    assert_eq!(locks[0].get_min_commit_ts(), 41);\n    assert_eq!(locks[1].get_key(), b\"k2\");\n    assert!(!locks[1].get_use_async_commit());\n\n    // Send a duplicated request to test the idempotency of prewrite when falling\n    // back to 2PC.\n    let (prewrite_tx, prewrite_rx) = channel();\n    storage\n        .sched_txn_command(\n            commands::Prewrite::new(\n                mutations,\n                b\"k1\".to_vec(),\n                10.into(),\n                20000,\n                false,\n                2,\n                11.into(),\n                50.into(),\n                Some(vec![]),\n                false,\n                AssertionLevel::Off,\n                Context::default(),\n            ),\n            Box::new(move |res| {\n                prewrite_tx.send(res).unwrap();\n            }),\n        )\n        .unwrap();\n    let res = prewrite_rx.recv().unwrap().unwrap();\n    assert!(res.min_commit_ts.is_zero());\n    assert!(res.one_pc_commit_ts.is_zero());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::is_zero", "code": "pub fn is_zero(&self) -> bool {\n        let len = word_cnt!(self.int_cnt) + word_cnt!(self.frac_cnt);\n        self.word_buf[0..len as usize].iter().all(|&x| x == 0)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_pessimistic_lock_check_valid", "test": "fn test_pessimistic_lock_check_valid() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.cfg.pessimistic_txn.pipelined = true;\n    cluster.cfg.pessimistic_txn.in_memory = true;\n    cluster.run();\n\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    let txn_ext = cluster\n        .must_get_snapshot_of_region(1)\n        .ext()\n        .get_txn_ext()\n        .unwrap()\n        .clone();\n\n    let region = cluster.get_region(b\"\");\n    let leader = region.get_peers()[0].clone();\n    fail::cfg(\"acquire_pessimistic_lock\", \"pause\").unwrap();\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));\n    let client = TikvClient::new(channel);\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n\n    let mut mutation = pb::Mutation::default();\n    mutation.set_op(Op::PessimisticLock);\n    mutation.key = b\"key\".to_vec();\n    let mut req = PessimisticLockRequest::default();\n    req.set_context(ctx.clone());\n    req.set_mutations(vec![mutation].into());\n    req.set_start_version(10);\n    req.set_for_update_ts(10);\n    req.set_primary_lock(b\"key\".to_vec());\n\n    let lock_resp = thread::spawn(move || client.kv_pessimistic_lock(&req).unwrap());\n    thread::sleep(Duration::from_millis(300));\n    // Set `status` to `TransferringLeader` to make the locks table not writable,\n    // but the region remains available to serve.\n    txn_ext.pessimistic_locks.write().status = LocksStatus::TransferringLeader;\n    fail::remove(\"acquire_pessimistic_lock\");\n\n    let resp = lock_resp.join().unwrap();\n    // There should be no region error.\n    assert!(!resp.has_region_error());\n    // The lock should not be written to the in-memory pessimistic lock table.\n    assert!(txn_ext.pessimistic_locks.read().is_empty());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transaction.rs::test_read_index_with_max_ts", "test": "fn test_read_index_with_max_ts() {\n    let mut cluster = new_server_cluster(0, 3);\n    // Increase the election tick to make this test case running reliably.\n    // Use async apply prewrite to let tikv response before applying on the leader\n    // peer.\n    configure_for_lease_read(&mut cluster.cfg, Some(50), Some(10_000));\n    cluster.cfg.storage.enable_async_apply_prewrite = true;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let k0 = b\"k0\";\n    let v0 = b\"v0\";\n    let r1 = cluster.run_conf_change();\n    let p2 = new_peer(2, 2);\n    cluster.pd_client.must_add_peer(r1, p2.clone());\n    let p3 = new_peer(3, 3);\n    cluster.pd_client.must_add_peer(r1, p3.clone());\n    cluster.must_put(k0, v0);\n    cluster.pd_client.must_none_pending_peer(p2.clone());\n    cluster.pd_client.must_none_pending_peer(p3.clone());\n\n    let region = cluster.get_region(k0);\n    cluster.must_transfer_leader(region.get_id(), p3.clone());\n\n    // Block all write cmd applying of Peer 3(leader), then start to write to it.\n    let k1 = b\"k1\";\n    let v1 = b\"v1\";\n    let mut ctx_p3 = Context::default();\n    ctx_p3.set_region_id(region.get_id());\n    ctx_p3.set_region_epoch(region.get_region_epoch().clone());\n    ctx_p3.set_peer(p3.clone());\n    let mut ctx_p2 = ctx_p3.clone();\n    ctx_p2.set_peer(p2.clone());\n\n    let start_ts = 10;\n    let mut mutation = pb::Mutation::default();\n    mutation.set_op(Op::Put);\n    mutation.key = k1.to_vec();\n    mutation.value = v1.to_vec();\n    let mut req = PrewriteRequest::default();\n    req.set_context(ctx_p3);\n    req.set_mutations(vec![mutation].into());\n    req.set_start_version(start_ts);\n    req.try_one_pc = true;\n    req.set_primary_lock(k1.to_vec());\n\n    let env = Arc::new(Environment::new(1));\n    let channel =\n        ChannelBuilder::new(env.clone()).connect(&cluster.sim.rl().get_addr(p3.get_store_id()));\n    let client_p3 = TikvClient::new(channel);\n    fail::cfg(\"on_apply_write_cmd\", \"sleep(2000)\").unwrap();\n    client_p3.kv_prewrite(&req).unwrap();\n\n    // The apply is blocked on leader, so the read index request with max ts should\n    // see the memory lock as it would be dropped after finishing apply.\n    let channel = ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(p2.get_store_id()));\n    let client_p2 = TikvClient::new(channel);\n    let mut req = GetRequest::new();\n    req.key = k1.to_vec();\n    req.version = u64::MAX;\n    ctx_p2.replica_read = true;\n    req.set_context(ctx_p2);\n    let resp = client_p2.kv_get(&req).unwrap();\n    assert!(resp.region_error.is_none());\n    assert_eq!(resp.error.unwrap().locked.unwrap().lock_version, start_ts);\n    fail::remove(\"on_apply_write_cmd\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/router/response_channel.rs::is_none", "code": "fn is_none(&self) -> bool {\n        false\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_transfer_leader_slow_apply", "test": "fn test_transfer_leader_slow_apply() {\n    // 3 nodes cluster.\n    let mut cluster = new_node_cluster(0, 3);\n\n    let pd_client = cluster.pd_client.clone();\n    pd_client.disable_default_operator();\n\n    let r1 = cluster.run_conf_change();\n    pd_client.must_add_peer(r1, new_peer(2, 1002));\n    pd_client.must_add_peer(r1, new_peer(3, 1003));\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    let fp = \"on_handle_apply_1003\";\n    fail::cfg(fp, \"pause\").unwrap();\n    for i in 0..=cluster.cfg.raft_store.leader_transfer_max_log_lag {\n        let bytes = format!(\"k{:03}\", i).into_bytes();\n        cluster.must_put(&bytes, &bytes);\n    }\n    cluster.transfer_leader(r1, new_peer(3, 1003));\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n    assert_ne!(cluster.leader_of_region(r1).unwrap(), new_peer(3, 1003));\n    fail::remove(fp);\n    cluster.must_transfer_leader(r1, new_peer(3, 1003));\n    cluster.must_put(b\"k3\", b\"v3\");\n    must_get_equal(&cluster.get_engine(3), b\"k3\", b\"v3\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::leader_of_region", "code": "pub fn leader_of_region(&mut self, region_id: u64) -> Option<metapb::Peer> {\n        let timer = Instant::now_coarse();\n        let timeout = Duration::from_secs(5);\n        let mut store_ids = None;\n        while timer.saturating_elapsed() < timeout {\n            match self.voter_store_ids_of_region(region_id) {\n                None => thread::sleep(Duration::from_millis(10)),\n                Some(ids) => {\n                    store_ids = Some(ids);\n                    break;\n                }\n            }\n        }\n        let store_ids = store_ids?;\n        if let Some(l) = self.leaders.get(&region_id) {\n            // leader may be stopped in some tests.\n            if self.valid_leader_id(region_id, l.get_store_id()) {\n                return Some(l.clone());\n            }\n        }\n        self.reset_leader_of_region(region_id);\n        let mut leader = None;\n        let mut leaders = HashMap::default();\n\n        let node_ids = self.sim.rl().get_node_ids();\n        // For some tests, we stop the node but pd still has this information,\n        // and we must skip this.\n        let alive_store_ids: Vec<_> = store_ids\n            .iter()\n            .filter(|id| node_ids.contains(id))\n            .cloned()\n            .collect();\n        while timer.saturating_elapsed() < timeout {\n            for store_id in &alive_store_ids {\n                let l = match self.query_leader(*store_id, region_id, Duration::from_secs(1)) {\n                    None => continue,\n                    Some(l) => l,\n                };\n                leaders\n                    .entry(l.get_id())\n                    .or_insert((l, vec![]))\n                    .1\n                    .push(*store_id);\n            }\n            if let Some((_, (l, c))) = leaders.iter().max_by_key(|(_, (_, c))| c.len()) {\n                if c.contains(&l.get_store_id()) {\n                    leader = Some(l.clone());\n                    // Technically, correct calculation should use two quorum when in joint\n                    // state. Here just for simplicity.\n                    if c.len() > store_ids.len() / 2 {\n                        break;\n                    }\n                }\n            }\n            debug!(\"failed to detect leaders\"; \"leaders\" => ?leaders, \"store_ids\" => ?store_ids);\n            sleep_ms(10);\n            leaders.clear();\n        }\n\n        if let Some(l) = leader {\n            self.leaders.insert(region_id, l);\n        }\n\n        self.leaders.get(&region_id).cloned()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_delete_lock_proposed_before_proposing_locks", "test": "fn test_delete_lock_proposed_before_proposing_locks() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_heartbeat_ticks = 20;\n    cluster.run();\n\n    let region_id = 1;\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    let leader = cluster.leader_of_region(region_id).unwrap();\n\n    let snapshot = cluster.must_get_snapshot_of_region(region_id);\n    let txn_ext = snapshot.txn_ext.unwrap();\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(\n            Key::from_raw(b\"key\"),\n            PessimisticLock {\n                primary: b\"key\".to_vec().into_boxed_slice(),\n                start_ts: 10.into(),\n                ttl: 1000,\n                for_update_ts: 10.into(),\n                min_commit_ts: 20.into(),\n                last_change_ts: 5.into(),\n                versions_to_last_change: 3,\n            },\n        )])\n        .unwrap();\n\n    let addr = cluster.sim.rl().get_addr(1);\n    let env = Arc::new(Environment::new(1));\n    let channel = ChannelBuilder::new(env).connect(&addr);\n    let client = TikvClient::new(channel);\n\n    let mut req = CleanupRequest::default();\n    let mut ctx = Context::default();\n    ctx.set_region_id(region_id);\n    ctx.set_region_epoch(cluster.get_region_epoch(region_id));\n    ctx.set_peer(leader);\n    req.set_context(ctx);\n    req.set_key(b\"key\".to_vec());\n    req.set_start_version(10);\n    req.set_current_ts(u64::MAX);\n\n    // Pause the command before it actually removes locks.\n    fail::cfg(\"scheduler_async_write_finish\", \"pause\").unwrap();\n    let (tx, resp_rx) = mpsc::channel();\n    thread::spawn(move || tx.send(client.kv_cleanup(&req).unwrap()).unwrap());\n\n    thread::sleep(Duration::from_millis(200));\n    resp_rx.try_recv().unwrap_err();\n\n    cluster.transfer_leader(1, new_peer(2, 2));\n    thread::sleep(Duration::from_millis(200));\n\n    // Transfer leader will not make the command fail.\n    fail::remove(\"scheduler_async_write_finish\");\n    let resp = resp_rx.recv().unwrap();\n    assert!(!resp.has_region_error());\n\n    for _ in 0..10 {\n        thread::sleep(Duration::from_millis(100));\n        cluster.reset_leader_of_region(region_id);\n        if cluster.leader_of_region(region_id).unwrap().id == 2 {\n            let snapshot = cluster.must_get_snapshot_of_region(1);\n            assert!(\n                snapshot\n                    .get_cf(CF_LOCK, &Key::from_raw(b\"key\"))\n                    .unwrap()\n                    .is_none()\n            );\n            return;\n        }\n    }\n    panic!(\"region should succeed to transfer leader to peer 2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/errors.rs::has_region_error", "code": "pub fn has_region_error(&self) -> bool {\n        matches!(\n            self,\n            Error::Kv(KvError(box EngineErrorInner::Request(_)))\n                | Error::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n                    box EngineErrorInner::Request(_),\n                ))))\n                | Error::Txn(TxnError(box TxnErrorInner::Mvcc(MvccError(\n                    box MvccErrorInner::Kv(KvError(box EngineErrorInner::Request(_))),\n                ))))\n                | Error::Request(_)\n        )\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_transfer_leader_msg_index", "test": "fn test_transfer_leader_msg_index() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.raft_entry_cache_life_time = ReadableDuration::secs(1000);\n    prevent_from_gc_raft_log(&mut cluster);\n    run_cluster_for_test_warmup_entry_cache(&mut cluster);\n\n    let (sx, rx) = channel::unbounded();\n    let recv_filter = Box::new(\n        RegionPacketFilter::new(1, 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgTransferLeader)\n            .set_msg_callback(Arc::new(move |m| {\n                sx.send(m.get_message().get_index()).unwrap();\n            })),\n    );\n    cluster.sim.wl().add_recv_filter(2, recv_filter);\n\n    // TransferLeaderMsg.index should be equal to the store3's replicated_index.\n    cluster.transfer_leader(1, new_peer(2, 2));\n    let replicated_index = cluster.raft_local_state(1, 3).last_index;\n    assert_eq!(\n        rx.recv_timeout(Duration::from_secs(2)).unwrap(),\n        replicated_index,\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_when_warmup_range_start_is_larger_than_last_index", "test": "fn test_when_warmup_range_start_is_larger_than_last_index() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.raft_entry_cache_life_time = ReadableDuration::secs(1000);\n    prevent_from_gc_raft_log(&mut cluster);\n    run_cluster_for_test_warmup_entry_cache(&mut cluster);\n    cluster.pd_client.disable_default_operator();\n\n    let s4 = cluster.add_new_engine();\n\n    // Prevent peer 4 from appending logs, so it's last index should\n    // be really small.\n    let recv_filter_s4 = Box::new(\n        RegionPacketFilter::new(1, s4)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend),\n    );\n    cluster.sim.wl().add_recv_filter(s4, recv_filter_s4);\n\n    let (sx, rx) = channel::unbounded();\n    let recv_filter_1 = Box::new(\n        RegionPacketFilter::new(1, 1)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgTransferLeader)\n            .set_msg_callback(Arc::new(move |m| {\n                sx.send(m.get_message().get_from()).unwrap();\n            })),\n    );\n    cluster.sim.wl().add_recv_filter(1, recv_filter_1);\n\n    cluster.pd_client.must_add_peer(1, new_peer(s4, s4));\n    cluster.transfer_leader(1, new_peer(s4, s4));\n    // Store(s4) should ack the transfer leader msg immediately.\n    assert_eq!(rx.recv_timeout(Duration::from_millis(500)).unwrap(), s4);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_when_warmup_fail_and_its_timeout_is_too_long", "test": "fn test_when_warmup_fail_and_its_timeout_is_too_long() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.max_entry_cache_warmup_duration = ReadableDuration::secs(1000);\n    prevent_from_gc_raft_log(&mut cluster);\n    run_cluster_for_test_warmup_entry_cache(&mut cluster);\n\n    fail::cfg(\"worker_async_fetch_raft_log\", \"pause\").unwrap();\n    cluster.transfer_leader(1, new_peer(2, 2));\n    // Theoretically, the leader transfer can't succeed unless it sleeps\n    // max_entry_cache_warmup_duration.\n    sleep_ms(50);\n    let leader = cluster.leader_of_region(1).unwrap();\n    assert_eq!(leader.get_id(), 1);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_transfer_leader.rs::test_when_warmup_succeed_and_not_become_leader", "test": "fn test_when_warmup_succeed_and_not_become_leader() {\n    let mut cluster = run_cluster_and_warm_up_cache_for_store2();\n\n    let (sx, rx) = channel::unbounded();\n    fail::cfg_callback(\"worker_async_fetch_raft_log\", move || {\n        sx.send(true).unwrap()\n    })\n    .unwrap();\n    fail::cfg(\"entry_cache_warmed_up_state_is_stale\", \"return\").unwrap();\n\n    // Since the warmup state is stale, the peer should exit warmup state,\n    // and the entry cache should be compacted during post_apply.\n    let applied_index = cluster.apply_state(1, 2).applied_index;\n    cluster.must_put(b\"kk1\", b\"vv1\");\n    cluster.wait_applied_index(1, 2, applied_index + 1);\n    // The peer should warm up cache again when it receives a new TransferLeaderMsg.\n    cluster.transfer_leader(1, new_peer(2, 2));\n    assert!(rx.recv_timeout(Duration::from_millis(500)).unwrap());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_send_report", "test": "fn test_unsafe_recovery_send_report() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Makes the leadership definite.\n    let store2_peer = find_peer(&region, nodes[1]).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), store2_peer);\n    cluster.put(b\"random_key1\", b\"random_val1\").unwrap();\n\n    // Blocks the raft apply process on store 1 entirely .\n    let (apply_triggered_tx, apply_triggered_rx) = mpsc::bounded::<()>(1);\n    let (apply_released_tx, apply_released_rx) = mpsc::bounded::<()>(1);\n    fail::cfg_callback(\"on_handle_apply_store_1\", move || {\n        let _ = apply_triggered_tx.send(());\n        let _ = apply_released_rx.recv();\n    })\n    .unwrap();\n\n    // Manually makes an update, and wait for the apply to be triggered, to\n    // simulate \"some entries are committed but not applied\" scenario.\n    cluster.put(b\"random_key2\", b\"random_val2\").unwrap();\n    apply_triggered_rx\n        .recv_timeout(Duration::from_secs(1))\n        .unwrap();\n\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n\n    // Triggers the unsafe recovery store reporting process.\n    let plan = pdpb::RecoveryPlan::default();\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // No store report is sent, since there are peers have unapplied entries.\n    for _ in 0..20 {\n        assert_eq!(pd_client.must_get_store_report(nodes[0]), None);\n        sleep_ms(100);\n    }\n\n    // Unblocks the apply process.\n    drop(apply_released_tx);\n\n    // Store reports are sent once the entries are applied.\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    fail::remove(\"on_handle_apply_store_1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_timeout_abort", "test": "fn test_unsafe_recovery_timeout_abort() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 5;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::millis(150);\n    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::millis(100);\n    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::millis(100);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Makes the leadership definite.\n    let store2_peer = find_peer(&region, nodes[1]).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), store2_peer);\n    cluster.put(b\"random_key1\", b\"random_val1\").unwrap();\n\n    // Blocks the raft apply process on store 1 entirely.\n    let (apply_triggered_tx, apply_triggered_rx) = mpsc::bounded::<()>(1);\n    let (apply_released_tx, apply_released_rx) = mpsc::bounded::<()>(1);\n    fail::cfg_callback(\"on_handle_apply_store_1\", move || {\n        let _ = apply_triggered_tx.send(());\n        let _ = apply_released_rx.recv();\n    })\n    .unwrap();\n\n    // Manually makes an update, and wait for the apply to be triggered, to\n    // simulate \"some entries are committed but not applied\" scenario.\n    cluster.put(b\"random_key2\", b\"random_val2\").unwrap();\n    apply_triggered_rx\n        .recv_timeout(Duration::from_secs(1))\n        .unwrap();\n\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n\n    // Triggers the unsafe recovery store reporting process.\n    let plan = pdpb::RecoveryPlan::default();\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // sleep for a while to trigger timeout\n    fail::cfg(\"unsafe_recovery_state_timeout\", \"return\").unwrap();\n    sleep_ms(200);\n    fail::remove(\"unsafe_recovery_state_timeout\");\n\n    // Unblocks the apply process.\n    drop(apply_released_tx);\n\n    // No store report is sent, cause the plan is aborted.\n    for _ in 0..20 {\n        assert_eq!(pd_client.must_get_store_report(nodes[0]), None);\n        sleep_ms(100);\n    }\n\n    // resend the plan\n    let plan = pdpb::RecoveryPlan::default();\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // Store reports are sent once the entries are applied.\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    fail::remove(\"on_handle_apply_store_1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_execution_result_report", "test": "fn test_unsafe_recovery_execution_result_report() {\n    let mut cluster = new_server_cluster(0, 3);\n    // Prolong force leader time.\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Makes the leadership definite.\n    let store2_peer = find_peer(&region, nodes[1]).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), store2_peer);\n    cluster.put(b\"random_key1\", b\"random_val1\").unwrap();\n\n    // Split the region into 2, and remove one of them, so that we can test both\n    // region peer list update and region creation.\n    pd_client.must_split_region(\n        region,\n        pdpb::CheckPolicy::Usekey,\n        vec![b\"random_key1\".to_vec()],\n    );\n    let region1 = pd_client.get_region(b\"random_key\".as_ref()).unwrap();\n    let region2 = pd_client.get_region(b\"random_key1\".as_ref()).unwrap();\n    let region1_store0_peer = find_peer(&region1, nodes[0]).unwrap().to_owned();\n    pd_client.must_remove_peer(region1.get_id(), region1_store0_peer);\n    cluster.must_remove_region(nodes[0], region1.get_id());\n\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    {\n        let put = new_put_cmd(b\"k2\", b\"v2\");\n        let req = new_request(\n            region2.get_id(),\n            region2.get_region_epoch().clone(),\n            vec![put],\n            true,\n        );\n        // marjority is lost, can't propose command successfully.\n        cluster\n            .call_command_on_leader(req, Duration::from_millis(10))\n            .unwrap_err();\n    }\n\n    cluster.must_enter_force_leader(region2.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    // Construct recovery plan.\n    let mut plan = pdpb::RecoveryPlan::default();\n\n    let to_be_removed: Vec<metapb::Peer> = region2\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region2.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n\n    let mut create = metapb::Region::default();\n    create.set_id(101);\n    create.set_end_key(b\"random_key1\".to_vec());\n    let mut peer = metapb::Peer::default();\n    peer.set_id(102);\n    peer.set_store_id(nodes[0]);\n    create.mut_peers().push(peer);\n    plan.mut_creates().push(create);\n\n    // Blocks the raft apply process on store 1 entirely .\n    let (apply_released_tx, apply_released_rx) = mpsc::bounded::<()>(1);\n    fail::cfg_callback(\"on_handle_apply_store_1\", move || {\n        let _ = apply_released_rx.recv();\n    })\n    .unwrap();\n\n    // Triggers the unsafe recovery plan execution.\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // No store report is sent, since there are peers have unapplied entries.\n    for _ in 0..20 {\n        assert_eq!(pd_client.must_get_store_report(nodes[0]), None);\n        sleep_ms(100);\n    }\n\n    // Unblocks the apply process.\n    drop(apply_released_tx);\n\n    // Store reports are sent once the entries are applied.\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    for peer_report in store_report.unwrap().get_peer_reports() {\n        let region = peer_report.get_region_state().get_region();\n        if region.get_id() == 101 {\n            assert_eq!(region.get_end_key(), b\"random_key1\".to_vec());\n        } else {\n            assert_eq!(region.get_id(), region2.get_id());\n            for peer in region.get_peers() {\n                if peer.get_store_id() != nodes[0] {\n                    assert_eq!(peer.get_role(), metapb::PeerRole::Learner);\n                }\n            }\n        }\n    }\n    fail::remove(\"on_handle_apply_store_1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_wait_for_snapshot_apply", "test": "fn test_unsafe_recovery_wait_for_snapshot_apply() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(8);\n    cluster.cfg.raft_store.merge_max_log_gap = 3;\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(10);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Makes the leadership definite.\n    let store2_peer = find_peer(&region, nodes[1]).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), store2_peer);\n    cluster.stop_node(nodes[1]);\n    let (raft_gc_triggered_tx, raft_gc_triggered_rx) = mpsc::bounded::<()>(1);\n    let (raft_gc_finished_tx, raft_gc_finished_rx) = mpsc::bounded::<()>(1);\n    fail::cfg_callback(\"worker_gc_raft_log\", move || {\n        let _ = raft_gc_triggered_rx.recv();\n    })\n    .unwrap();\n    fail::cfg_callback(\"worker_gc_raft_log_finished\", move || {\n        let _ = raft_gc_finished_tx.send(());\n    })\n    .unwrap();\n    (0..10).for_each(|_| cluster.must_put(b\"random_k\", b\"random_v\"));\n    // Unblock raft log GC.\n    drop(raft_gc_triggered_tx);\n    // Wait until logs are GCed.\n    raft_gc_finished_rx\n        .recv_timeout(Duration::from_secs(3))\n        .unwrap();\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[2]);\n\n    // Blocks the raft snap apply process.\n    let (apply_triggered_tx, apply_triggered_rx) = mpsc::bounded::<()>(1);\n    let (apply_released_tx, apply_released_rx) = mpsc::bounded::<()>(1);\n    fail::cfg_callback(\"region_apply_snap\", move || {\n        let _ = apply_triggered_tx.send(());\n        let _ = apply_released_rx.recv();\n    })\n    .unwrap();\n\n    cluster.run_node(nodes[1]).unwrap();\n\n    apply_triggered_rx\n        .recv_timeout(Duration::from_secs(1))\n        .unwrap();\n\n    // Triggers the unsafe recovery store reporting process.\n    let plan = pdpb::RecoveryPlan::default();\n    pd_client.must_set_unsafe_recovery_plan(nodes[1], plan);\n    cluster.must_send_store_heartbeat(nodes[1]);\n\n    // No store report is sent, since there are peers have unapplied entries.\n    for _ in 0..20 {\n        assert_eq!(pd_client.must_get_store_report(nodes[1]), None);\n        sleep_ms(100);\n    }\n\n    // Unblocks the snap apply process.\n    drop(apply_released_tx);\n\n    // Store reports are sent once the entries are applied.\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[1]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n\n    fail::remove(\"worker_gc_raft_log\");\n    fail::remove(\"worker_gc_raft_log_finished\");\n    fail::remove(\"region_apply_snap\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_demotion_reentrancy", "test": "fn test_unsafe_recovery_demotion_reentrancy() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Makes the leadership definite.\n    let store2_peer = find_peer(&region, nodes[2]).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), store2_peer);\n\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    {\n        let put = new_put_cmd(b\"k2\", b\"v2\");\n        let req = new_request(\n            region.get_id(),\n            region.get_region_epoch().clone(),\n            vec![put],\n            true,\n        );\n        // marjority is lost, can't propose command successfully.\n        cluster\n            .call_command_on_leader(req, Duration::from_millis(10))\n            .unwrap_err();\n    }\n\n    cluster.must_enter_force_leader(region.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    // Construct recovery plan.\n    let mut plan = pdpb::RecoveryPlan::default();\n\n    let to_be_removed: Vec<metapb::Peer> = region\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n\n    // Blocks the raft apply process on store 1 entirely .\n    let (apply_released_tx, apply_released_rx) = mpsc::bounded::<()>(1);\n    fail::cfg_callback(\"on_handle_apply_store_1\", move || {\n        let _ = apply_released_rx.recv();\n    })\n    .unwrap();\n\n    // Triggers the unsafe recovery plan execution.\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan.clone());\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // No store report is sent, since there are peers have unapplied entries.\n    for _ in 0..10 {\n        assert_eq!(pd_client.must_get_store_report(nodes[0]), None);\n        sleep_ms(100);\n    }\n\n    // Send the plan again.\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // Unblocks the apply process.\n    drop(apply_released_tx);\n\n    let mut demoted = false;\n    for _ in 0..10 {\n        let region_in_pd = block_on(pd_client.get_region_by_id(region.get_id()))\n            .unwrap()\n            .unwrap();\n        assert_eq!(region_in_pd.get_peers().len(), 3);\n        demoted = region_in_pd\n            .get_peers()\n            .iter()\n            .filter(|peer| peer.get_store_id() != nodes[0])\n            .all(|peer| peer.get_role() == metapb::PeerRole::Learner);\n        sleep_ms(100);\n    }\n    assert_eq!(demoted, true);\n    fail::remove(\"on_handle_apply_store_1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_create_destroy_reentrancy", "test": "fn test_unsafe_recovery_create_destroy_reentrancy() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Makes the leadership definite.\n    let store2_peer = find_peer(&region, nodes[1]).unwrap().to_owned();\n    cluster.must_transfer_leader(region.get_id(), store2_peer);\n    cluster.put(b\"random_key1\", b\"random_val1\").unwrap();\n\n    // Split the region into 2, and remove one of them, so that we can test both\n    // region peer list update and region creation.\n    pd_client.must_split_region(\n        region,\n        pdpb::CheckPolicy::Usekey,\n        vec![b\"random_key1\".to_vec()],\n    );\n    let region1 = pd_client.get_region(b\"random_key\".as_ref()).unwrap();\n    let region2 = pd_client.get_region(b\"random_key1\".as_ref()).unwrap();\n    let region1_store0_peer = find_peer(&region1, nodes[0]).unwrap().to_owned();\n    pd_client.must_remove_peer(region1.get_id(), region1_store0_peer);\n    cluster.must_remove_region(nodes[0], region1.get_id());\n\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    {\n        let put = new_put_cmd(b\"k2\", b\"v2\");\n        let req = new_request(\n            region2.get_id(),\n            region2.get_region_epoch().clone(),\n            vec![put],\n            true,\n        );\n        // marjority is lost, can't propose command successfully.\n        cluster\n            .call_command_on_leader(req, Duration::from_millis(10))\n            .unwrap_err();\n    }\n\n    cluster.must_enter_force_leader(region2.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    // Construct recovery plan.\n    let mut plan = pdpb::RecoveryPlan::default();\n\n    let mut create = metapb::Region::default();\n    create.set_id(101);\n    create.set_end_key(b\"random_key1\".to_vec());\n    let mut peer = metapb::Peer::default();\n    peer.set_id(102);\n    peer.set_store_id(nodes[0]);\n    create.mut_peers().push(peer);\n    plan.mut_creates().push(create);\n\n    plan.mut_tombstones().push(region2.get_id());\n\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan.clone());\n    cluster.must_send_store_heartbeat(nodes[0]);\n    sleep_ms(100);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan.clone());\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // Store reports are sent once the entries are applied.\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    let report = store_report.unwrap();\n    let peer_reports = report.get_peer_reports();\n    assert_eq!(peer_reports.len(), 1);\n    let reported_region = peer_reports[0].get_region_state().get_region();\n    assert_eq!(reported_region.get_id(), 101);\n    assert_eq!(reported_region.get_peers().len(), 1);\n    assert_eq!(reported_region.get_peers()[0].get_id(), 102);\n    fail::remove(\"on_handle_apply_store_1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_unsafe_recovery.rs::test_unsafe_recovery_rollback_merge", "test": "fn test_unsafe_recovery_rollback_merge() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.cfg.raft_store.merge_check_tick_interval = ReadableDuration::millis(20);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    for i in 0..10 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), b\"v\");\n    }\n\n    // Block merge commit, let go of the merge prepare.\n    fail::cfg(\"on_schedule_merge\", \"return()\").unwrap();\n\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k2\").unwrap();\n\n    // Makes the leadership definite.\n    let left_peer_2 = find_peer(&left, nodes[2]).unwrap().to_owned();\n    let right_peer_2 = find_peer(&right, nodes[2]).unwrap().to_owned();\n    cluster.must_transfer_leader(left.get_id(), left_peer_2);\n    cluster.must_transfer_leader(right.get_id(), right_peer_2);\n    cluster.must_try_merge(left.get_id(), right.get_id());\n\n    // Makes the group lose its quorum.\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    {\n        let put = new_put_cmd(b\"k2\", b\"v2\");\n        let req = new_request(\n            region.get_id(),\n            region.get_region_epoch().clone(),\n            vec![put],\n            true,\n        );\n        // marjority is lost, can't propose command successfully.\n        cluster\n            .call_command_on_leader(req, Duration::from_millis(10))\n            .unwrap_err();\n    }\n\n    cluster.must_enter_force_leader(left.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n    cluster.must_enter_force_leader(right.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    // Construct recovery plan.\n    let mut plan = pdpb::RecoveryPlan::default();\n\n    let left_demote_peers: Vec<metapb::Peer> = left\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut left_demote = pdpb::DemoteFailedVoters::default();\n    left_demote.set_region_id(left.get_id());\n    left_demote.set_failed_voters(left_demote_peers.into());\n    let right_demote_peers: Vec<metapb::Peer> = right\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut right_demote = pdpb::DemoteFailedVoters::default();\n    right_demote.set_region_id(right.get_id());\n    right_demote.set_failed_voters(right_demote_peers.into());\n    plan.mut_demotes().push(left_demote);\n    plan.mut_demotes().push(right_demote);\n\n    // Triggers the unsafe recovery plan execution.\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan.clone());\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    // Can't propose demotion as it's in merging mode\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    let has_force_leader = store_report\n        .unwrap()\n        .get_peer_reports()\n        .iter()\n        .any(|p| p.get_is_force_leader());\n    // Force leader is not exited due to demotion failure\n    assert!(has_force_leader);\n\n    fail::remove(\"on_schedule_merge\");\n    fail::cfg(\"on_schedule_merge_ret_err\", \"return()\").unwrap();\n\n    // Make sure merge check is scheduled, and rollback merge is triggered\n    sleep_ms(50);\n\n    // Re-triggers the unsafe recovery plan execution.\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    // No force leader\n    for peer_report in store_report.unwrap().get_peer_reports() {\n        assert!(!peer_report.get_is_force_leader());\n    }\n\n    // Demotion is done\n    let mut demoted = false;\n    for _ in 0..10 {\n        let new_left = block_on(pd_client.get_region_by_id(left.get_id()))\n            .unwrap()\n            .unwrap();\n        let new_right = block_on(pd_client.get_region_by_id(right.get_id()))\n            .unwrap()\n            .unwrap();\n        assert_eq!(new_left.get_peers().len(), 3);\n        assert_eq!(new_right.get_peers().len(), 3);\n        demoted = new_left\n            .get_peers()\n            .iter()\n            .filter(|peer| peer.get_store_id() != nodes[0])\n            .all(|peer| peer.get_role() == metapb::PeerRole::Learner)\n            && new_right\n                .get_peers()\n                .iter()\n                .filter(|peer| peer.get_store_id() != nodes[0])\n                .all(|peer| peer.get_role() == metapb::PeerRole::Learner);\n        if demoted {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_eq!(demoted, true);\n\n    fail::remove(\"on_schedule_merge_ret_err\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_witness_update_region_in_local_reader", "test": "fn test_witness_update_region_in_local_reader() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n    assert_eq!(nodes[2], 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    // update region but the peer is not destroyed yet\n    fail::cfg(\"change_peer_after_update_region_store_3\", \"pause\").unwrap();\n\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store3.clone());\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cmd(b\"k0\")],\n        false,\n    );\n    request.mut_header().set_peer(peer_on_store3);\n    request.mut_header().set_replica_read(true);\n\n    let resp = cluster\n        .read(None, request.clone(), Duration::from_millis(100))\n        .unwrap();\n    assert_eq!(\n        resp.get_header().get_error().get_is_witness(),\n        &kvproto::errorpb::IsWitness {\n            region_id: region.get_id(),\n            ..Default::default()\n        }\n    );\n\n    fail::remove(\"change_peer_after_update_region_store_3\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_witness_not_reported_while_disabled", "test": "fn test_witness_not_reported_while_disabled() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n    assert_eq!(nodes[2], 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    // update region but the peer is not destroyed yet\n    fail::cfg(\"change_peer_after_update_region_store_3\", \"pause\").unwrap();\n\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store3.clone());\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cmd(b\"k0\")],\n        false,\n    );\n    request.mut_header().set_peer(peer_on_store3);\n    request.mut_header().set_replica_read(true);\n\n    let resp = cluster\n        .read(None, request.clone(), Duration::from_millis(100))\n        .unwrap();\n    assert!(resp.get_header().has_error());\n    assert!(!resp.get_header().get_error().has_is_witness());\n    fail::remove(\"change_peer_after_update_region_store_3\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_witness_raftlog_gc_pull_voter_replicated_index", "test": "fn test_witness_raftlog_gc_pull_voter_replicated_index() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(100);\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(50);\n    cluster\n        .cfg\n        .raft_store\n        .request_voter_replicated_index_interval = ReadableDuration::millis(100);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(200));\n    let mut before_states = HashMap::default();\n    for (&id, engines) in &cluster.engines {\n        let mut state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        before_states.insert(id, state.take_truncated_state());\n    }\n\n    // one follower is down\n    cluster.stop_node(nodes[1]);\n\n    // write some data to make log gap exceeds the gc limit\n    for i in 1..1000 {\n        let (k, v) = (format!(\"k{}\", i), format!(\"v{}\", i));\n        let key = k.as_bytes();\n        let value = v.as_bytes();\n        cluster.must_put(key, value);\n    }\n\n    // the witness truncated index is not advanced\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        if id == 2 {\n            assert_eq!(\n                state.get_truncated_state().get_index() - before_states[&id].get_index(),\n                0\n            );\n        } else {\n            assert_ne!(\n                900,\n                state.get_truncated_state().get_index() - before_states[&id].get_index()\n            );\n        }\n    }\n\n    fail::cfg(\"on_raft_gc_log_tick\", \"return\").unwrap();\n\n    // the follower is back online\n    cluster.run_node(nodes[1]).unwrap();\n    cluster.must_put(b\"k00\", b\"v00\");\n    must_get_equal(&cluster.get_engine(nodes[1]), b\"k00\", b\"v00\");\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(300));\n\n    // the truncated index is advanced now, as all the peers has replicated\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        assert_ne!(\n            900,\n            state.get_truncated_state().get_index() - before_states[&id].get_index()\n        );\n    }\n    fail::remove(\"on_raft_gc_log_tick\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_witness_raftlog_gc_after_reboot", "test": "fn test_witness_raftlog_gc_after_reboot() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(100);\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(50);\n    cluster\n        .cfg\n        .raft_store\n        .request_voter_replicated_index_interval = ReadableDuration::millis(100);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(200));\n    let mut before_states = HashMap::default();\n    for (&id, engines) in &cluster.engines {\n        let mut state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        before_states.insert(id, state.take_truncated_state());\n    }\n\n    // one follower is down\n    cluster.stop_node(nodes[1]);\n\n    // write some data to make log gap exceeds the gc limit\n    for i in 1..1000 {\n        let (k, v) = (format!(\"k{}\", i), format!(\"v{}\", i));\n        let key = k.as_bytes();\n        let value = v.as_bytes();\n        cluster.must_put(key, value);\n    }\n\n    // the witness truncated index is not advanced\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        if id == 2 {\n            assert_eq!(\n                state.get_truncated_state().get_index() - before_states[&id].get_index(),\n                0\n            );\n        } else {\n            assert_ne!(\n                900,\n                state.get_truncated_state().get_index() - before_states[&id].get_index()\n            );\n        }\n    }\n\n    fail::cfg(\"on_raft_gc_log_tick\", \"return\").unwrap();\n\n    // the follower is back online\n    cluster.run_node(nodes[1]).unwrap();\n    cluster.must_put(b\"k00\", b\"v00\");\n    must_get_equal(&cluster.get_engine(nodes[1]), b\"k00\", b\"v00\");\n\n    // the witness is down\n    cluster.stop_node(nodes[2]);\n    std::thread::sleep(Duration::from_millis(100));\n    // the witness is back online\n    cluster.run_node(nodes[2]).unwrap();\n\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(300));\n\n    // the truncated index is advanced now, as all the peers has replicated\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        assert_ne!(\n            900,\n            state.get_truncated_state().get_index() - before_states[&id].get_index()\n        );\n    }\n    fail::remove(\"on_raft_gc_log_tick\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_request_snapshot_after_reboot", "test": "fn test_request_snapshot_after_reboot() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.pd_heartbeat_tick_interval = ReadableDuration::millis(20);\n    cluster.cfg.raft_store.check_request_snapshot_interval = ReadableDuration::millis(20);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    std::thread::sleep(Duration::from_millis(100));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n\n    // witness -> nonwitness\n    let fp = \"ignore request snapshot\";\n    fail::cfg(fp, \"return\").unwrap();\n    cluster\n        .pd_client\n        .switch_witnesses(region.get_id(), vec![peer_on_store3.get_id()], vec![false]);\n    std::thread::sleep(Duration::from_millis(500));\n    // as we ignore request snapshot, so snapshot should still not applied yet\n    assert_eq!(cluster.pd_client.get_pending_peers().len(), 1);\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n\n    cluster.stop_node(nodes[2]);\n    fail::remove(fp);\n    std::thread::sleep(Duration::from_millis(100));\n    // the PeerState is Unavailable, so it will request snapshot immediately after\n    // start.\n    cluster.run_node(nodes[2]).unwrap();\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    std::thread::sleep(Duration::from_millis(500));\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    assert_eq!(cluster.pd_client.get_pending_peers().len(), 0);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_request_snapshot_after_term_change", "test": "fn test_request_snapshot_after_term_change() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.pd_heartbeat_tick_interval = ReadableDuration::millis(20);\n    cluster.cfg.raft_store.check_request_snapshot_interval = ReadableDuration::millis(20);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    std::thread::sleep(Duration::from_millis(100));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n\n    // witness -> nonwitness\n    let fp1 = \"ignore generate snapshot\";\n    fail::cfg(fp1, \"return\").unwrap();\n    cluster\n        .pd_client\n        .switch_witnesses(region.get_id(), vec![peer_on_store3.get_id()], vec![false]);\n    std::thread::sleep(Duration::from_millis(500));\n    // as we ignore generate snapshot, so snapshot should still not applied yet\n    assert_eq!(cluster.pd_client.get_pending_peers().len(), 1);\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n\n    let peer_on_store2 = find_peer(&region, nodes[1]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store2.clone());\n    // After leader changes, the `term` and `last term` no longer match, so\n    // continue to receive `MsgAppend` until the two get equal, then retry to\n    // request snapshot and complete the application.\n    std::thread::sleep(Duration::from_millis(500));\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    assert_eq!(cluster.pd_client.get_pending_peers().len(), 0);\n    fail::remove(fp1);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_non_witness_replica_read", "test": "fn test_non_witness_replica_read() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.check_request_snapshot_interval = ReadableDuration::millis(20);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    // witness -> nonwitness\n    fail::cfg(\"ignore request snapshot\", \"return\").unwrap();\n    cluster\n        .pd_client\n        .switch_witnesses(region.get_id(), vec![peer_on_store3.get_id()], vec![false]);\n    std::thread::sleep(Duration::from_millis(100));\n    // as we ignore request snapshot, so snapshot should still not applied yet\n\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cmd(b\"k0\")],\n        false,\n    );\n    request.mut_header().set_peer(peer_on_store3.clone());\n    request.mut_header().set_replica_read(true);\n\n    let resp = cluster\n        .read(None, request, Duration::from_millis(100))\n        .unwrap();\n    assert_eq!(\n        resp.get_header().get_error().get_is_witness(),\n        &kvproto::errorpb::IsWitness {\n            region_id: region.get_id(),\n            ..Default::default()\n        }\n    );\n\n    // start requesting snapshot and give enough time for applying snapshot to\n    // complete\n    fail::remove(\"ignore request snapshot\");\n    std::thread::sleep(Duration::from_millis(500));\n\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cmd(b\"k0\")],\n        false,\n    );\n    request.mut_header().set_peer(peer_on_store3);\n    request.mut_header().set_replica_read(true);\n\n    let resp = cluster\n        .read(None, request, Duration::from_millis(100))\n        .unwrap();\n    assert_eq!(resp.get_header().has_error(), false);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_witness_leader_transfer_out", "test": "fn test_witness_leader_transfer_out() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n\n    // prevent this peer from applying the switch witness command until it's elected\n    // as the Raft leader\n    fail::cfg(\"before_exec_batch_switch_witness\", \"pause\").unwrap();\n    let peer_on_store2 = find_peer(&region, nodes[1]).unwrap().clone();\n    // nonwitness -> witness\n    cluster\n        .pd_client\n        .switch_witnesses(region.get_id(), vec![peer_on_store2.get_id()], vec![true]);\n    // make sure the left peers have applied switch witness cmd\n    std::thread::sleep(Duration::from_millis(500));\n\n    // the other follower is isolated\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    for i in 1..10 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), format!(\"v{}\", i).as_bytes());\n    }\n    // the leader is down\n    cluster.stop_node(1);\n\n    // new leader would help to replicate the logs\n    cluster.clear_send_filters();\n    std::thread::sleep(Duration::from_millis(1000));\n    // make sure the new leader has became to the witness\n    fail::remove(\"before_exec_batch_switch_witness\");\n    std::thread::sleep(Duration::from_millis(500));\n\n    // forbid writes\n    let put = new_put_cmd(b\"k3\", b\"v3\");\n    must_get_error_is_witness(&mut cluster, &region, put);\n    // forbid reads\n    let get = new_get_cmd(b\"k1\");\n    must_get_error_is_witness(&mut cluster, &region, get);\n    // forbid read index\n    let read_index = new_read_index_cmd();\n    must_get_error_is_witness(&mut cluster, &region, read_index);\n\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n\n    cluster.must_transfer_leader(region.get_id(), peer_on_store3);\n    cluster.must_put(b\"k1\", b\"v1\");\n    assert_eq!(\n        cluster.leader_of_region(region.get_id()).unwrap().store_id,\n        nodes[2],\n    );\n    assert_eq!(cluster.must_get(b\"k9\"), Some(b\"v9\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/service.rs::get_id", "code": "pub fn get_id(&self) -> ConnId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/failpoints/cases/test_witness.rs::test_witness_leader_ignore_gen_snapshot", "test": "fn test_witness_leader_ignore_gen_snapshot() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(100);\n    configure_for_snapshot(&mut cluster.cfg);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // the other follower is isolated\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(200));\n    let mut before_states = HashMap::default();\n    for (&id, engines) in &cluster.engines {\n        let mut state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        before_states.insert(id, state.take_truncated_state());\n    }\n\n    // write some data to make log gap exceeds the gc limit\n    for i in 1..1000 {\n        let (k, v) = (format!(\"k{}\", i), format!(\"v{}\", i));\n        let key = k.as_bytes();\n        let value = v.as_bytes();\n        cluster.must_put(key, value);\n    }\n\n    std::thread::sleep(Duration::from_millis(200));\n\n    // the truncated index is advanced\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        let diff = state.get_truncated_state().get_index() - before_states[&id].get_index();\n        error!(\"EEEEE\";\n            \"id\" => &id,\n            \"diff\" => diff,\n            \"state.get_truncated_state().get_index()\" => state.get_truncated_state().get_index(),\n            \"before_states[&id].get_index()\" => before_states[&id].get_index()\n        );\n        assert_ne!(\n            900,\n            state.get_truncated_state().get_index() - before_states[&id].get_index()\n        );\n    }\n\n    // ingore raft log gc to avoid canceling snapshots\n    fail::cfg(\"on_raft_gc_log_tick\", \"return\").unwrap();\n    // wait for leader applied switch to witness\n    fail::cfg(\"before_region_gen_snap\", \"pause\").unwrap();\n    fail::cfg(\"ignore_snap_try_cnt\", \"return\").unwrap();\n    // After the snapshot is generated, it will be checked as invalidated and will\n    // not be regenerated (handle_snapshot will not generate a snapshot for\n    // witness)\n    cluster.clear_send_filters();\n    std::thread::sleep(Duration::from_millis(500));\n\n    // non-witness -> witness\n    fail::cfg(\"ignore_forbid_leader_to_be_witness\", \"return\").unwrap();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store1.get_id()],\n        vec![true],\n    );\n    fail::remove(\"before_region_gen_snap\");\n\n    std::thread::sleep(Duration::from_millis(500));\n\n    // forbid writes\n    let put = new_put_cmd(b\"k3\", b\"v3\");\n    must_get_error_is_witness(&mut cluster, &region, put);\n    // forbid reads\n    let get = new_get_cmd(b\"k1\");\n    must_get_error_is_witness(&mut cluster, &region, get);\n    // forbid read index\n    let read_index = new_read_index_cmd();\n    must_get_error_is_witness(&mut cluster, &region, read_index);\n\n    // reject to transfer, as can't send snapshot to peer_on_store3, there's a log\n    // gap\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    let _ = cluster.try_transfer_leader(region.get_id(), peer_on_store3);\n    std::thread::sleep(Duration::from_secs(5));\n    assert_eq!(cluster.leader_of_region(1).unwrap(), peer_on_store1);\n\n    // should be enable to transfer leader to peer_on_store2\n    let peer_on_store2 = find_peer(&region, nodes[1]).unwrap().clone();\n    cluster.must_transfer_leader(1, peer_on_store2);\n    cluster.must_put(b\"k1\", b\"v1\");\n    assert_eq!(\n        cluster.leader_of_region(region.get_id()).unwrap().store_id,\n        nodes[1],\n    );\n    assert_eq!(cluster.must_get(b\"k9\"), Some(b\"v9\".to_vec()));\n\n    fail::remove(\"on_raft_gc_log_tick\");\n    fail::remove(\"ignore_snap_try_cnt\");\n    fail::remove(\"ignore_forbid_leader_to_be_witness\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/backup/mod.rs::test_backup_and_import", "test": "fn test_backup_and_import() {\n    let mut suite = TestSuite::new(3, 144 * 1024 * 1024, ApiVersion::V1);\n    // 3 version for each key.\n    let key_count = 60;\n    suite.must_kv_put(key_count, 3);\n\n    // Push down backup request.\n    let tmp = Builder::new().tempdir().unwrap();\n    let backup_ts = suite.alloc_ts();\n    let storage_path = make_unique_dir(tmp.path());\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        &storage_path,\n    );\n    let resps1 = block_on(rx.collect::<Vec<_>>());\n    // Only leader can handle backup.\n    assert_eq!(resps1.len(), 1);\n    let files1 = resps1[0].files.clone();\n    // Short value is piggybacked in write cf, so we get 1 sst at least.\n    assert!(!resps1[0].get_files().is_empty());\n\n    // Delete all data, there should be no backup files.\n    suite.cluster.must_delete_range_cf(CF_DEFAULT, b\"\", b\"\");\n    suite.cluster.must_delete_range_cf(CF_WRITE, b\"\", b\"\");\n    // Backup file should have same contents.\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        &make_unique_dir(tmp.path()),\n    );\n    let resps2 = block_on(rx.collect::<Vec<_>>());\n    assert!(resps2[0].get_files().is_empty(), \"{:?}\", resps2);\n\n    // Use importer to restore backup files.\n    let backend = make_local_backend(&storage_path);\n    let storage = create_storage(&backend, Default::default()).unwrap();\n    let region = suite.cluster.get_region(b\"\");\n    let mut sst_meta = SstMeta::default();\n    sst_meta.region_id = region.get_id();\n    sst_meta.set_region_epoch(region.get_region_epoch().clone());\n    sst_meta.set_uuid(uuid::Uuid::new_v4().as_bytes().to_vec());\n    let mut metas = vec![];\n    for f in files1.clone().into_iter() {\n        let mut reader = storage.read(&f.name);\n        let mut content = vec![];\n        block_on(reader.read_to_end(&mut content)).unwrap();\n        let mut m = sst_meta.clone();\n        m.crc32 = calc_crc32_bytes(&content);\n        m.length = content.len() as _;\n        m.cf_name = name_to_cf(&f.name).to_owned();\n        metas.push((m, content));\n    }\n\n    for (m, c) in &metas {\n        for importer in suite.cluster.sim.rl().importers.values() {\n            let mut f = importer.create(m).unwrap();\n            f.append(c).unwrap();\n            f.finish().unwrap();\n        }\n\n        // Make ingest command.\n        let mut ingest = Request::default();\n        ingest.set_cmd_type(CmdType::IngestSst);\n        ingest.mut_ingest_sst().set_sst(m.clone());\n        let mut header = RaftRequestHeader::default();\n        let leader = suite.context.get_peer().clone();\n        header.set_peer(leader);\n        header.set_region_id(suite.context.get_region_id());\n        header.set_region_epoch(suite.context.get_region_epoch().clone());\n        let mut cmd = RaftCmdRequest::default();\n        cmd.set_header(header);\n        cmd.mut_requests().push(ingest);\n        let resp = suite\n            .cluster\n            .call_command_on_leader(cmd, Duration::from_secs(5))\n            .unwrap();\n        assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    }\n\n    // Backup file should have same contents.\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        &make_unique_dir(tmp.path()),\n    );\n    let resps3 = block_on(rx.collect::<Vec<_>>());\n    assert_same_files(files1.into_vec(), resps3[0].files.clone().into_vec());\n\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/coprocessor/mod.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cmds.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/backup/mod.rs::test_backup_huge_range_and_import", "test": "fn test_backup_huge_range_and_import() {\n    let mut suite = TestSuite::new(3, 100, ApiVersion::V1);\n    // 3 version for each key.\n    // make sure we will have two batch files\n    let key_count = 1024 * 3 / 2;\n    suite.must_kv_put(key_count, 3);\n\n    // Push down backup request.\n    let tmp = Builder::new().tempdir().unwrap();\n    let backup_ts = suite.alloc_ts();\n    let storage_path = make_unique_dir(tmp.path());\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        &storage_path,\n    );\n    let mut resps1 = block_on(rx.collect::<Vec<_>>());\n    resps1.sort_by(|r1, r2| r1.start_key.cmp(&r2.start_key));\n\n    // Only leader can handle backup.\n    // ... But the response may be split into two parts (when meeting huge region).\n    assert_eq!(resps1.len(), 2, \"{:?}\", resps1);\n    let mut files1 = resps1\n        .iter()\n        .flat_map(|x| x.files.iter())\n        .cloned()\n        .collect::<Vec<_>>();\n    // Short value is piggybacked in write cf, so we get 1 sst at least.\n    assert!(!resps1[0].get_files().is_empty());\n\n    // Sort the files for avoiding race conditions. (would this happen?)\n    files1.sort_by(|f1, f2| f1.start_key.cmp(&f2.start_key));\n\n    assert_eq!(resps1[0].start_key, b\"\".to_vec());\n    assert_eq!(resps1[0].end_key, resps1[1].start_key);\n    assert_eq!(resps1[1].end_key, b\"\".to_vec());\n\n    assert_eq!(files1.len(), 2);\n    assert_ne!(files1[0].start_key, files1[0].end_key);\n    assert_ne!(files1[1].start_key, files1[1].end_key);\n    assert_eq!(files1[0].end_key, files1[1].start_key);\n\n    // Use importer to restore backup files.\n    let backend = make_local_backend(&storage_path);\n    let storage = create_storage(&backend, Default::default()).unwrap();\n    let region = suite.cluster.get_region(b\"\");\n    let mut sst_meta = SstMeta::default();\n    sst_meta.region_id = region.get_id();\n    sst_meta.set_region_epoch(region.get_region_epoch().clone());\n    let mut metas = vec![];\n    for f in files1.clone().into_iter() {\n        let mut reader = storage.read(&f.name);\n        let mut content = vec![];\n        block_on(reader.read_to_end(&mut content)).unwrap();\n        let mut m = sst_meta.clone();\n        m.crc32 = calc_crc32_bytes(&content);\n        m.length = content.len() as _;\n        // set different uuid for each file\n        m.set_uuid(uuid::Uuid::new_v4().as_bytes().to_vec());\n        m.cf_name = name_to_cf(&f.name).to_owned();\n        metas.push((m, content));\n    }\n\n    for (m, c) in &metas {\n        for importer in suite.cluster.sim.rl().importers.values() {\n            let mut f = importer.create(m).unwrap();\n            f.append(c).unwrap();\n            f.finish().unwrap();\n        }\n\n        // Make ingest command.\n        let mut ingest = Request::default();\n        ingest.set_cmd_type(CmdType::IngestSst);\n        ingest.mut_ingest_sst().set_sst(m.clone());\n        let mut header = RaftRequestHeader::default();\n        let leader = suite.context.get_peer().clone();\n        header.set_peer(leader);\n        header.set_region_id(suite.context.get_region_id());\n        header.set_region_epoch(suite.context.get_region_epoch().clone());\n        let mut cmd = RaftCmdRequest::default();\n        cmd.set_header(header);\n        cmd.mut_requests().push(ingest);\n        let resp = suite\n            .cluster\n            .call_command_on_leader(cmd, Duration::from_secs(5))\n            .unwrap();\n        assert!(!resp.get_header().has_error(), \"{:?}\", resp);\n    }\n\n    // Backup file should have same contents.\n    let rx = suite.backup(\n        vec![],   // start\n        vec![],   // end\n        0.into(), // begin_ts\n        backup_ts,\n        &make_unique_dir(tmp.path()),\n    );\n    let resps3 = block_on(rx.collect::<Vec<_>>());\n    assert_same_files(\n        files1,\n        resps3\n            .iter()\n            .flat_map(|x| x.files.iter())\n            .cloned()\n            .collect(),\n    );\n\n    suite.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/coprocessor/mod.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cmds.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/mod.rs::test_do_not_use_unified_readpool_with_legacy_config", "test": "fn test_do_not_use_unified_readpool_with_legacy_config() {\n    let content = r#\"\n        [readpool.storage]\n        normal-concurrency = 1\n\n        [readpool.coprocessor]\n        normal-concurrency = 1\n    \"#;\n    let cfg: TikvConfig = toml::from_str(content).unwrap();\n    assert!(!cfg.readpool.is_unified_pool_enabled());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/config/mod.rs::is_unified_pool_enabled", "code": "pub fn is_unified_pool_enabled(&self) -> bool {\n        self.storage.use_unified_pool() || self.coprocessor.use_unified_pool()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/mod.rs::test_block_cache_backward_compatible", "test": "fn test_block_cache_backward_compatible() {\n    let content = read_file_in_project_dir(\"integrations/config/test-cache-compatible.toml\");\n    let mut cfg: TikvConfig = toml::from_str(&content).unwrap();\n    assert!(cfg.storage.block_cache.capacity.is_none());\n    cfg.compatible_adjust();\n    assert!(cfg.storage.block_cache.capacity.is_some());\n    assert_eq!(\n        cfg.storage.block_cache.capacity.unwrap().0,\n        cfg.rocksdb.defaultcf.block_cache_size.0\n            + cfg.rocksdb.writecf.block_cache_size.0\n            + cfg.rocksdb.lockcf.block_cache_size.0\n            + cfg.raftdb.defaultcf.block_cache_size.0\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_none", "code": "pub fn is_none(&self) -> bool {\n        !self.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/test_config_client.rs::test_update_config", "test": "fn test_update_config() {\n    let (mut cfg, _dir) = TikvConfig::with_tmp().unwrap();\n    cfg.validate().unwrap();\n    let cfg_controller = ConfigController::new(cfg);\n    let mut cfg = cfg_controller.get_current();\n\n    // normal update\n    cfg_controller\n        .update(change(\"raftstore.raft-log-gc-threshold\", \"2000\"))\n        .unwrap();\n    cfg.raft_store.raft_log_gc_threshold = 2000;\n    assert_eq!(cfg_controller.get_current(), cfg);\n\n    // update not support config\n    let res = cfg_controller.update(change(\"server.addr\", \"localhost:3000\"));\n    res.unwrap_err();\n    assert_eq!(cfg_controller.get_current(), cfg);\n\n    // update to invalid config\n    let res = cfg_controller.update(change(\"raftstore.raft-log-gc-threshold\", \"0\"));\n    res.unwrap_err();\n    assert_eq!(cfg_controller.get_current(), cfg);\n\n    // bad update request\n    let res = cfg_controller.update(change(\"xxx.yyy\", \"0\"));\n    res.unwrap_err();\n    let res = cfg_controller.update(change(\"raftstore.xxx\", \"0\"));\n    res.unwrap_err();\n    let res = cfg_controller.update(change(\"raftstore.raft-log-gc-threshold\", \"10MB\"));\n    res.unwrap_err();\n    let res = cfg_controller.update(change(\"raft-log-gc-threshold\", \"10MB\"));\n    res.unwrap_err();\n    assert_eq!(cfg_controller.get_current(), cfg);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/config/mod.rs::get_current", "code": "pub fn get_current(&self) -> TikvConfig {\n        self.inner.read().unwrap().current.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/test_config_client.rs::test_dispatch_change", "test": "fn test_dispatch_change() {\n    use std::{error::Error, result::Result};\n\n    use online_config::ConfigManager;\n\n    #[derive(Clone)]\n    struct CfgManager(Arc<Mutex<RaftstoreConfig>>);\n\n    impl ConfigManager for CfgManager {\n        fn dispatch(&mut self, c: ConfigChange) -> Result<(), Box<dyn Error>> {\n            self.0.lock().unwrap().update(c)\n        }\n    }\n\n    let (mut cfg, _dir) = TikvConfig::with_tmp().unwrap();\n    cfg.validate().unwrap();\n    let cfg_controller = ConfigController::new(cfg);\n    let mut cfg = cfg_controller.get_current();\n    let mgr = CfgManager(Arc::new(Mutex::new(cfg.raft_store.clone())));\n    cfg_controller.register(Module::Raftstore, Box::new(mgr.clone()));\n\n    cfg_controller\n        .update(change(\"raftstore.raft-log-gc-threshold\", \"2000\"))\n        .unwrap();\n\n    // config update\n    cfg.raft_store.raft_log_gc_threshold = 2000;\n    assert_eq!(cfg_controller.get_current(), cfg);\n\n    // config change should also dispatch to raftstore config manager\n    assert_eq!(mgr.0.lock().unwrap().raft_log_gc_threshold, 2000);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/config/mod.rs::get_current", "code": "pub fn get_current(&self) -> TikvConfig {\n        self.inner.read().unwrap().current.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/test_config_client.rs::test_write_update_to_file", "test": "fn test_write_update_to_file() {\n    let (mut cfg, tmp_dir) = TikvConfig::with_tmp().unwrap();\n    cfg.cfg_path = tmp_dir.path().join(\"cfg_file\").to_str().unwrap().to_owned();\n    {\n        let c = r#\"\n## comment should be reserve\n[raftstore]\n\n# config that comment out by one `#` should be update in place\n## pd-heartbeat-tick-interval = \"30s\"\n# pd-heartbeat-tick-interval = \"30s\"\n\n[rocksdb.defaultcf]\n## config should be update in place\nblock-cache-size = \"10GB\"\n\n[rocksdb.lockcf]\n## this config will not update even it has the same last \n## name as `rocksdb.defaultcf.block-cache-size`\nblock-cache-size = \"512MB\"\n\n[coprocessor]\n## the update to `coprocessor.region-split-keys`, which do not show up \n## as key-value pair after [coprocessor], will be written at the end of [coprocessor]\n\n[gc]\n## config should be update in place\nmax-write-bytes-per-sec = \"1KB\"\n\n[rocksdb.defaultcf.titan]\nblob-run-mode = \"normal\"\n\"#;\n        let mut f = File::create(&cfg.cfg_path).unwrap();\n        f.write_all(c.as_bytes()).unwrap();\n        f.sync_all().unwrap();\n    }\n    let cfg_controller = ConfigController::new(cfg);\n    let change = {\n        let mut change = HashMap::new();\n        change.insert(\n            \"raftstore.pd-heartbeat-tick-interval\".to_owned(),\n            \"1h\".to_owned(),\n        );\n        change.insert(\n            \"coprocessor.region-split-keys\".to_owned(),\n            \"10000\".to_owned(),\n        );\n        change.insert(\"gc.max-write-bytes-per-sec\".to_owned(), \"100MB\".to_owned());\n        change.insert(\n            \"rocksdb.defaultcf.block-cache-size\".to_owned(),\n            \"1GB\".to_owned(),\n        );\n        change.insert(\n            \"rocksdb.defaultcf.titan.blob-run-mode\".to_owned(),\n            \"read-only\".to_owned(),\n        );\n        change\n    };\n    cfg_controller.update(change).unwrap();\n    let res = {\n        let mut buf = Vec::new();\n        let mut f = File::open(cfg_controller.get_current().cfg_path).unwrap();\n        f.read_to_end(&mut buf).unwrap();\n        buf\n    };\n\n    let expect = r#\"\n## comment should be reserve\n[raftstore]\n\n# config that comment out by one `#` should be update in place\n## pd-heartbeat-tick-interval = \"30s\"\npd-heartbeat-tick-interval = \"1h\"\n\n[rocksdb.defaultcf]\n## config should be update in place\nblock-cache-size = \"1GB\"\n\n[rocksdb.lockcf]\n## this config will not update even it has the same last \n## name as `rocksdb.defaultcf.block-cache-size`\nblock-cache-size = \"512MB\"\n\n[coprocessor]\n## the update to `coprocessor.region-split-keys`, which do not show up \n## as key-value pair after [coprocessor], will be written at the end of [coprocessor]\n\nregion-split-keys = 10000\n[gc]\n## config should be update in place\nmax-write-bytes-per-sec = \"100MB\"\n\n[rocksdb.defaultcf.titan]\nblob-run-mode = \"read-only\"\n\"#;\n    assert_eq!(expect.as_bytes(), res.as_slice());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::as_bytes", "code": "pub fn as_bytes(&self) -> Option<BytesRef<'_>> {\n        EvaluableRef::borrow_scalar_value(self)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/test_config_client.rs::test_update_from_toml_file", "test": "fn test_update_from_toml_file() {\n    use std::{error::Error, result::Result};\n\n    use online_config::ConfigManager;\n\n    #[derive(Clone)]\n    struct CfgManager(Arc<Mutex<RaftstoreConfig>>);\n\n    impl ConfigManager for CfgManager {\n        fn dispatch(&mut self, c: ConfigChange) -> Result<(), Box<dyn Error>> {\n            self.0.lock().unwrap().update(c)\n        }\n    }\n\n    let (cfg, _dir) = TikvConfig::with_tmp().unwrap();\n    let cfg_controller = ConfigController::new(cfg);\n    let cfg = cfg_controller.get_current();\n    let mgr = CfgManager(Arc::new(Mutex::new(cfg.raft_store.clone())));\n    cfg_controller.register(Module::Raftstore, Box::new(mgr));\n\n    // update config file\n    let c = r#\"\n[raftstore]\nraft-log-gc-threshold = 2000\n\"#;\n    let mut f = File::create(&cfg.cfg_path).unwrap();\n    f.write_all(c.as_bytes()).unwrap();\n    // before update this configuration item should be the default value\n    assert_eq!(\n        cfg_controller\n            .get_current()\n            .raft_store\n            .raft_log_gc_threshold,\n        50\n    );\n    // config update from config file\n    cfg_controller.update_from_toml_file().unwrap();\n    // after update this configration item should be constant with the modified\n    // configuration file\n    assert_eq!(\n        cfg_controller\n            .get_current()\n            .raft_store\n            .raft_log_gc_threshold,\n        2000\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/config/mod.rs::get_current", "code": "pub fn get_current(&self) -> TikvConfig {\n        self.inner.read().unwrap().current.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/dynamic/pessimistic_txn.rs::test_lock_manager_cfg_update", "test": "fn test_lock_manager_cfg_update() {\n    const DEFAULT_TIMEOUT: u64 = 3000;\n    const DEFAULT_DELAY: u64 = 100;\n    let (mut cfg, _dir) = TikvConfig::with_tmp().unwrap();\n    cfg.pessimistic_txn.wait_for_lock_timeout = ReadableDuration::millis(DEFAULT_TIMEOUT);\n    cfg.pessimistic_txn.wake_up_delay_duration = ReadableDuration::millis(DEFAULT_DELAY);\n    cfg.pessimistic_txn.pipelined = false;\n    cfg.pessimistic_txn.in_memory = false;\n    cfg.validate().unwrap();\n    let (cfg_controller, waiter, deadlock, mut lock_mgr) = setup(cfg);\n\n    // update of other module's config should not effect lock manager config\n    cfg_controller\n        .update_config(\"raftstore.raft-log-gc-threshold\", \"2000\")\n        .unwrap();\n    validate_waiter(&waiter, move |timeout: ReadableDuration| {\n        assert_eq!(timeout.as_millis(), DEFAULT_TIMEOUT);\n    });\n    validate_dead_lock(&deadlock, move |ttl: u64| {\n        assert_eq!(ttl, DEFAULT_TIMEOUT);\n    });\n\n    // only update wait_for_lock_timeout\n    cfg_controller\n        .update_config(\"pessimistic-txn.wait-for-lock-timeout\", \"4000ms\")\n        .unwrap();\n    validate_waiter(&waiter, move |timeout: ReadableDuration| {\n        assert_eq!(timeout.as_millis(), 4000);\n    });\n    validate_dead_lock(&deadlock, move |ttl: u64| {\n        assert_eq!(ttl, 4000);\n    });\n\n    // update pipelined\n    assert!(\n        !lock_mgr\n            .get_storage_dynamic_configs()\n            .pipelined_pessimistic_lock\n            .load(Ordering::SeqCst)\n    );\n    cfg_controller\n        .update_config(\"pessimistic-txn.pipelined\", \"true\")\n        .unwrap();\n    assert!(\n        lock_mgr\n            .get_storage_dynamic_configs()\n            .pipelined_pessimistic_lock\n            .load(Ordering::SeqCst)\n    );\n\n    // update in-memory\n    assert!(\n        !lock_mgr\n            .get_storage_dynamic_configs()\n            .in_memory_pessimistic_lock\n            .load(Ordering::SeqCst)\n    );\n    cfg_controller\n        .update_config(\"pessimistic-txn.in-memory\", \"true\")\n        .unwrap();\n    assert!(\n        lock_mgr\n            .get_storage_dynamic_configs()\n            .in_memory_pessimistic_lock\n            .load(Ordering::SeqCst)\n    );\n\n    // update wake-up-delay-duration\n    assert_eq!(\n        lock_mgr\n            .get_storage_dynamic_configs()\n            .wake_up_delay_duration_ms\n            .load(Ordering::SeqCst),\n        DEFAULT_DELAY\n    );\n    cfg_controller\n        .update_config(\"pessimistic-txn.wake-up-delay-duration\", \"500ms\")\n        .unwrap();\n    assert_eq!(\n        lock_mgr\n            .get_storage_dynamic_configs()\n            .wake_up_delay_duration_ms\n            .load(Ordering::SeqCst),\n        500\n    );\n\n    lock_mgr.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tikv_util/src/config.rs::as_millis", "code": "pub fn as_millis(&self) -> u64 {\n        crate::time::duration_to_ms(self.0)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/config/dynamic/snap.rs::test_update_server_config", "test": "fn test_update_server_config() {\n    let (mut config, _dir) = TikvConfig::with_tmp().unwrap();\n    config.validate().unwrap();\n    let (cfg_controller, snap_worker, snap_mgr) = start_server(config.clone(), &_dir);\n    let mut svr_cfg = config.server.clone();\n    // dispatch updated config\n    let change = {\n        let mut m = std::collections::HashMap::new();\n        m.insert(\n            \"server.snap-io-max-bytes-per-sec\".to_owned(),\n            \"512MB\".to_owned(),\n        );\n        m.insert(\n            \"server.concurrent-send-snap-limit\".to_owned(),\n            \"100\".to_owned(),\n        );\n        m\n    };\n    cfg_controller.update(change).unwrap();\n\n    svr_cfg.snap_io_max_bytes_per_sec = ReadableSize::mb(512);\n    svr_cfg.concurrent_send_snap_limit = 100;\n    // config should be updated\n    assert_eq!(snap_mgr.get_speed_limit() as u64, 536870912);\n    validate(&snap_worker.scheduler(), move |cfg: &ServerConfig| {\n        assert_eq!(cfg, &svr_cfg);\n    });\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/snap.rs::get_speed_limit", "code": "pub fn get_speed_limit(&self) -> f64 {\n        self.core.limiter.speed_limit()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_column_with_lock", "test": "fn test_analyze_column_with_lock() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    for &iso_level in &[IsolationLevel::Si, IsolationLevel::Rc] {\n        let (_, endpoint, _) = init_data_with_commit(&product, &data, false);\n\n        let mut req = new_analyze_column_req(&product, 3, 3, 3, 3, 4, 32);\n        let mut ctx = Context::default();\n        ctx.set_isolation_level(iso_level);\n        req.set_context(ctx);\n\n        let resp = handle_request(&endpoint, req);\n        match iso_level {\n            IsolationLevel::Si => {\n                assert!(resp.get_data().is_empty(), \"{:?}\", resp);\n                assert!(resp.has_locked(), \"{:?}\", resp);\n            }\n            IsolationLevel::Rc => {\n                let mut analyze_resp = AnalyzeColumnsResp::default();\n                analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n                let hist = analyze_resp.get_pk_hist();\n                assert!(hist.get_buckets().is_empty());\n                assert_eq!(hist.get_ndv(), 0);\n            }\n            IsolationLevel::RcCheckTs => unimplemented!(),\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_column", "test": "fn test_analyze_column() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, None, 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, true);\n\n    let req = new_analyze_column_req(&product, 3, 3, 3, 3, 4, 32);\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_data().is_empty());\n    let mut analyze_resp = AnalyzeColumnsResp::default();\n    analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n    let hist = analyze_resp.get_pk_hist();\n    assert_eq!(hist.get_buckets().len(), 2);\n    assert_eq!(hist.get_ndv(), 4);\n    let collectors = analyze_resp.get_collectors().to_vec();\n    assert_eq!(collectors.len(), product.columns_info().len() - 1);\n    assert_eq!(collectors[0].get_null_count(), 1);\n    assert_eq!(collectors[0].get_count(), 3);\n    let rows = collectors[0].get_cm_sketch().get_rows();\n    assert_eq!(rows.len(), 4);\n    let sum: u32 = rows.first().unwrap().get_counters().iter().sum();\n    assert_eq!(sum, 3);\n    assert_eq!(collectors[0].get_total_size(), 21);\n    assert_eq!(collectors[1].get_total_size(), 4);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_single_primary_column", "test": "fn test_analyze_single_primary_column() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, None, 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, true);\n\n    let req = new_analyze_column_req(&product, 1, 3, 3, 3, 4, 32);\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_data().is_empty());\n    let mut analyze_resp = AnalyzeColumnsResp::default();\n    analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n    let hist = analyze_resp.get_pk_hist();\n    assert_eq!(hist.get_buckets().len(), 2);\n    assert_eq!(hist.get_ndv(), 4);\n    let collectors = analyze_resp.get_collectors().to_vec();\n    assert_eq!(collectors.len(), 0);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_index_with_lock", "test": "fn test_analyze_index_with_lock() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    for &iso_level in &[IsolationLevel::Si, IsolationLevel::Rc] {\n        let (_, endpoint, _) = init_data_with_commit(&product, &data, false);\n\n        let mut req = new_analyze_index_req(&product, 3, product[\"name\"].index, 4, 32, 0, 1);\n        let mut ctx = Context::default();\n        ctx.set_isolation_level(iso_level);\n        req.set_context(ctx);\n\n        let resp = handle_request(&endpoint, req);\n        match iso_level {\n            IsolationLevel::Si => {\n                assert!(resp.get_data().is_empty(), \"{:?}\", resp);\n                assert!(resp.has_locked(), \"{:?}\", resp);\n            }\n            IsolationLevel::Rc => {\n                let mut analyze_resp = AnalyzeIndexResp::default();\n                analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n                let hist = analyze_resp.get_hist();\n                assert!(hist.get_buckets().is_empty());\n                assert_eq!(hist.get_ndv(), 0);\n            }\n            IsolationLevel::RcCheckTs => unimplemented!(),\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_index", "test": "fn test_analyze_index() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, None, 4),\n        (6, Some(\"name:1\"), 1),\n        (7, Some(\"name:1\"), 1),\n        (8, Some(\"name:1\"), 1),\n        (9, Some(\"name:2\"), 1),\n        (10, Some(\"name:2\"), 1),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, true);\n\n    let req = new_analyze_index_req(&product, 3, product[\"name\"].index, 4, 32, 2, 2);\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_data().is_empty());\n    let mut analyze_resp = AnalyzeIndexResp::default();\n    analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n    let hist = analyze_resp.get_hist();\n    assert_eq!(hist.get_ndv(), 6);\n    assert_eq!(hist.get_buckets().len(), 2);\n    assert_eq!(hist.get_buckets()[0].get_count(), 5);\n    assert_eq!(hist.get_buckets()[0].get_ndv(), 3);\n    assert_eq!(hist.get_buckets()[1].get_count(), 9);\n    assert_eq!(hist.get_buckets()[1].get_ndv(), 3);\n    let rows = analyze_resp.get_cms().get_rows();\n    assert_eq!(rows.len(), 4);\n    let sum: u32 = rows.first().unwrap().get_counters().iter().sum();\n    assert_eq!(sum, 13);\n    let top_n = analyze_resp.get_cms().get_top_n();\n    let mut top_n_count = top_n\n        .iter()\n        .map(|data| data.get_count())\n        .collect::<Vec<_>>();\n    top_n_count.sort_unstable();\n    assert_eq!(top_n_count, vec![2, 3]);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_sampling_reservoir", "test": "fn test_analyze_sampling_reservoir() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, None, 4),\n        (6, Some(\"name:1\"), 1),\n        (7, Some(\"name:1\"), 1),\n        (8, Some(\"name:1\"), 1),\n        (9, Some(\"name:2\"), 1),\n        (10, Some(\"name:2\"), 1),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, true);\n\n    // Pass the 2nd column as a column group.\n    let req = new_analyze_sampling_req(&product, 1, 5, 0.0);\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_data().is_empty());\n    let mut analyze_resp = AnalyzeColumnsResp::default();\n    analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n    let collector = analyze_resp.get_row_collector();\n    assert_eq!(collector.get_samples().len(), 5);\n    // The column group is at 4th place and the data should be equal to the 2nd.\n    assert_eq!(collector.get_null_counts(), vec![0, 1, 0, 1]);\n    assert_eq!(collector.get_count(), 9);\n    assert_eq!(collector.get_fm_sketch().len(), 4);\n    assert_eq!(collector.get_total_size(), vec![72, 56, 9, 56]);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_analyze_sampling_bernoulli", "test": "fn test_analyze_sampling_bernoulli() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, None, 4),\n        (6, Some(\"name:1\"), 1),\n        (7, Some(\"name:1\"), 1),\n        (8, Some(\"name:1\"), 1),\n        (9, Some(\"name:2\"), 1),\n        (10, Some(\"name:2\"), 1),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, true);\n\n    // Pass the 2nd column as a column group.\n    let req = new_analyze_sampling_req(&product, 1, 0, 0.5);\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_data().is_empty());\n    let mut analyze_resp = AnalyzeColumnsResp::default();\n    analyze_resp.merge_from_bytes(resp.get_data()).unwrap();\n    let collector = analyze_resp.get_row_collector();\n    // The column group is at 4th place and the data should be equal to the 2nd.\n    assert_eq!(collector.get_null_counts(), vec![0, 1, 0, 1]);\n    assert_eq!(collector.get_count(), 9);\n    assert_eq!(collector.get_fm_sketch().len(), 4);\n    assert_eq!(collector.get_total_size(), vec![72, 56, 9, 56]);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_analyze.rs::test_invalid_range", "test": "fn test_invalid_range() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, true);\n    let mut req = new_analyze_index_req(&product, 3, product[\"name\"].index, 4, 32, 0, 1);\n    let mut key_range = KeyRange::default();\n    key_range.set_start(b\"xxx\".to_vec());\n    key_range.set_end(b\"zzz\".to_vec());\n    req.set_ranges(vec![key_range].into());\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_other_error().is_empty());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_stream_batch_row_limit", "test": "fn test_stream_batch_row_limit() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n        (8, Some(\"name:2\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let stream_row_limit = 2;\n    let (_, endpoint, _) = {\n        let engine = TestEngineBuilder::new().build().unwrap();\n        let mut cfg = Config::default();\n        cfg.end_point_stream_batch_row_limit = stream_row_limit;\n        init_data_with_details(Context::default(), engine, &product, &data, true, &cfg)\n    };\n\n    let req = DagSelect::from(&product).build();\n    assert_eq!(req.get_ranges().len(), 1);\n\n    // only ignore first 7 bytes of the row id\n    let ignored_suffix_len = tidb_query_datatype::codec::table::RECORD_ROW_KEY_LEN - 1;\n\n    // `expected_ranges_last_bytes` checks those assertions:\n    // 1. We always fetch no more than stream_row_limit rows.\n    // 2. The responses' key ranges are disjoint.\n    // 3. Each returned key range should cover the returned rows.\n    let mut expected_ranges_last_bytes: Vec<(&[u8], &[u8])> = vec![\n        (b\"\\x00\", b\"\\x02\\x00\"),\n        (b\"\\x02\\x00\", b\"\\x05\\x00\"),\n        (b\"\\x05\\x00\", b\"\\xFF\"),\n    ];\n    let check_range = move |resp: &Response| {\n        let (start_last_bytes, end_last_bytes) = expected_ranges_last_bytes.remove(0);\n        let start = resp.get_range().get_start();\n        let end = resp.get_range().get_end();\n        assert_eq!(&start[ignored_suffix_len..], start_last_bytes);\n\n        assert_eq!(&end[ignored_suffix_len..], end_last_bytes);\n    };\n\n    let resps = handle_streaming_select(&endpoint, req, check_range);\n    assert_eq!(resps.len(), 3);\n    let expected_output_counts = vec![vec![2_i64], vec![2_i64], vec![1_i64]];\n    for (i, resp) in resps.into_iter().enumerate() {\n        let mut chunk = Chunk::default();\n        chunk.merge_from_bytes(resp.get_data()).unwrap();\n        assert_eq!(\n            resp.get_output_counts(),\n            expected_output_counts[i].as_slice(),\n        );\n\n        let chunks = vec![chunk];\n        let chunk_data_limit = stream_row_limit * 3; // we have 3 fields.\n        check_chunk_datum_count(&chunks, chunk_data_limit);\n\n        let spliter = DagChunkSpliter::new(chunks, 3);\n        let j = cmp::min((i + 1) * stream_row_limit, data.len());\n        let cur_data = &data[i * stream_row_limit..j];\n        for (row, &(id, name, cnt)) in spliter.zip(cur_data) {\n            let name_datum = name.map(|s| s.as_bytes()).into();\n            let expected_encoded = datum::encode_value(\n                &mut EvalContext::default(),\n                &[Datum::I64(id), name_datum, cnt.into()],\n            )\n            .unwrap();\n            let result_encoded = datum::encode_value(&mut EvalContext::default(), &row).unwrap();\n            assert_eq!(result_encoded, &*expected_encoded);\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_select_failed", "test": "fn test_select_failed() {\n    let mut cluster = test_raftstore::new_server_cluster(0, 3);\n    cluster.cfg.raft_store.check_leader_lease_interval = ReadableDuration::hours(10);\n    cluster.run();\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(b\"\"), None);\n    let region = cluster.get_region(b\"\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let engine = cluster.sim.rl().storages[&leader.get_id()].clone();\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader);\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) =\n        init_data_with_engine_and_commit(ctx.clone(), engine, &product, &[], true);\n\n    // Sleep until the leader lease is expired.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_heartbeat_interval()\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32\n            * 2,\n    );\n    for id in 1..=3 {\n        if id != ctx.get_peer().get_store_id() {\n            cluster.stop_node(id);\n        }\n    }\n    let req = DagSelect::from(&product).build_with(ctx.clone(), &[0]);\n    let f = endpoint.parse_and_handle_unary_request(req, None);\n    cluster.stop_node(ctx.get_peer().get_store_id());\n    drop(cluster);\n    let _ = futures::executor::block_on(f);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_index_aggr_count", "test": "fn test_index_aggr_count() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:3\"), 3),\n        (4, Some(\"name:0\"), 1),\n        (5, Some(\"name:5\"), 4),\n        (6, Some(\"name:5\"), 4),\n        (7, None, 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &data);\n    // for dag\n    let req = DagSelect::from_index(&product, &product[\"name\"])\n        .count(&product[\"id\"])\n        .output_offsets(Some(vec![0]))\n        .build();\n    let mut resp = handle_select(&endpoint, req);\n    let mut spliter = DagChunkSpliter::new(resp.take_chunks().into(), 1);\n    let expected_encoded = datum::encode_value(\n        &mut EvalContext::default(),\n        &[Datum::U64(data.len() as u64)],\n    )\n    .unwrap();\n    let ret_data = spliter.next();\n    assert_eq!(ret_data.is_some(), true);\n    let result_encoded =\n        datum::encode_value(&mut EvalContext::default(), &ret_data.unwrap()).unwrap();\n    assert_eq!(&*result_encoded, &*expected_encoded);\n    assert_eq!(spliter.next().is_none(), true);\n\n    let exp = vec![\n        (Datum::Null, 1),\n        (Datum::Bytes(b\"name:0\".to_vec()), 2),\n        (Datum::Bytes(b\"name:3\".to_vec()), 1),\n        (Datum::Bytes(b\"name:5\".to_vec()), 2),\n    ];\n    // for dag\n    let req = DagSelect::from_index(&product, &product[\"name\"])\n        .count(&product[\"id\"])\n        .group_by(&[&product[\"name\"]])\n        .output_offsets(Some(vec![0, 1]))\n        .build();\n    resp = handle_select(&endpoint, req);\n    let mut row_count = 0;\n    let exp_len = exp.len();\n    let spliter = DagChunkSpliter::new(resp.take_chunks().into(), 2);\n    let mut results = spliter.collect::<Vec<Vec<Datum>>>();\n    sort_by!(results, 1, Bytes);\n    for (row, (name, cnt)) in results.iter().zip(exp) {\n        let expected_datum = vec![Datum::U64(cnt), name];\n        let expected_encoded =\n            datum::encode_value(&mut EvalContext::default(), &expected_datum).unwrap();\n        let result_encoded = datum::encode_value(&mut EvalContext::default(), row).unwrap();\n        assert_eq!(&*result_encoded, &*expected_encoded);\n        row_count += 1;\n    }\n    assert_eq!(row_count, exp_len);\n\n    let exp = vec![\n        (vec![Datum::Null, Datum::I64(4)], 1),\n        (vec![Datum::Bytes(b\"name:0\".to_vec()), Datum::I64(1)], 1),\n        (vec![Datum::Bytes(b\"name:0\".to_vec()), Datum::I64(2)], 1),\n        (vec![Datum::Bytes(b\"name:3\".to_vec()), Datum::I64(3)], 1),\n        (vec![Datum::Bytes(b\"name:5\".to_vec()), Datum::I64(4)], 2),\n    ];\n    let req = DagSelect::from_index(&product, &product[\"name\"])\n        .count(&product[\"id\"])\n        .group_by(&[&product[\"name\"], &product[\"count\"]])\n        .build();\n    resp = handle_select(&endpoint, req);\n    let mut row_count = 0;\n    let exp_len = exp.len();\n    let spliter = DagChunkSpliter::new(resp.take_chunks().into(), 3);\n    let mut results = spliter.collect::<Vec<Vec<Datum>>>();\n    sort_by!(results, 1, Bytes);\n    for (row, (gk_data, cnt)) in results.iter().zip(exp) {\n        let mut expected_datum = vec![Datum::U64(cnt)];\n        expected_datum.extend_from_slice(gk_data.as_slice());\n        let expected_encoded =\n            datum::encode_value(&mut EvalContext::default(), &expected_datum).unwrap();\n        let result_encoded = datum::encode_value(&mut EvalContext::default(), row).unwrap();\n        assert_eq!(&*result_encoded, &*expected_encoded);\n        row_count += 1;\n    }\n    assert_eq!(row_count, exp_len);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_key_is_locked_for_primary", "test": "fn test_key_is_locked_for_primary() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, false);\n\n    let req = DagSelect::from(&product).build();\n    let resp = handle_request(&endpoint, req);\n    assert!(resp.get_data().is_empty(), \"{:?}\", resp);\n    assert!(resp.has_locked(), \"{:?}\", resp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_key_is_locked_for_index", "test": "fn test_key_is_locked_for_index() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint, _) = init_data_with_commit(&product, &data, false);\n\n    let req = DagSelect::from_index(&product, &product[\"name\"]).build();\n    let resp = handle_request(&endpoint, req);\n    assert!(resp.get_data().is_empty(), \"{:?}\", resp);\n    assert!(resp.has_locked(), \"{:?}\", resp);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/coprocessor/test_select.rs::test_invalid_range", "test": "fn test_invalid_range() {\n    let data = vec![\n        (1, Some(\"name:0\"), 2),\n        (2, Some(\"name:4\"), 3),\n        (4, Some(\"name:3\"), 1),\n        (5, Some(\"name:1\"), 4),\n    ];\n\n    let product = ProductTable::new();\n    let (_, endpoint) = init_with_data(&product, &data);\n\n    let mut select = DagSelect::from(&product);\n    select.key_ranges[0].set_start(b\"xxx\".to_vec());\n    select.key_ranges[0].set_end(b\"zzz\".to_vec());\n    let req = select.build();\n    let resp = handle_request(&endpoint, req);\n    assert!(!resp.get_other_error().is_empty());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/import/sst_service.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.pending_writes.is_empty() && self.unpacked_size == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_upload_sst", "test": "fn test_upload_sst() {\n    let (_cluster, ctx, _, import) = new_cluster_and_tikv_import_client();\n\n    let data = vec![1; 1024];\n    let crc32 = calc_data_crc32(&data);\n    let length = data.len() as u64;\n\n    // Mismatch crc32\n    let meta = new_sst_meta(0, length);\n    assert_to_string_contains!(send_upload_sst(&import, &meta, &data).unwrap_err(), \"crc32\");\n\n    let mut meta = new_sst_meta(crc32, length);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n    send_upload_sst(&import, &meta, &data).unwrap();\n\n    // Can't upload the same uuid file again.\n    assert_to_string_contains!(\n        send_upload_sst(&import, &meta, &data).unwrap_err(),\n        \"FileExists\"\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/util.rs::send_upload_sst", "code": "pub fn send_upload_sst(\n    client: &ImportSstClient,\n    meta: &SstMeta,\n    data: &[u8],\n) -> Result<UploadResponse> {\n    let mut r1 = UploadRequest::default();\n    r1.set_meta(meta.clone());\n    let mut r2 = UploadRequest::default();\n    r2.set_data(data.to_vec());\n    let reqs: Vec<_> = vec![r1, r2]\n        .into_iter()\n        .map(|r| Result::Ok((r, WriteFlags::default())))\n        .collect();\n    let (mut tx, rx) = client.upload().unwrap();\n    let mut stream = stream::iter(reqs);\n    block_on(async move {\n        tx.send_all(&mut stream).await?;\n        tx.close().await?;\n        rx.await\n    })\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_cleanup_sst", "test": "fn test_cleanup_sst() {\n    let (mut cluster, ctx, _, import) = new_cluster_and_tikv_import_client();\n\n    let temp_dir = Builder::new().prefix(\"test_cleanup_sst\").tempdir().unwrap();\n\n    let sst_path = temp_dir.path().join(\"test_split.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(ctx.get_region_id());\n    meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n    send_upload_sst(&import, &meta, &data).unwrap();\n\n    // Can not upload the same file when it exists.\n    assert_to_string_contains!(\n        send_upload_sst(&import, &meta, &data).unwrap_err(),\n        \"FileExists\"\n    );\n\n    // The uploaded SST should be deleted if the region split.\n    let region = cluster.get_region(&[]);\n    cluster.must_split(&region, &[100]);\n\n    check_sst_deleted(&import, &meta, &data);\n\n    let left = cluster.get_region(&[]);\n    let right = cluster.get_region(&[100]);\n\n    let sst_path = temp_dir.path().join(\"test_merge.sst\");\n    let sst_range = (0, 100);\n    let (mut meta, data) = gen_sst_file(sst_path, sst_range);\n    meta.set_region_id(left.get_id());\n    meta.set_region_epoch(left.get_region_epoch().clone());\n\n    send_upload_sst(&import, &meta, &data).unwrap();\n\n    // The uploaded SST should be deleted if the region merged.\n    cluster.pd_client.must_merge(left.get_id(), right.get_id());\n    let res = block_on(cluster.pd_client.get_region_by_id(left.get_id()));\n    assert_eq!(res.unwrap(), None);\n\n    check_sst_deleted(&import, &meta, &data);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/util.rs::send_upload_sst", "code": "pub fn send_upload_sst(\n    client: &ImportSstClient,\n    meta: &SstMeta,\n    data: &[u8],\n) -> Result<UploadResponse> {\n    let mut r1 = UploadRequest::default();\n    r1.set_meta(meta.clone());\n    let mut r2 = UploadRequest::default();\n    r2.set_data(data.to_vec());\n    let reqs: Vec<_> = vec![r1, r2]\n        .into_iter()\n        .map(|r| Result::Ok((r, WriteFlags::default())))\n        .collect();\n    let (mut tx, rx) = client.upload().unwrap();\n    let mut stream = stream::iter(reqs);\n    block_on(async move {\n        tx.send_all(&mut stream).await?;\n        tx.close().await?;\n        rx.await\n    })\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/import/test_sst_service.rs::test_suspend_import", "test": "fn test_suspend_import() {\n    let (_cluster, ctx, tikv, import) = new_cluster_and_tikv_import_client();\n    let sst_range = (0, 10);\n    let write = |sst_range: (u8, u8)| {\n        let mut meta = new_sst_meta(0, 0);\n        meta.set_region_id(ctx.get_region_id());\n        meta.set_region_epoch(ctx.get_region_epoch().clone());\n\n        let mut keys = vec![];\n        let mut values = vec![];\n        for i in sst_range.0..sst_range.1 {\n            keys.push(vec![i]);\n            values.push(vec![i]);\n        }\n        send_write_sst(&import, &meta, keys, values, 1)\n    };\n    let ingest = |sst_meta: &SstMeta| {\n        let mut ingest = IngestRequest::default();\n        ingest.set_context(ctx.clone());\n        ingest.set_sst(sst_meta.clone());\n        import.ingest(&ingest)\n    };\n    let multi_ingest = |sst_metas: &[SstMeta]| {\n        let mut multi_ingest = MultiIngestRequest::default();\n        multi_ingest.set_context(ctx.clone());\n        multi_ingest.set_ssts(sst_metas.to_vec().into());\n        import.multi_ingest(&multi_ingest)\n    };\n    let suspendctl = |for_time| {\n        let mut req = SuspendImportRpcRequest::default();\n        req.set_caller(\"test_suspend_import\".to_owned());\n        if for_time == 0 {\n            req.set_should_suspend_imports(false);\n        } else {\n            req.set_should_suspend_imports(true);\n            req.set_duration_in_secs(for_time);\n        }\n        req\n    };\n\n    let write_res = write(sst_range).unwrap();\n    assert_eq!(write_res.metas.len(), 1);\n    let sst = write_res.metas[0].clone();\n\n    assert!(\n        !import\n            .suspend_import_rpc(&suspendctl(6000))\n            .unwrap()\n            .already_suspended\n    );\n    let write_res = write(sst_range);\n    write_res.unwrap();\n    let ingest_res = ingest(&sst);\n    assert_to_string_contains!(ingest_res.unwrap_err(), \"Suspended\");\n    let multi_ingest_res = multi_ingest(&[sst.clone()]);\n    assert_to_string_contains!(multi_ingest_res.unwrap_err(), \"Suspended\");\n\n    assert!(\n        import\n            .suspend_import_rpc(&suspendctl(0))\n            .unwrap()\n            .already_suspended\n    );\n\n    let ingest_res = ingest(&sst);\n    assert!(ingest_res.is_ok(), \"{:?} => {:?}\", sst, ingest_res);\n\n    check_ingested_txn_kvs(&tikv, &ctx, sst_range, 2);\n\n    // test timeout.\n    assert!(\n        !import\n            .suspend_import_rpc(&suspendctl(1))\n            .unwrap()\n            .already_suspended\n    );\n    let sst_range = (10, 20);\n    let write_res = write(sst_range);\n    let sst = write_res.unwrap().metas;\n    let res = multi_ingest(&sst);\n    assert_to_string_contains!(res.unwrap_err(), \"Suspended\");\n    std::thread::sleep(Duration::from_secs(1));\n    multi_ingest(&sst).unwrap();\n\n    // check an insane value should be rejected.\n    import\n        .suspend_import_rpc(&suspendctl(u64::MAX - 42))\n        .unwrap_err();\n    let sst_range = (20, 30);\n    let ssts = write(sst_range).unwrap();\n    multi_ingest(ssts.get_metas()).unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_rpc_client", "test": "fn test_rpc_client() {\n    let rt = setup_runtime();\n    let _g = rt.enter();\n    let eps_count = 1;\n    let server = MockServer::new(eps_count);\n    let eps = server.bind_addrs();\n\n    let mut client = new_client_v2(eps.clone(), None);\n    assert_ne!(client.fetch_cluster_id().unwrap(), 0);\n\n    let store_id = client.alloc_id().unwrap();\n    let mut store = metapb::Store::default();\n    store.set_id(store_id);\n    debug!(\"bootstrap store {:?}\", store);\n\n    let peer_id = client.alloc_id().unwrap();\n    let mut peer = metapb::Peer::default();\n    peer.set_id(peer_id);\n    peer.set_store_id(store_id);\n\n    let region_id = client.alloc_id().unwrap();\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    region.mut_peers().push(peer.clone());\n    debug!(\"bootstrap region {:?}\", region);\n\n    client\n        .bootstrap_cluster(store.clone(), region.clone())\n        .unwrap();\n    assert_eq!(client.is_cluster_bootstrapped().unwrap(), true);\n\n    let tmp_stores = client.get_all_stores(false).unwrap();\n    assert_eq!(tmp_stores.len(), 1);\n    assert_eq!(tmp_stores[0], store);\n\n    let tmp_store = client.get_store(store_id).unwrap();\n    assert_eq!(tmp_store.get_id(), store.get_id());\n\n    let region_key = region.get_start_key();\n    let tmp_region = client.get_region(region_key).unwrap();\n    assert_eq!(tmp_region.get_id(), region.get_id());\n\n    let region_info = client.get_region_info(region_key).unwrap();\n    assert_eq!(region_info.region, region);\n    assert_eq!(region_info.leader, None);\n\n    let tmp_region = block_on(client.get_region_by_id(region_id))\n        .unwrap()\n        .unwrap();\n    assert_eq!(tmp_region.get_id(), region.get_id());\n\n    let ts = must_get_tso(&mut client, 1);\n    assert_ne!(ts, TimeStamp::zero());\n\n    let ts100 = must_get_tso(&mut client, 100);\n    assert_eq!(ts.logical() + 100, ts100.logical());\n\n    let mut prev_id = 0;\n    for _ in 0..10 {\n        let mut client = new_client_v2(eps.clone(), None);\n        let alloc_id = client.alloc_id().unwrap();\n        assert!(alloc_id > prev_id);\n        prev_id = alloc_id;\n    }\n\n    let (tx, mut responses) = client\n        .create_region_heartbeat_stream(WakePolicy::Immediately)\n        .unwrap();\n    let mut req = pdpb::RegionHeartbeatRequest::default();\n    req.set_region(region.clone());\n    req.set_leader(peer.clone());\n    tx.send(req).unwrap();\n    block_on(tokio::time::timeout(\n        Duration::from_secs(3),\n        responses.next(),\n    ))\n    .unwrap();\n\n    let region_info = client.get_region_info(region_key).unwrap();\n    assert_eq!(region_info.region, region);\n    assert_eq!(region_info.leader.unwrap(), peer);\n\n    block_on(client.store_heartbeat(\n        pdpb::StoreStats::default(),\n        None, // store_report\n        None,\n    ))\n    .unwrap();\n    block_on(client.ask_batch_split(metapb::Region::default(), 1)).unwrap();\n    block_on(client.report_batch_split(vec![metapb::Region::default(), metapb::Region::default()]))\n        .unwrap();\n\n    let region_info = client.get_region_info(region_key).unwrap();\n    client.scatter_region(region_info).unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client_v2.rs::fetch_cluster_id", "code": "fn fetch_cluster_id(&mut self) -> Result<u64> {\n        if !self.raw_client.initialized() {\n            block_on(self.raw_client.wait_for_ready())?;\n        }\n        let id = self.raw_client.cluster_id();\n        assert!(id > 0);\n        Ok(id)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_get_tombstone_stores", "test": "fn test_get_tombstone_stores() {\n    let eps_count = 1;\n    let server = MockServer::new(eps_count);\n    let eps = server.bind_addrs();\n    let mut client = new_client_v2(eps, None);\n\n    let mut all_stores = vec![];\n    let store_id = client.alloc_id().unwrap();\n    let mut store = metapb::Store::default();\n    store.set_id(store_id);\n    let region_id = client.alloc_id().unwrap();\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    client.bootstrap_cluster(store.clone(), region).unwrap();\n\n    all_stores.push(store);\n    assert_eq!(client.is_cluster_bootstrapped().unwrap(), true);\n    let s = client.get_all_stores(false).unwrap();\n    assert_eq!(s, all_stores);\n\n    // Add tombstone store.\n    let mut store99 = metapb::Store::default();\n    store99.set_id(99);\n    store99.set_state(metapb::StoreState::Tombstone);\n    server.default_handler().add_store(store99.clone());\n\n    // do not include tombstone.\n    let s = client.get_all_stores(true).unwrap();\n    assert_eq!(s, all_stores);\n\n    all_stores.push(store99.clone());\n    all_stores.sort_by_key(|a| a.get_id());\n    // include tombstone, there should be 2 stores.\n    let mut s = client.get_all_stores(false).unwrap();\n    s.sort_by_key(|a| a.get_id());\n    assert_eq!(s, all_stores);\n\n    // Add another tombstone store.\n    let mut store199 = store99;\n    store199.set_id(199);\n    server.default_handler().add_store(store199.clone());\n\n    all_stores.push(store199);\n    all_stores.sort_by_key(|a| a.get_id());\n    let mut s = client.get_all_stores(false).unwrap();\n    s.sort_by_key(|a| a.get_id());\n    assert_eq!(s, all_stores);\n\n    client.get_store(store_id).unwrap();\n    client.get_store(99).unwrap_err();\n    client.get_store(199).unwrap_err();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::is_cluster_bootstrapped", "code": "fn is_cluster_bootstrapped(&self) -> Result<bool> {\n        let _timer = PD_REQUEST_HISTOGRAM_VEC\n            .is_cluster_bootstrapped\n            .start_coarse_timer();\n\n        let mut req = pdpb::IsBootstrappedRequest::default();\n        req.set_header(self.header());\n\n        let resp = sync_request(&self.pd_client, LEADER_CHANGE_RETRY, |client, option| {\n            client.is_bootstrapped_opt(&req, option)\n        })?;\n        check_resp_header(resp.get_header())?;\n\n        Ok(resp.get_bootstrapped())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_get_tombstone_store", "test": "fn test_get_tombstone_store() {\n    let eps_count = 1;\n    let server = MockServer::new(eps_count);\n    let eps = server.bind_addrs();\n    let mut client = new_client_v2(eps, None);\n\n    let mut all_stores = vec![];\n    let store_id = client.alloc_id().unwrap();\n    let mut store = metapb::Store::default();\n    store.set_id(store_id);\n    let region_id = client.alloc_id().unwrap();\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    client.bootstrap_cluster(store.clone(), region).unwrap();\n\n    all_stores.push(store);\n    assert_eq!(client.is_cluster_bootstrapped().unwrap(), true);\n    let s = client.get_all_stores(false).unwrap();\n    assert_eq!(s, all_stores);\n\n    // Add tombstone store.\n    let mut store99 = metapb::Store::default();\n    store99.set_id(99);\n    store99.set_state(metapb::StoreState::Tombstone);\n    server.default_handler().add_store(store99.clone());\n\n    let r = client.get_store(99);\n    assert_eq!(r.unwrap_err().error_code(), error_code::pd::STORE_TOMBSTONE);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::is_cluster_bootstrapped", "code": "fn is_cluster_bootstrapped(&self) -> Result<bool> {\n        let _timer = PD_REQUEST_HISTOGRAM_VEC\n            .is_cluster_bootstrapped\n            .start_coarse_timer();\n\n        let mut req = pdpb::IsBootstrappedRequest::default();\n        req.set_header(self.header());\n\n        let resp = sync_request(&self.pd_client, LEADER_CHANGE_RETRY, |client, option| {\n            client.is_bootstrapped_opt(&req, option)\n        })?;\n        check_resp_header(resp.get_header())?;\n\n        Ok(resp.get_bootstrapped())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_reboot", "test": "fn test_reboot() {\n    let eps_count = 1;\n    let server = MockServer::with_case(eps_count, Arc::new(AlreadyBootstrapped));\n    let eps = server.bind_addrs();\n    let mut client = new_client_v2(eps, None);\n\n    assert!(!client.is_cluster_bootstrapped().unwrap());\n\n    match client.bootstrap_cluster(metapb::Store::default(), metapb::Region::default()) {\n        Err(PdError::ClusterBootstrapped(_)) => (),\n        _ => {\n            panic!(\"failed, should return ClusterBootstrapped\");\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::is_cluster_bootstrapped", "code": "fn is_cluster_bootstrapped(&self) -> Result<bool> {\n        let _timer = PD_REQUEST_HISTOGRAM_VEC\n            .is_cluster_bootstrapped\n            .start_coarse_timer();\n\n        let mut req = pdpb::IsBootstrappedRequest::default();\n        req.set_header(self.header());\n\n        let resp = sync_request(&self.pd_client, LEADER_CHANGE_RETRY, |client, option| {\n            client.is_bootstrapped_opt(&req, option)\n        })?;\n        check_resp_header(resp.get_header())?;\n\n        Ok(resp.get_bootstrapped())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_incompatible_version", "test": "fn test_incompatible_version() {\n    let incompatible = Arc::new(Incompatible);\n    let server = MockServer::with_case(1, incompatible);\n    let eps = server.bind_addrs();\n\n    let mut client = new_client_v2(eps, None);\n\n    let resp = block_on(client.ask_batch_split(metapb::Region::default(), 2));\n    assert_eq!(\n        resp.unwrap_err().to_string(),\n        PdError::Incompatible.to_string()\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/datum.rs::to_string", "code": "pub fn to_string(&self) -> Result<String> {\n        let s = match *self {\n            Datum::I64(i) => format!(\"{}\", i),\n            Datum::U64(u) => format!(\"{}\", u),\n            Datum::F64(f) => format!(\"{}\", f),\n            Datum::Bytes(ref bs) => String::from_utf8(bs.to_vec())?,\n            Datum::Time(t) => format!(\"{}\", t),\n            Datum::Dur(ref d) => format!(\"{}\", d),\n            Datum::Dec(ref d) => format!(\"{}\", d),\n            Datum::Json(ref d) => d.to_string(),\n            Datum::Enum(ref e) => e.to_string(),\n            Datum::Set(ref s) => s.to_string(),\n            ref d => return Err(invalid_type!(\"can't convert {} to string\", d)),\n        };\n        Ok(s)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_pd_client_heartbeat_send_failed", "test": "fn test_pd_client_heartbeat_send_failed() {\n    let rt = setup_runtime();\n    let _g = rt.enter();\n    let pd_client_send_fail_fp = \"region_heartbeat_send_failed\";\n    fail::cfg(pd_client_send_fail_fp, \"return()\").unwrap();\n    let server = MockServer::with_case(1, Arc::new(AlreadyBootstrapped));\n    let eps = server.bind_addrs();\n\n    let mut client = new_client_v2(eps, None);\n\n    let (tx, mut responses) = client\n        .create_region_heartbeat_stream(WakePolicy::Immediately)\n        .unwrap();\n\n    let mut heartbeat_send_fail = |ok| {\n        let mut region = metapb::Region::default();\n        region.set_id(1);\n        let mut req = pdpb::RegionHeartbeatRequest::default();\n        req.set_region(region);\n        tx.send(req).unwrap();\n\n        let rsp = block_on(tokio::time::timeout(\n            Duration::from_millis(100),\n            responses.next(),\n        ));\n        if ok {\n            assert!(rsp.is_ok());\n            assert_eq!(rsp.unwrap().unwrap().unwrap().get_region_id(), 1);\n        } else {\n            rsp.unwrap_err();\n        }\n\n        let region = block_on(client.get_region_by_id(1));\n        if ok {\n            assert!(region.is_ok());\n            let r = region.unwrap();\n            assert!(r.is_some());\n            assert_eq!(1, r.unwrap().get_id());\n        } else {\n            region.unwrap_err();\n        }\n    };\n    // send fail if network is block.\n    heartbeat_send_fail(false);\n    fail::remove(pd_client_send_fail_fp);\n    // send success after network recovered.\n    heartbeat_send_fail(true);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_pd/src/mocker/retry.rs::is_ok", "code": "fn is_ok(&self) -> bool {\n        let count = self.count.fetch_add(1, Ordering::SeqCst);\n        if count != 0 && count % self.retry == 0 {\n            // it's ok.\n            return true;\n        }\n        // let's sleep awhile, so that client will update its connection.\n        thread::sleep(REQUEST_RECONNECT_INTERVAL);\n        false\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client.rs::test_cluster_version", "test": "fn test_cluster_version() {\n    let server = MockServer::<Service>::new(3);\n    let eps = server.bind_addrs();\n\n    let feature_a = Feature::require(0, 0, 1);\n    let feature_b = Feature::require(5, 0, 0);\n    let feature_c = Feature::require(5, 0, 1);\n\n    let mut client = new_client_v2(eps, None);\n    let feature_gate = client.feature_gate().clone();\n    assert!(!feature_gate.can_enable(feature_a));\n\n    let mut client_clone = client.clone();\n    let mut emit_heartbeat = || {\n        let req = pdpb::StoreStats::default();\n        block_on(client_clone.store_heartbeat(req, /* store_report= */ None, None)).unwrap();\n    };\n\n    let set_cluster_version = |version: &str| {\n        let h = server.default_handler();\n        h.set_cluster_version(version.to_owned());\n    };\n\n    // Empty version string will be treated as invalid.\n    emit_heartbeat();\n    assert!(!feature_gate.can_enable(feature_a));\n\n    // Explicitly invalid version string.\n    set_cluster_version(\"invalid-version\");\n    emit_heartbeat();\n    assert!(!feature_gate.can_enable(feature_a));\n\n    // Correct version string.\n    set_cluster_version(\"5.0.0\");\n    emit_heartbeat();\n    assert!(feature_gate.can_enable(feature_a));\n    assert!(feature_gate.can_enable(feature_b));\n    assert!(!feature_gate.can_enable(feature_c));\n\n    // Version can't go backwards.\n    set_cluster_version(\"4.99\");\n    emit_heartbeat();\n    assert!(feature_gate.can_enable(feature_b));\n    assert!(!feature_gate.can_enable(feature_c));\n\n    // After reconnect the version should be still accessable.\n    // The GLOBAL_RECONNECT_INTERVAL is 0.1s so sleeps 0.2s here.\n    thread::sleep(Duration::from_millis(200));\n    client.reconnect().unwrap();\n    assert!(feature_gate.can_enable(feature_b));\n    assert!(!feature_gate.can_enable(feature_c));\n\n    // Version can go forwards.\n    set_cluster_version(\"5.0.1\");\n    emit_heartbeat();\n    assert!(feature_gate.can_enable(feature_c));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/feature_gate.rs::can_enable", "code": "pub fn can_enable(&self, feature: Feature) -> bool {\n        self.version.load(Ordering::Relaxed) >= feature.ver\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_rpc_client", "test": "fn test_rpc_client() {\n    let eps_count = 1;\n    let server = MockServer::new(eps_count);\n    let eps = server.bind_addrs();\n\n    let client = new_client(eps.clone(), None);\n    assert_ne!(client.get_cluster_id().unwrap(), 0);\n\n    let store_id = client.alloc_id().unwrap();\n    let mut store = metapb::Store::default();\n    store.set_id(store_id);\n    debug!(\"bootstrap store {:?}\", store);\n\n    let peer_id = client.alloc_id().unwrap();\n    let mut peer = metapb::Peer::default();\n    peer.set_id(peer_id);\n    peer.set_store_id(store_id);\n\n    let region_id = client.alloc_id().unwrap();\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    region.mut_peers().push(peer.clone());\n    debug!(\"bootstrap region {:?}\", region);\n\n    client\n        .bootstrap_cluster(store.clone(), region.clone())\n        .unwrap();\n    assert_eq!(client.is_cluster_bootstrapped().unwrap(), true);\n\n    let tmp_stores = client.get_all_stores(false).unwrap();\n    assert_eq!(tmp_stores.len(), 1);\n    assert_eq!(tmp_stores[0], store);\n\n    let tmp_store = client.get_store(store_id).unwrap();\n    assert_eq!(tmp_store.get_id(), store.get_id());\n\n    let region_key = region.get_start_key();\n    let tmp_region = client.get_region(region_key).unwrap();\n    assert_eq!(tmp_region.get_id(), region.get_id());\n\n    let region_info = client.get_region_info(region_key).unwrap();\n    assert_eq!(region_info.region, region);\n    assert_eq!(region_info.leader, None);\n\n    let tmp_region = block_on(client.get_region_by_id(region_id))\n        .unwrap()\n        .unwrap();\n    assert_eq!(tmp_region.get_id(), region.get_id());\n\n    let ts = block_on(client.get_tso()).unwrap();\n    assert_ne!(ts, TimeStamp::zero());\n\n    let ts100 = block_on(client.batch_get_tso(100)).unwrap();\n    assert_eq!(ts.logical() + 100, ts100.logical());\n\n    let mut prev_id = 0;\n    for _ in 0..100 {\n        let client = new_client(eps.clone(), None);\n        let alloc_id = client.alloc_id().unwrap();\n        assert!(alloc_id > prev_id);\n        prev_id = alloc_id;\n    }\n\n    let poller = Builder::new_multi_thread()\n        .thread_name(thd_name!(\"poller\"))\n        .worker_threads(1)\n        .build()\n        .unwrap();\n    let (tx, rx) = mpsc::channel();\n    let f = client.handle_region_heartbeat_response(1, move |resp| {\n        let _ = tx.send(resp);\n    });\n    poller.spawn(f);\n    poller.spawn(client.region_heartbeat(\n        store::RAFT_INIT_LOG_TERM,\n        region.clone(),\n        peer.clone(),\n        RegionStat::default(),\n        None,\n    ));\n    rx.recv_timeout(Duration::from_secs(3)).unwrap();\n\n    let region_info = client.get_region_info(region_key).unwrap();\n    assert_eq!(region_info.region, region);\n    assert_eq!(region_info.leader.unwrap(), peer);\n\n    block_on(client.store_heartbeat(\n        pdpb::StoreStats::default(),\n        None, // store_report\n        None,\n    ))\n    .unwrap();\n    block_on(client.ask_batch_split(metapb::Region::default(), 1)).unwrap();\n    block_on(client.report_batch_split(vec![metapb::Region::default(), metapb::Region::default()]))\n        .unwrap();\n\n    let region_info = client.get_region_info(region_key).unwrap();\n    client.scatter_region(region_info).unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::get_cluster_id", "code": "fn get_cluster_id(&self) -> Result<u64> {\n        Ok(self.cluster_id)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_get_tombstone_stores", "test": "fn test_get_tombstone_stores() {\n    let eps_count = 1;\n    let server = MockServer::new(eps_count);\n    let eps = server.bind_addrs();\n    let client = new_client(eps, None);\n\n    let mut all_stores = vec![];\n    let store_id = client.alloc_id().unwrap();\n    let mut store = metapb::Store::default();\n    store.set_id(store_id);\n    let region_id = client.alloc_id().unwrap();\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    client.bootstrap_cluster(store.clone(), region).unwrap();\n\n    all_stores.push(store);\n    assert_eq!(client.is_cluster_bootstrapped().unwrap(), true);\n    let s = client.get_all_stores(false).unwrap();\n    assert_eq!(s, all_stores);\n\n    // Add tombstone store.\n    let mut store99 = metapb::Store::default();\n    store99.set_id(99);\n    store99.set_state(metapb::StoreState::Tombstone);\n    server.default_handler().add_store(store99.clone());\n\n    // do not include tombstone.\n    let s = client.get_all_stores(true).unwrap();\n    assert_eq!(s, all_stores);\n\n    all_stores.push(store99.clone());\n    all_stores.sort_by_key(|a| a.get_id());\n    // include tombstone, there should be 2 stores.\n    let mut s = client.get_all_stores(false).unwrap();\n    s.sort_by_key(|a| a.get_id());\n    assert_eq!(s, all_stores);\n\n    // Add another tombstone store.\n    let mut store199 = store99;\n    store199.set_id(199);\n    server.default_handler().add_store(store199.clone());\n\n    all_stores.push(store199);\n    all_stores.sort_by_key(|a| a.get_id());\n    let mut s = client.get_all_stores(false).unwrap();\n    s.sort_by_key(|a| a.get_id());\n    assert_eq!(s, all_stores);\n\n    client.get_store(store_id).unwrap();\n    client.get_store(99).unwrap_err();\n    client.get_store(199).unwrap_err();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::is_cluster_bootstrapped", "code": "fn is_cluster_bootstrapped(&self) -> Result<bool> {\n        let _timer = PD_REQUEST_HISTOGRAM_VEC\n            .is_cluster_bootstrapped\n            .start_coarse_timer();\n\n        let mut req = pdpb::IsBootstrappedRequest::default();\n        req.set_header(self.header());\n\n        let resp = sync_request(&self.pd_client, LEADER_CHANGE_RETRY, |client, option| {\n            client.is_bootstrapped_opt(&req, option)\n        })?;\n        check_resp_header(resp.get_header())?;\n\n        Ok(resp.get_bootstrapped())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_get_tombstone_store", "test": "fn test_get_tombstone_store() {\n    let eps_count = 1;\n    let server = MockServer::new(eps_count);\n    let eps = server.bind_addrs();\n    let client = new_client(eps, None);\n\n    let mut all_stores = vec![];\n    let store_id = client.alloc_id().unwrap();\n    let mut store = metapb::Store::default();\n    store.set_id(store_id);\n    let region_id = client.alloc_id().unwrap();\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    client.bootstrap_cluster(store.clone(), region).unwrap();\n\n    all_stores.push(store);\n    assert_eq!(client.is_cluster_bootstrapped().unwrap(), true);\n    let s = client.get_all_stores(false).unwrap();\n    assert_eq!(s, all_stores);\n\n    // Add tombstone store.\n    let mut store99 = metapb::Store::default();\n    store99.set_id(99);\n    store99.set_state(metapb::StoreState::Tombstone);\n    server.default_handler().add_store(store99.clone());\n\n    let r = block_on(client.get_store_async(99));\n    assert_eq!(r.unwrap_err().error_code(), error_code::pd::STORE_TOMBSTONE);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::is_cluster_bootstrapped", "code": "fn is_cluster_bootstrapped(&self) -> Result<bool> {\n        let _timer = PD_REQUEST_HISTOGRAM_VEC\n            .is_cluster_bootstrapped\n            .start_coarse_timer();\n\n        let mut req = pdpb::IsBootstrappedRequest::default();\n        req.set_header(self.header());\n\n        let resp = sync_request(&self.pd_client, LEADER_CHANGE_RETRY, |client, option| {\n            client.is_bootstrapped_opt(&req, option)\n        })?;\n        check_resp_header(resp.get_header())?;\n\n        Ok(resp.get_bootstrapped())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_reboot", "test": "fn test_reboot() {\n    let eps_count = 1;\n    let server = MockServer::with_case(eps_count, Arc::new(AlreadyBootstrapped));\n    let eps = server.bind_addrs();\n    let client = new_client(eps, None);\n\n    assert!(!client.is_cluster_bootstrapped().unwrap());\n\n    match client.bootstrap_cluster(metapb::Store::default(), metapb::Region::default()) {\n        Err(PdError::ClusterBootstrapped(_)) => (),\n        _ => {\n            panic!(\"failed, should return ClusterBootstrapped\");\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/client.rs::is_cluster_bootstrapped", "code": "fn is_cluster_bootstrapped(&self) -> Result<bool> {\n        let _timer = PD_REQUEST_HISTOGRAM_VEC\n            .is_cluster_bootstrapped\n            .start_coarse_timer();\n\n        let mut req = pdpb::IsBootstrappedRequest::default();\n        req.set_header(self.header());\n\n        let resp = sync_request(&self.pd_client, LEADER_CHANGE_RETRY, |client, option| {\n            client.is_bootstrapped_opt(&req, option)\n        })?;\n        check_resp_header(resp.get_header())?;\n\n        Ok(resp.get_bootstrapped())\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_incompatible_version", "test": "fn test_incompatible_version() {\n    let incompatible = Arc::new(Incompatible);\n    let server = MockServer::with_case(1, incompatible);\n    let eps = server.bind_addrs();\n\n    let client = new_client(eps, None);\n\n    let resp = block_on(client.ask_batch_split(metapb::Region::default(), 2));\n    assert_eq!(\n        resp.unwrap_err().to_string(),\n        PdError::Incompatible.to_string()\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/datum.rs::to_string", "code": "pub fn to_string(&self) -> Result<String> {\n        let s = match *self {\n            Datum::I64(i) => format!(\"{}\", i),\n            Datum::U64(u) => format!(\"{}\", u),\n            Datum::F64(f) => format!(\"{}\", f),\n            Datum::Bytes(ref bs) => String::from_utf8(bs.to_vec())?,\n            Datum::Time(t) => format!(\"{}\", t),\n            Datum::Dur(ref d) => format!(\"{}\", d),\n            Datum::Dec(ref d) => format!(\"{}\", d),\n            Datum::Json(ref d) => d.to_string(),\n            Datum::Enum(ref e) => e.to_string(),\n            Datum::Set(ref s) => s.to_string(),\n            ref d => return Err(invalid_type!(\"can't convert {} to string\", d)),\n        };\n        Ok(s)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_change_leader_async", "test": "fn test_change_leader_async() {\n    let eps_count = 3;\n    let server = MockServer::with_case(eps_count, Arc::new(LeaderChange::new()));\n    let eps = server.bind_addrs();\n\n    let counter = Arc::new(AtomicUsize::new(0));\n    let client = new_client(eps, None);\n    let counter1 = Arc::clone(&counter);\n    client.handle_reconnect(move || {\n        counter1.fetch_add(1, Ordering::SeqCst);\n    });\n    let leader = client.get_leader();\n\n    for _ in 0..5 {\n        let region = block_on(client.get_region_by_id(1));\n        region.ok();\n\n        let new = client.get_leader();\n        if new != leader {\n            assert!(counter.load(Ordering::SeqCst) >= 1);\n            return;\n        }\n        thread::sleep(LeaderChange::get_leader_interval());\n    }\n\n    panic!(\"failed, leader should changed\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_pd_client_heartbeat_send_failed", "test": "fn test_pd_client_heartbeat_send_failed() {\n    let pd_client_send_fail_fp = \"region_heartbeat_send_failed\";\n    fail::cfg(pd_client_send_fail_fp, \"return()\").unwrap();\n    let server = MockServer::with_case(1, Arc::new(AlreadyBootstrapped));\n    let eps = server.bind_addrs();\n\n    let client = new_client(eps, None);\n    let poller = Builder::new_multi_thread()\n        .thread_name(thd_name!(\"poller\"))\n        .worker_threads(1)\n        .build()\n        .unwrap();\n    let (tx, rx) = mpsc::channel();\n    let f =\n        client.handle_region_heartbeat_response(1, move |resp| tx.send(resp).unwrap_or_default());\n    poller.spawn(f);\n\n    let heartbeat_send_fail = |ok| {\n        let mut region = metapb::Region::default();\n        region.set_id(1);\n        poller.spawn(client.region_heartbeat(\n            store::RAFT_INIT_LOG_TERM,\n            region,\n            metapb::Peer::default(),\n            RegionStat::default(),\n            None,\n        ));\n        let rsp = rx.recv_timeout(Duration::from_millis(100));\n        if ok {\n            assert!(rsp.is_ok());\n            assert_eq!(rsp.unwrap().get_region_id(), 1);\n        } else {\n            rsp.unwrap_err();\n        }\n\n        let region = block_on(client.get_region_by_id(1));\n        if ok {\n            assert!(region.is_ok());\n            let r = region.unwrap();\n            assert!(r.is_some());\n            assert_eq!(1, r.unwrap().get_id());\n        } else {\n            region.unwrap_err();\n        }\n    };\n    // send fail if network is block.\n    heartbeat_send_fail(false);\n    fail::remove(pd_client_send_fail_fp);\n    // send success after network recovered.\n    heartbeat_send_fail(true);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_pd/src/mocker/retry.rs::is_ok", "code": "fn is_ok(&self) -> bool {\n        let count = self.count.fetch_add(1, Ordering::SeqCst);\n        if count != 0 && count % self.retry == 0 {\n            // it's ok.\n            return true;\n        }\n        // let's sleep awhile, so that client will update its connection.\n        thread::sleep(REQUEST_RECONNECT_INTERVAL);\n        false\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_periodical_update", "test": "fn test_periodical_update() {\n    let eps_count = 3;\n    let server = MockServer::with_case(eps_count, Arc::new(LeaderChange::new()));\n    let eps = server.bind_addrs();\n\n    let counter = Arc::new(AtomicUsize::new(0));\n    let client = new_client_with_update_interval(eps, None, ReadableDuration::secs(3));\n    let counter1 = Arc::clone(&counter);\n    client.handle_reconnect(move || {\n        counter1.fetch_add(1, Ordering::SeqCst);\n    });\n    let leader = client.get_leader();\n\n    for _ in 0..5 {\n        let new = client.get_leader();\n        if new != leader {\n            assert!(counter.load(Ordering::SeqCst) >= 1);\n            return;\n        }\n        thread::sleep(LeaderChange::get_leader_interval());\n    }\n\n    panic!(\"failed, leader should changed\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/pd/test_rpc_client_legacy.rs::test_cluster_version", "test": "fn test_cluster_version() {\n    let server = MockServer::<Service>::new(3);\n    let eps = server.bind_addrs();\n\n    let feature_a = Feature::require(0, 0, 1);\n    let feature_b = Feature::require(5, 0, 0);\n    let feature_c = Feature::require(5, 0, 1);\n\n    let client = new_client(eps, None);\n    let feature_gate = client.feature_gate();\n    assert!(!feature_gate.can_enable(feature_a));\n\n    let emit_heartbeat = || {\n        let req = pdpb::StoreStats::default();\n        block_on(client.store_heartbeat(req, /* store_report= */ None, None)).unwrap();\n    };\n\n    let set_cluster_version = |version: &str| {\n        let h = server.default_handler();\n        h.set_cluster_version(version.to_owned());\n    };\n\n    // Empty version string will be treated as invalid.\n    emit_heartbeat();\n    assert!(!feature_gate.can_enable(feature_a));\n\n    // Explicitly invalid version string.\n    set_cluster_version(\"invalid-version\");\n    emit_heartbeat();\n    assert!(!feature_gate.can_enable(feature_a));\n\n    // Correct version string.\n    set_cluster_version(\"5.0.0\");\n    emit_heartbeat();\n    assert!(feature_gate.can_enable(feature_a));\n    assert!(feature_gate.can_enable(feature_b));\n    assert!(!feature_gate.can_enable(feature_c));\n\n    // Version can't go backwards.\n    set_cluster_version(\"4.99\");\n    emit_heartbeat();\n    assert!(feature_gate.can_enable(feature_b));\n    assert!(!feature_gate.can_enable(feature_c));\n\n    // After reconnect the version should be still accessable.\n    // The GLOBAL_RECONNECT_INTERVAL is 0.1s so sleeps 0.2s here.\n    thread::sleep(Duration::from_millis(200));\n    client.reconnect().unwrap();\n    assert!(feature_gate.can_enable(feature_b));\n    assert!(!feature_gate.can_enable(feature_c));\n\n    // Version can go forwards.\n    set_cluster_version(\"5.0.1\");\n    emit_heartbeat();\n    assert!(feature_gate.can_enable(feature_c));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/pd_client/src/feature_gate.rs::can_enable", "code": "pub fn can_enable(&self, feature: Feature) -> bool {\n        self.version.load(Ordering::Relaxed) >= feature.ver\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_bootstrap.rs::test_node_bootstrap_with_prepared_data", "test": "fn test_node_bootstrap_with_prepared_data() {\n    // create a node\n    let pd_client = Arc::new(TestPdClient::new(0, false));\n    let cfg = new_tikv_config(0);\n\n    let (_, system) = fsm::create_raft_batch_system(&cfg.raft_store, &None);\n    let simulate_trans = SimulateTransport::new(ChannelTransport::new());\n    let tmp_path = Builder::new().prefix(\"test_cluster\").tempdir().unwrap();\n    let engine =\n        engine_rocks::util::new_engine(tmp_path.path().to_str().unwrap(), ALL_CFS).unwrap();\n    let tmp_path_raft = tmp_path.path().join(Path::new(\"raft\"));\n    let raft_engine =\n        engine_rocks::util::new_engine(tmp_path_raft.to_str().unwrap(), &[CF_DEFAULT]).unwrap();\n    let engines = Engines::new(engine.clone(), raft_engine);\n    let tmp_mgr = Builder::new().prefix(\"test_cluster\").tempdir().unwrap();\n    let bg_worker = WorkerBuilder::new(\"background\").thread_count(2).create();\n    let mut node = Node::new(\n        system,\n        &cfg.server,\n        Arc::new(VersionTrack::new(cfg.raft_store.clone())),\n        cfg.storage.api_version(),\n        Arc::clone(&pd_client),\n        Arc::default(),\n        bg_worker,\n        None,\n        None,\n    );\n    let snap_mgr = SnapManager::new(tmp_mgr.path().to_str().unwrap());\n    let pd_worker = LazyWorker::new(\"test-pd-worker\");\n\n    // assume there is a node has bootstrapped the cluster and add region in pd\n    // successfully\n    bootstrap_with_first_region(Arc::clone(&pd_client)).unwrap();\n\n    // now another node at same time begin bootstrap node, but panic after prepared\n    // bootstrap now rocksDB must have some prepare data\n    bootstrap_store(&engines, 0, 1).unwrap();\n    let region = node.prepare_bootstrap_cluster(&engines, 1).unwrap();\n    assert!(\n        engine\n            .get_msg::<metapb::Region>(keys::PREPARE_BOOTSTRAP_KEY)\n            .unwrap()\n            .is_some()\n    );\n    let region_state_key = keys::region_state_key(region.get_id());\n    assert!(\n        engine\n            .get_msg_cf::<RegionLocalState>(CF_RAFT, &region_state_key)\n            .unwrap()\n            .is_some()\n    );\n\n    // Create coprocessor.\n    let coprocessor_host = CoprocessorHost::new(node.get_router(), cfg.coprocessor);\n\n    let importer = {\n        let dir = tmp_path.path().join(\"import-sst\");\n        Arc::new(SstImporter::new(&cfg.import, dir, None, cfg.storage.api_version()).unwrap())\n    };\n    let (split_check_scheduler, _) = dummy_scheduler();\n\n    node.try_bootstrap_store(engines.clone()).unwrap();\n    // try to restart this node, will clear the prepare data\n    node.start(\n        engines,\n        simulate_trans,\n        snap_mgr,\n        pd_worker,\n        Arc::new(Mutex::new(StoreMeta::new(0))),\n        coprocessor_host,\n        importer,\n        split_check_scheduler,\n        AutoSplitController::default(),\n        ConcurrencyManager::new(1.into()),\n        CollectorRegHandle::new_for_test(),\n        None,\n        Arc::new(AtomicU64::new(0)),\n    )\n    .unwrap();\n    assert!(\n        engine\n            .get_msg::<metapb::Region>(keys::PREPARE_BOOTSTRAP_KEY)\n            .unwrap()\n            .is_none()\n    );\n    assert!(\n        engine\n            .get_msg_cf::<RegionLocalState>(CF_RAFT, &region_state_key)\n            .unwrap()\n            .is_none()\n    );\n    assert_eq!(pd_client.get_regions_number() as u32, 1);\n    node.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_hibernate.rs::test_proposal_prevent_sleep", "test": "fn test_proposal_prevent_sleep() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_hibernate(&mut cluster.cfg);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 2\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1).direction(Direction::Send),\n    ));\n    let region = block_on(cluster.pd_client.get_region_by_id(1))\n        .unwrap()\n        .unwrap();\n\n    let put = new_put_cmd(b\"k2\", b\"v2\");\n    let mut req = new_request(1, region.get_region_epoch().clone(), vec![put], true);\n    req.mut_header().set_peer(new_peer(1, 1));\n    // ignore error, we just want to send this command to peer (1, 1),\n    // and the command can't be executed because we have only one peer,\n    // so here will return timeout error, we should ignore it.\n    let _ = cluster.call_command(req, Duration::from_millis(10));\n    cluster.clear_send_filters();\n    must_get_equal(&cluster.get_engine(3), b\"k2\", b\"v2\");\n    assert_eq!(cluster.leader_of_region(1), Some(new_peer(1, 1)));\n\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 2\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1).direction(Direction::Send),\n    ));\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_read_index_cmd()],\n        true,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    // send to peer 2\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    thread::sleep(Duration::from_millis(10));\n    cluster.clear_send_filters();\n    let resp = rx.recv_timeout(Duration::from_secs(5)).unwrap();\n    assert!(\n        !resp.get_header().has_error(),\n        \"{:?}\",\n        resp.get_header().get_error()\n    );\n\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 2\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1).direction(Direction::Send),\n    ));\n    let conf_change = new_change_peer_request(ConfChangeType::RemoveNode, new_peer(3, 3));\n    let mut admin_req = new_admin_request(1, region.get_region_epoch(), conf_change);\n    admin_req.mut_header().set_peer(new_peer(1, 1));\n    let (cb, _rx) = make_cb(&admin_req);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, admin_req, cb)\n        .unwrap();\n    thread::sleep(Duration::from_millis(10));\n    cluster.clear_send_filters();\n    cluster.pd_client.must_none_peer(1, new_peer(3, 3));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::leader_of_region", "code": "pub fn leader_of_region(&mut self, region_id: u64) -> Option<metapb::Peer> {\n        let timer = Instant::now_coarse();\n        let timeout = Duration::from_secs(5);\n        let mut store_ids = None;\n        while timer.saturating_elapsed() < timeout {\n            match self.voter_store_ids_of_region(region_id) {\n                None => thread::sleep(Duration::from_millis(10)),\n                Some(ids) => {\n                    store_ids = Some(ids);\n                    break;\n                }\n            }\n        }\n        let store_ids = store_ids?;\n        if let Some(l) = self.leaders.get(&region_id) {\n            // leader may be stopped in some tests.\n            if self.valid_leader_id(region_id, l.get_store_id()) {\n                return Some(l.clone());\n            }\n        }\n        self.reset_leader_of_region(region_id);\n        let mut leader = None;\n        let mut leaders = HashMap::default();\n\n        let node_ids = self.sim.rl().get_node_ids();\n        // For some tests, we stop the node but pd still has this information,\n        // and we must skip this.\n        let alive_store_ids: Vec<_> = store_ids\n            .iter()\n            .filter(|id| node_ids.contains(id))\n            .cloned()\n            .collect();\n        while timer.saturating_elapsed() < timeout {\n            for store_id in &alive_store_ids {\n                let l = match self.query_leader(*store_id, region_id, Duration::from_secs(1)) {\n                    None => continue,\n                    Some(l) => l,\n                };\n                leaders\n                    .entry(l.get_id())\n                    .or_insert((l, vec![]))\n                    .1\n                    .push(*store_id);\n            }\n            if let Some((_, (l, c))) = leaders.iter().max_by_key(|(_, (_, c))| c.len()) {\n                if c.contains(&l.get_store_id()) {\n                    leader = Some(l.clone());\n                    // Technically, correct calculation should use two quorum when in joint\n                    // state. Here just for simplicity.\n                    if c.len() > store_ids.len() / 2 {\n                        break;\n                    }\n                }\n            }\n            debug!(\"failed to detect leaders\"; \"leaders\" => ?leaders, \"store_ids\" => ?store_ids);\n            sleep_ms(10);\n            leaders.clear();\n        }\n\n        if let Some(l) = leader {\n            self.leaders.insert(region_id, l);\n        }\n\n        self.leaders.get(&region_id).cloned()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_hibernate.rs::test_transfer_leader_delay", "test": "fn test_transfer_leader_delay() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_hibernate(&mut cluster.cfg);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    let messages = Arc::new(Mutex::new(vec![]));\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 3)\n            .direction(Direction::Send)\n            .msg_type(MessageType::MsgTransferLeader)\n            .reserve_dropped(messages.clone()),\n    ));\n\n    cluster.transfer_leader(1, new_peer(3, 3));\n    let timer = Instant::now();\n    while timer.saturating_elapsed() < Duration::from_secs(3) && messages.lock().unwrap().is_empty()\n    {\n        thread::sleep(Duration::from_millis(10));\n    }\n    assert_eq!(messages.lock().unwrap().len(), 1);\n\n    // Wait till leader peer goes to sleep again.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 2\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n\n    cluster.clear_send_filters();\n    cluster.add_send_filter(CloneFilterFactory(DropMessageFilter::new(Arc::new(|m| {\n        m.get_message().get_msg_type() != MessageType::MsgTimeoutNow\n    }))));\n    let router = cluster.sim.wl().get_router(1).unwrap();\n    router\n        .send_raft_message(messages.lock().unwrap().pop().unwrap())\n        .unwrap();\n\n    let timer = Instant::now();\n    while timer.saturating_elapsed() < Duration::from_secs(3) {\n        let resp = cluster.request(\n            b\"k2\",\n            vec![new_put_cmd(b\"k2\", b\"v2\")],\n            false,\n            Duration::from_secs(5),\n        );\n        let header = resp.get_header();\n        if !header.has_error() {\n            return;\n        }\n        if !header\n            .get_error()\n            .get_message()\n            .contains(\"proposal dropped\")\n        {\n            panic!(\"response {:?} has error\", resp);\n        }\n        thread::sleep(Duration::from_millis(10));\n    }\n    panic!(\"failed to request after 3 seconds\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/txn/store.rs::len", "code": "pub fn len(&self) -> usize {\n        self.entries.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_hibernate.rs::test_inconsistent_configuration", "test": "fn test_inconsistent_configuration() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_hibernate(&mut cluster.cfg);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 3\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n\n    // Ensure leader can sleep if all nodes enable hibernate region.\n    let awakened = Arc::new(AtomicBool::new(false));\n    let filter = Arc::new(AtomicBool::new(true));\n    let a = awakened.clone();\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1)\n            .direction(Direction::Send)\n            .set_msg_callback(Arc::new(move |_| {\n                a.store(true, Ordering::SeqCst);\n            }))\n            .when(filter.clone()),\n    ));\n    thread::sleep(cluster.cfg.raft_store.raft_heartbeat_interval() * 2);\n    assert!(!awakened.load(Ordering::SeqCst));\n\n    // Simulate rolling disable hibernate region in followers\n    filter.store(false, Ordering::SeqCst);\n    cluster.cfg.raft_store.hibernate_regions = false;\n    cluster.stop_node(3);\n    cluster.run_node(3).unwrap();\n    cluster.must_put(b\"k2\", b\"v2\");\n    // In case leader changes.\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    must_get_equal(&cluster.get_engine(3), b\"k2\", b\"v2\");\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 3\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n    awakened.store(false, Ordering::SeqCst);\n    filter.store(true, Ordering::SeqCst);\n    thread::sleep(cluster.cfg.raft_store.raft_heartbeat_interval() * 2);\n    // Leader should keep awake as peer 3 won't agree to sleep.\n    assert!(awakened.load(Ordering::SeqCst));\n    cluster.reset_leader_of_region(1);\n    assert_eq!(cluster.leader_of_region(1), Some(new_peer(1, 1)));\n\n    // Simulate rolling disable hibernate region in leader\n    cluster.clear_send_filters();\n    cluster.must_transfer_leader(1, new_peer(3, 3));\n    cluster.must_put(b\"k3\", b\"v3\");\n    must_get_equal(&cluster.get_engine(1), b\"k3\", b\"v3\");\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 3\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n    awakened.store(false, Ordering::SeqCst);\n    let a = awakened.clone();\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 3)\n            .direction(Direction::Send)\n            .set_msg_callback(Arc::new(move |_| {\n                a.store(true, Ordering::SeqCst);\n            })),\n    ));\n    thread::sleep(cluster.cfg.raft_store.raft_heartbeat_interval() * 2);\n    // Leader should keep awake as hibernate region is disabled.\n    assert!(awakened.load(Ordering::SeqCst));\n    cluster.reset_leader_of_region(1);\n    assert_eq!(cluster.leader_of_region(1), Some(new_peer(3, 3)));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_hibernate.rs::test_hibernate_feature_gate", "test": "fn test_hibernate_feature_gate() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.pd_client.reset_version(\"4.0.0\");\n    configure_for_hibernate(&mut cluster.cfg);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Wait for hibernation check.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 3\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n\n    // Ensure leader won't sleep if cluster version is small.\n    let awakened = Arc::new(AtomicBool::new(false));\n    let filter = Arc::new(AtomicBool::new(true));\n    let a = awakened.clone();\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1)\n            .direction(Direction::Send)\n            .set_msg_callback(Arc::new(move |_| {\n                a.store(true, Ordering::SeqCst);\n            }))\n            .when(filter.clone()),\n    ));\n    thread::sleep(cluster.cfg.raft_store.raft_heartbeat_interval() * 2);\n    assert!(awakened.load(Ordering::SeqCst));\n\n    // Simulating all binaries are upgraded to 5.0.0.\n    cluster.pd_client.reset_version(\"5.0.0\");\n    filter.store(false, Ordering::SeqCst);\n    // Wait till leader peer goes to sleep.\n    thread::sleep(\n        cluster.cfg.raft_store.raft_base_tick_interval.0\n            * 3\n            * cluster.cfg.raft_store.raft_election_timeout_ticks as u32,\n    );\n    awakened.store(false, Ordering::SeqCst);\n    filter.store(true, Ordering::SeqCst);\n    thread::sleep(cluster.cfg.raft_store.raft_heartbeat_interval() * 2);\n    // Leader can go to sleep as version requirement is met.\n    assert!(!awakened.load(Ordering::SeqCst));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_joint_consensus.rs::test_joint_consensus_conf_change", "test": "fn test_joint_consensus_conf_change() {\n    let mut cluster = new_node_cluster(0, 4);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    assert_eq!(cluster.get(b\"k1\"), Some(b\"v1\".to_vec()));\n\n    // add multiple nodes\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::AddNode, new_peer(2, 2)),\n            (ConfChangeType::AddNode, new_peer(3, 3)),\n            (ConfChangeType::AddLearnerNode, new_learner_peer(4, 4)),\n        ],\n    );\n    pd_client.must_leave_joint(region_id);\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(4), b\"k1\", b\"v1\");\n\n    // remove multiple nodes\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::AddLearnerNode, new_learner_peer(3, 3)),\n            (ConfChangeType::RemoveNode, new_learner_peer(4, 4)),\n        ],\n    );\n    pd_client.must_leave_joint(region_id);\n    assert_eq!(\n        find_peer(&pd_client.get_region(b\"\").unwrap(), 3).unwrap(),\n        &new_learner_peer(3, 3)\n    );\n    must_get_none(&cluster.get_engine(4), b\"k1\");\n\n    // replace node\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::RemoveNode, new_learner_peer(3, 3)),\n            (ConfChangeType::AddNode, new_peer(4, 5)),\n        ],\n    );\n    pd_client.must_leave_joint(region_id);\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    must_get_equal(&cluster.get_engine(4), b\"k1\", b\"v1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::get", "code": "pub fn get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, false)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_joint_consensus.rs::test_enter_joint_state", "test": "fn test_enter_joint_state() {\n    let mut cluster = new_node_cluster(0, 4);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    // normal confchange request will not enter joint state\n    pd_client.must_add_peer(region_id, new_peer(2, 2));\n    assert!(!pd_client.is_in_joint(region_id));\n    pd_client.must_add_peer(region_id, new_peer(3, 3));\n    assert!(!pd_client.is_in_joint(region_id));\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // confchange_v2 request with one conchange request will not enter joint state\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![(ConfChangeType::RemoveNode, new_peer(3, 3))],\n    );\n    assert!(!pd_client.is_in_joint(region_id));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    pd_client.must_joint_confchange(region_id, vec![(ConfChangeType::AddNode, new_peer(3, 3))]);\n    assert!(!pd_client.is_in_joint(region_id));\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Enter joint\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::AddLearnerNode, new_learner_peer(3, 3)),\n            (ConfChangeType::AddNode, new_peer(4, 4)),\n        ],\n    );\n    assert!(pd_client.is_in_joint(region_id));\n\n    // In joint state any confchange request besides leave joint request\n    // will be rejected\n    let resp = call_conf_change(\n        &mut cluster,\n        region_id,\n        ConfChangeType::RemoveNode,\n        new_learner_peer(3, 3),\n    )\n    .unwrap();\n    must_contains_error(&resp, \"in joint\");\n\n    let resp = call_conf_change_v2(\n        &mut cluster,\n        region_id,\n        vec![change_peer(\n            ConfChangeType::RemoveNode,\n            new_learner_peer(3, 3),\n        )],\n    )\n    .unwrap();\n    must_contains_error(&resp, \"in joint\");\n\n    // Leave joint\n    pd_client.must_leave_joint(region_id);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_pd_client/src/pd.rs::is_in_joint", "code": "pub fn is_in_joint(&self, region_id: u64) -> bool {\n        let region = block_on(self.get_region_by_id(region_id))\n            .unwrap()\n            .expect(\"region not exist\");\n        region.get_peers().iter().any(|p| {\n            p.get_role() == PeerRole::IncomingVoter || p.get_role() == PeerRole::DemotingVoter\n        })\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_joint_consensus.rs::test_request_in_joint_state", "test": "fn test_request_in_joint_state() {\n    let mut cluster = new_node_cluster(0, 3);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    pd_client.must_add_peer(region_id, new_peer(2, 2));\n    pd_client.must_add_peer(region_id, new_learner_peer(3, 3));\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Enter joint, now we have C_old(1, 2) and C_new(1, 3)\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::AddLearnerNode, new_learner_peer(2, 2)),\n            (ConfChangeType::AddNode, new_peer(3, 3)),\n        ],\n    );\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    // Both new and old configuation have the newest log\n    must_get_equal(&cluster.get_engine(2), b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(3), b\"k2\", b\"v2\");\n\n    let region = cluster.get_region(b\"k1\");\n\n    // Isolated peer 2, so the old configuation can't reach quorum\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n    let mut rx = cluster\n        .async_request(put_request(&region, 1, b\"k3\", b\"v3\"))\n        .unwrap();\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n    cluster.clear_send_filters();\n\n    // Isolated peer 3, so the new configuation can't reach quorum\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    let mut rx = cluster\n        .async_request(put_request(&region, 1, b\"k4\", b\"v4\"))\n        .unwrap();\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n    cluster.clear_send_filters();\n\n    // Leave joint\n    pd_client.must_leave_joint(region_id);\n\n    // Isolated peer 2, but it is not in quorum any more\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n    cluster.must_put(b\"k5\", b\"v5\");\n    must_get_equal(&cluster.get_engine(3), b\"k5\", b\"v5\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_joint_consensus.rs::test_invalid_confchange_request", "test": "fn test_invalid_confchange_request() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.allow_remove_leader = false;\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n    let region = cluster.get_region(b\"\");\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    pd_client.must_add_peer(region_id, new_peer(2, 2));\n    pd_client.must_add_peer(region_id, new_learner_peer(3, 3));\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Can not remove voter directly in joint confchange request\n    let resp = call_conf_change_v2(\n        &mut cluster,\n        region_id,\n        vec![\n            change_peer(ConfChangeType::RemoveNode, new_peer(2, 2)),\n            change_peer(ConfChangeType::AddLearnerNode, new_learner_peer(4, 4)),\n        ],\n    )\n    .unwrap();\n    must_contains_error(&resp, \"can not remove voter\");\n\n    // Can not have multiple commands for the same peer\n    let resp = call_conf_change_v2(\n        &mut cluster,\n        region_id,\n        vec![\n            change_peer(ConfChangeType::AddLearnerNode, new_learner_peer(2, 2)),\n            change_peer(ConfChangeType::RemoveNode, new_learner_peer(2, 2)),\n        ],\n    )\n    .unwrap();\n    must_contains_error(&resp, \"multiple commands for the same peer\");\n\n    // Can not have multiple changes that only effect learner\n    let resp = call_conf_change_v2(\n        &mut cluster,\n        region_id,\n        vec![\n            change_peer(ConfChangeType::RemoveNode, new_learner_peer(3, 3)),\n            change_peer(ConfChangeType::AddLearnerNode, new_learner_peer(4, 4)),\n        ],\n    )\n    .unwrap();\n    must_contains_error(&resp, \"multiple changes that only effect learner\");\n\n    // Can not demote leader with simple confchange\n    let resp = call_conf_change_v2(\n        &mut cluster,\n        region_id,\n        vec![change_peer(\n            ConfChangeType::AddLearnerNode,\n            new_learner_peer(1, 1),\n        )],\n    )\n    .unwrap();\n    must_contains_error(&resp, \"ignore remove leader or demote leader\");\n\n    let resp = call_conf_change(\n        &mut cluster,\n        region_id,\n        ConfChangeType::AddLearnerNode,\n        new_learner_peer(1, 1),\n    )\n    .unwrap();\n    must_contains_error(&resp, \"ignore remove leader or demote leader\");\n\n    // Can not leave a non-joint config\n    let resp = leave_joint(&mut cluster, region_id).unwrap();\n    must_contains_error(&resp, \"leave a non-joint config\");\n\n    // Split region\n    cluster.must_split(&region, b\"k3\");\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k5\").unwrap();\n    assert_eq!(region_id, right.get_id());\n    // Enter joint\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::AddLearnerNode, new_learner_peer(2, 2)),\n            (ConfChangeType::AddNode, new_peer(3, 3)),\n        ],\n    );\n    assert!(pd_client.is_in_joint(region_id));\n\n    // Can not merge region while in jonit state\n    let resp = cluster.try_merge(right.get_id(), left.get_id());\n    must_contains_error(&resp, \"in joint state, can not propose merge command\");\n\n    // Can not leave joint if which will demote leader\n    cluster.must_transfer_leader(region_id, new_peer(2, 2));\n    let resp = leave_joint(&mut cluster, region_id).unwrap();\n    must_contains_error(&resp, \"ignore leave joint command that demoting leader\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_id", "code": "pub fn get_id(&self) -> DownstreamId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_joint_consensus.rs::test_restart_in_joint_state", "test": "fn test_restart_in_joint_state() {\n    let mut cluster = new_node_cluster(0, 3);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    pd_client.must_add_peer(region_id, new_peer(2, 2));\n    pd_client.must_add_peer(region_id, new_learner_peer(3, 3));\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n\n    // Enter joint\n    pd_client.must_joint_confchange(\n        region_id,\n        vec![\n            (ConfChangeType::AddLearnerNode, new_learner_peer(2, 2)),\n            (ConfChangeType::AddNode, new_peer(3, 3)),\n        ],\n    );\n    assert!(pd_client.is_in_joint(region_id));\n\n    cluster.stop_node(1);\n    sleep_ms(50);\n\n    cluster.run_node(1).unwrap();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    // Still in joint state\n    assert!(pd_client.is_in_joint(region_id));\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(2), b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(3), b\"k2\", b\"v2\");\n\n    // Leave joint\n    pd_client.must_leave_joint(region_id);\n\n    // Joint confchange finished\n    let region = cluster.get_region(b\"k2\");\n    must_has_peer(&region, 1, PeerRole::Voter);\n    must_has_peer(&region, 2, PeerRole::Learner);\n    must_has_peer(&region, 3, PeerRole::Voter);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_pd_client/src/pd.rs::is_in_joint", "code": "pub fn is_in_joint(&self, region_id: u64) -> bool {\n        let region = block_on(self.get_region_by_id(region_id))\n            .unwrap()\n            .expect(\"region not exist\");\n        region.get_peers().iter().any(|p| {\n            p.get_role() == PeerRole::IncomingVoter || p.get_role() == PeerRole::DemotingVoter\n        })\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_lease_read.rs::test_node_callback_when_destroyed", "test": "fn test_node_callback_when_destroyed() {\n    let count = 3;\n    let mut cluster = new_node_cluster(0, count);\n    // Increase the election tick to make this test case running reliably.\n    configure_for_lease_read(&mut cluster.cfg, None, Some(50));\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n    let leader = cluster.leader_of_region(1).unwrap();\n    let cc = new_change_peer_request(ConfChangeType::RemoveNode, leader.clone());\n    let epoch = cluster.get_region_epoch(1);\n    let req = new_admin_request(1, &epoch, cc);\n    // so the leader can't commit the conf change yet.\n    let block = Arc::new(AtomicBool::new(true));\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, leader.get_store_id())\n            .msg_type(MessageType::MsgAppendResponse)\n            .direction(Direction::Recv)\n            .when(Arc::clone(&block)),\n    ));\n    let mut filter = LeaseReadFilter::default();\n    filter.take = true;\n    // so the leader can't perform read index.\n    cluster.add_send_filter(CloneFilterFactory(filter.clone()));\n    // it always timeout, no need to wait.\n    let _ = cluster.call_command_on_leader(req, Duration::from_millis(500));\n\n    // To make sure `get` is handled before destroy leader, we must issue\n    // `get` then unblock append responses.\n    let leader_node_id = leader.get_store_id();\n    let get = new_get_cmd(b\"k1\");\n    let mut req = new_request(1, epoch, vec![get], true);\n    req.mut_header().set_peer(leader);\n    let (cb, mut rx) = make_cb(&req);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(leader_node_id, req, cb)\n        .unwrap();\n    // Unblock append responses after we issue the req.\n    block.store(false, Ordering::SeqCst);\n    let resp = rx.recv_timeout(Duration::from_secs(3)).unwrap();\n\n    assert!(\n        !filter.ctx.rl().is_empty(),\n        \"read index should be performed\"\n    );\n    assert!(\n        resp.get_header().get_error().has_region_not_found(),\n        \"{:?}\",\n        resp\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/server/gc_worker/compaction_filter.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        match &self.batch {\n            Either::Left(batch) => batch.is_empty(),\n            Either::Right(keys) => keys.is_empty(),\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_lease_read.rs::test_lease_read_callback_destroy", "test": "fn test_lease_read_callback_destroy() {\n    // Only server cluster can fake sending message successfully in raftstore layer.\n    let mut cluster = new_server_cluster(0, 3);\n    // Increase the Raft tick interval to make this test case running reliably.\n    let election_timeout = configure_for_lease_read(&mut cluster.cfg, Some(50), None);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    // Isolate the target peer to make transfer leader fail.\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    cluster.transfer_leader(1, new_peer(3, 3));\n    thread::sleep(election_timeout * 2);\n    // Trigger ReadIndex on the leader.\n    assert_eq!(cluster.must_get(b\"k1\"), Some(b\"v1\".to_vec()));\n    cluster.must_put(b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_lease_read.rs::test_infinite_lease", "test": "fn test_infinite_lease() {\n    let mut cluster = new_node_cluster(0, 3);\n    // Avoid triggering the log compaction in this test case.\n    cluster.cfg.raft_store.raft_log_gc_threshold = 100;\n    // Increase the Raft tick interval to make this test case running reliably.\n    // Use large election timeout to make leadership stable.\n    configure_for_lease_read(&mut cluster.cfg, Some(50), Some(10_000));\n    // Override max leader lease to 2 seconds.\n    let max_lease = Duration::from_secs(2);\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration(max_lease);\n\n    let peer = new_peer(1, 1);\n    cluster.pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    let key = b\"k\";\n    cluster.must_put(key, b\"v0\");\n    for id in 2..=cluster.engines.len() as u64 {\n        cluster.pd_client.must_add_peer(region_id, new_peer(id, id));\n        must_get_equal(&cluster.get_engine(id), key, b\"v0\");\n    }\n\n    // Force `peer` to become leader.\n    let region = cluster.get_region(key);\n    let region_id = region.get_id();\n    cluster.must_transfer_leader(region_id, peer.clone());\n\n    let detector = LeaseReadFilter::default();\n    cluster.add_send_filter(CloneFilterFactory(detector.clone()));\n\n    // Issue a read request and check the value on response.\n    must_read_on_peer(&mut cluster, peer.clone(), region.clone(), key, b\"v0\");\n    assert_eq!(detector.ctx.rl().len(), 0);\n\n    // Wait for the leader lease to expire.\n    thread::sleep(max_lease);\n\n    // Check if renew-lease-tick proposed a read index and renewed the leader lease.\n    assert_eq!(cluster.leader_of_region(region_id), Some(peer.clone()));\n    assert_eq!(detector.ctx.rl().len(), 1);\n    // Issue a read request to verify the lease.\n    must_read_on_peer(&mut cluster, peer.clone(), region, key, b\"v0\");\n    assert_eq!(cluster.leader_of_region(region_id), Some(peer));\n    assert_eq!(detector.ctx.rl().len(), 1);\n\n    // renew-lease-tick shouldn't propose any request if the leader lease is not\n    // expired.\n    for _ in 0..4 {\n        cluster.must_put(key, b\"v0\");\n        thread::sleep(max_lease / 4);\n    }\n    assert_eq!(detector.ctx.rl().len(), 1);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/vector.rs::len", "code": "pub fn len(&self) -> usize {\n        match_template_evaltype! {\n            TT, match self {\n                VectorValue::TT(v) => v.len(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_lease_read.rs::test_node_local_read_renew_lease", "test": "fn test_node_local_read_renew_lease() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(500);\n    let (base_tick_ms, election_ticks) = (50, 10);\n    configure_for_lease_read(&mut cluster.cfg, Some(50), Some(10));\n    cluster.pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    let key = b\"k\";\n    cluster.must_put(key, b\"v0\");\n    for id in 2..=3 {\n        cluster.pd_client.must_add_peer(region_id, new_peer(id, id));\n        must_get_equal(&cluster.get_engine(id), key, b\"v0\");\n    }\n\n    // Write the initial value for a key.\n    let key = b\"k\";\n    cluster.must_put(key, b\"v1\");\n    // Force `peer` to become leader.\n    let region = cluster.get_region(key);\n    let region_id = region.get_id();\n    let peer = new_peer(1, 1);\n    cluster.must_transfer_leader(region_id, peer.clone());\n\n    let detector = LeaseReadFilter::default();\n    cluster.add_send_filter(CloneFilterFactory(detector.clone()));\n\n    // election_timeout_ticks * base_tick_interval * 3\n    let hibernate_wait = election_ticks * Duration::from_millis(base_tick_ms) * 3;\n    let request_wait = Duration::from_millis(base_tick_ms);\n    let max_renew_lease_time = 3;\n    let round = hibernate_wait.as_millis() / request_wait.as_millis();\n    for i in 0..round {\n        // Issue a read request and check the value on response.\n        must_read_on_peer(&mut cluster, peer.clone(), region.clone(), key, b\"v1\");\n        // Plus 1 to prevent case failure when test machine is too slow.\n        assert_le!(detector.ctx.rl().len(), max_renew_lease_time + 1, \"{}\", i);\n        thread::sleep(request_wait);\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/vector.rs::len", "code": "pub fn len(&self) -> usize {\n        match_template_evaltype! {\n            TT, match self {\n                VectorValue::TT(v) => v.len(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_life.rs::test_gc_removed_peer", "test": "fn test_gc_removed_peer() {\n    let mut cluster = test_raftstore::new_node_cluster(1, 2);\n    cluster.cfg.raft_store.enable_v2_compatible_learner = true;\n    cluster.pd_client.disable_default_operator();\n    let region_id = cluster.run_conf_change();\n\n    let (tx, rx) = channel();\n    let tx = Mutex::new(tx);\n    let factory = ForwardFactory {\n        node_id: 1,\n        chain_send: Arc::new(move |m| {\n            if m.get_extra_msg().get_type() == ExtraMessageType::MsgGcPeerResponse {\n                let _ = tx.lock().unwrap().send(m);\n            }\n        }),\n        keep_msg: true,\n    };\n    cluster.add_send_filter(factory);\n\n    let check_gc_peer = |to_peer: kvproto::metapb::Peer, timeout| -> bool {\n        let epoch = cluster.get_region_epoch(region_id);\n        let mut msg = RaftMessage::default();\n        msg.set_is_tombstone(true);\n        msg.set_region_id(region_id);\n        msg.set_from_peer(new_peer(1, 1));\n        msg.set_to_peer(to_peer.clone());\n        msg.set_region_epoch(epoch.clone());\n        let extra_msg = msg.mut_extra_msg();\n        extra_msg.set_type(ExtraMessageType::MsgGcPeerRequest);\n        let check_peer = extra_msg.mut_check_gc_peer();\n        check_peer.set_from_region_id(region_id);\n        check_peer.set_check_region_id(region_id);\n        check_peer.set_check_peer(to_peer.clone());\n        check_peer.set_check_region_epoch(epoch);\n\n        cluster.sim.wl().send_raft_msg(msg.clone()).unwrap();\n        let Ok(gc_resp) = rx.recv_timeout(timeout) else {\n            return false;\n        };\n        assert_eq!(gc_resp.get_region_id(), region_id);\n        assert_eq!(*gc_resp.get_from_peer(), to_peer);\n        true\n    };\n\n    // Mock gc a peer that has been removed before creation.\n    assert!(check_gc_peer(\n        new_learner_peer(2, 5),\n        Duration::from_secs(5)\n    ));\n\n    cluster\n        .pd_client\n        .must_add_peer(region_id, new_learner_peer(2, 4));\n    // Make sure learner is created.\n    cluster.wait_peer_state(region_id, 2, PeerState::Normal);\n\n    cluster\n        .pd_client\n        .must_remove_peer(region_id, new_learner_peer(2, 4));\n    // Make sure learner is removed.\n    cluster.wait_peer_state(region_id, 2, PeerState::Tombstone);\n\n    // Mock gc peer request. GC learner(2, 4).\n    let start = Instant::now();\n    loop {\n        if check_gc_peer(new_learner_peer(2, 4), Duration::from_millis(200)) {\n            return;\n        }\n        if start.saturating_elapsed() > Duration::from_secs(5) {\n            break;\n        }\n    }\n    assert!(check_gc_peer(\n        new_learner_peer(2, 4),\n        Duration::from_millis(200)\n    ));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/peer_storage.rs::get_region_id", "code": "pub fn get_region_id(&self) -> u64 {\n        self.region().get_id()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_multi.rs::test_leader_drop_with_pessimistic_lock", "test": "fn test_leader_drop_with_pessimistic_lock() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n\n    let txn_ext = cluster\n        .must_get_snapshot_of_region(1)\n        .ext()\n        .get_txn_ext()\n        .unwrap()\n        .clone();\n    txn_ext\n        .pessimistic_locks\n        .write()\n        .insert(vec![(\n            Key::from_raw(b\"k1\"),\n            PessimisticLock {\n                primary: b\"k1\".to_vec().into_boxed_slice(),\n                start_ts: 10.into(),\n                ttl: 1000,\n                for_update_ts: 10.into(),\n                min_commit_ts: 10.into(),\n                last_change_ts: 5.into(),\n                versions_to_last_change: 3,\n            },\n        )])\n        .unwrap();\n\n    // Isolate node 1, leader should be transferred to another node.\n    cluster.add_send_filter(IsolationFilterFactory::new(1));\n    cluster.must_put(b\"k1\", b\"v1\");\n    assert_ne!(cluster.leader_of_region(1).unwrap().id, 1);\n\n    // When peer 1 becomes leader again, the pessimistic locks should be cleared\n    // before.\n    cluster.clear_send_filters();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    assert!(txn_ext.pessimistic_locks.read().is_empty());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::leader_of_region", "code": "pub fn leader_of_region(&mut self, region_id: u64) -> Option<metapb::Peer> {\n        let timer = Instant::now_coarse();\n        let timeout = Duration::from_secs(5);\n        let mut store_ids = None;\n        while timer.saturating_elapsed() < timeout {\n            match self.voter_store_ids_of_region(region_id) {\n                None => thread::sleep(Duration::from_millis(10)),\n                Some(ids) => {\n                    store_ids = Some(ids);\n                    break;\n                }\n            }\n        }\n        let store_ids = store_ids?;\n        if let Some(l) = self.leaders.get(&region_id) {\n            // leader may be stopped in some tests.\n            if self.valid_leader_id(region_id, l.get_store_id()) {\n                return Some(l.clone());\n            }\n        }\n        self.reset_leader_of_region(region_id);\n        let mut leader = None;\n        let mut leaders = HashMap::default();\n\n        let node_ids = self.sim.rl().get_node_ids();\n        // For some tests, we stop the node but pd still has this information,\n        // and we must skip this.\n        let alive_store_ids: Vec<_> = store_ids\n            .iter()\n            .filter(|id| node_ids.contains(id))\n            .cloned()\n            .collect();\n        while timer.saturating_elapsed() < timeout {\n            for store_id in &alive_store_ids {\n                let l = match self.query_leader(*store_id, region_id, Duration::from_secs(1)) {\n                    None => continue,\n                    Some(l) => l,\n                };\n                leaders\n                    .entry(l.get_id())\n                    .or_insert((l, vec![]))\n                    .1\n                    .push(*store_id);\n            }\n            if let Some((_, (l, c))) = leaders.iter().max_by_key(|(_, (_, c))| c.len()) {\n                if c.contains(&l.get_store_id()) {\n                    leader = Some(l.clone());\n                    // Technically, correct calculation should use two quorum when in joint\n                    // state. Here just for simplicity.\n                    if c.len() > store_ids.len() / 2 {\n                        break;\n                    }\n                }\n            }\n            debug!(\"failed to detect leaders\"; \"leaders\" => ?leaders, \"store_ids\" => ?store_ids);\n            sleep_ms(10);\n            leaders.clear();\n        }\n\n        if let Some(l) = leader {\n            self.leaders.insert(region_id, l);\n        }\n\n        self.leaders.get(&region_id).cloned()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_replication_mode.rs::test_sync_recover_after_apply_snapshot", "test": "fn test_sync_recover_after_apply_snapshot() {\n    let mut cluster = prepare_cluster();\n    configure_for_snapshot(&mut cluster);\n    run_cluster(&mut cluster);\n    let region = cluster.get_region(b\"k1\");\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_put_cf_cmd(\"default\", b\"k2\", b\"v2\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n    must_get_none(&cluster.get_engine(1), b\"k2\");\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 1);\n    assert_eq!(state.state, RegionReplicationState::IntegrityOverLabel);\n\n    // swith to async\n    cluster\n        .pd_client\n        .switch_replication_mode(DrAutoSyncState::Async, vec![]);\n    rx.recv_timeout(Duration::from_millis(100)).unwrap();\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n    thread::sleep(Duration::from_millis(100));\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 2);\n    assert_eq!(state.state, RegionReplicationState::SimpleMajority);\n\n    // Write some data to trigger snapshot.\n    for i in 10..110 {\n        let key = format!(\"k{}\", i);\n        let value = format!(\"v{}\", i);\n        cluster.must_put_cf(\"default\", key.as_bytes(), value.as_bytes());\n    }\n\n    cluster\n        .pd_client\n        .switch_replication_mode(DrAutoSyncState::SyncRecover, vec![]);\n    thread::sleep(Duration::from_millis(100));\n    // Add node 3 back, snapshot will apply\n    cluster.clear_send_filters();\n    cluster.must_transfer_leader(region.get_id(), new_peer(3, 3));\n    must_get_equal(&cluster.get_engine(3), b\"k100\", b\"v100\");\n    thread::sleep(Duration::from_millis(100));\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 3);\n    assert_eq!(state.state, RegionReplicationState::IntegrityOverLabel);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_replication_mode.rs::test_switching_replication_mode", "test": "fn test_switching_replication_mode() {\n    let mut cluster = prepare_cluster();\n    run_cluster(&mut cluster);\n    let region = cluster.get_region(b\"k1\");\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_put_cf_cmd(\"default\", b\"k2\", b\"v2\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n    must_get_none(&cluster.get_engine(1), b\"k2\");\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 1);\n    assert_eq!(state.state, RegionReplicationState::IntegrityOverLabel);\n\n    cluster\n        .pd_client\n        .switch_replication_mode(DrAutoSyncState::Async, vec![]);\n    rx.recv_timeout(Duration::from_millis(100)).unwrap();\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n    thread::sleep(Duration::from_millis(100));\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 2);\n    assert_eq!(state.state, RegionReplicationState::SimpleMajority);\n\n    cluster\n        .pd_client\n        .switch_replication_mode(DrAutoSyncState::SyncRecover, vec![]);\n    thread::sleep(Duration::from_millis(100));\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_put_cf_cmd(\"default\", b\"k3\", b\"v3\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    // sync recover should not block write. ref https://github.com/tikv/tikv/issues/14975.\n    assert_eq!(rx.recv_timeout(Duration::from_millis(100)).is_ok(), true);\n    must_get_equal(&cluster.get_engine(1), b\"k3\", b\"v3\");\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 3);\n    assert_eq!(state.state, RegionReplicationState::SimpleMajority);\n\n    cluster.clear_send_filters();\n    must_get_equal(&cluster.get_engine(1), b\"k3\", b\"v3\");\n    thread::sleep(Duration::from_millis(100));\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 3);\n    assert_eq!(state.state, RegionReplicationState::IntegrityOverLabel);\n\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_put_cf_cmd(\"default\", b\"k4\", b\"v4\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    // already enable group commit.\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_replication_mode.rs::test_replication_mode_allowlist", "test": "fn test_replication_mode_allowlist() {\n    let mut cluster = prepare_cluster();\n    run_cluster(&mut cluster);\n    cluster\n        .pd_client\n        .switch_replication_mode(DrAutoSyncState::Async, vec![1]);\n    thread::sleep(Duration::from_millis(100));\n\n    // 2,3 are paused, so it should not be able to write.\n    let region = cluster.get_region(b\"k1\");\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_put_cf_cmd(\"default\", b\"k2\", b\"v2\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n\n    // clear allowlist.\n    cluster\n        .pd_client\n        .switch_replication_mode(DrAutoSyncState::Async, vec![]);\n    rx.recv_timeout(Duration::from_millis(100)).unwrap();\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_replication_mode.rs::test_migrate_replication_mode", "test": "fn test_migrate_replication_mode() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.pd_client.disable_default_operator();\n    cluster.cfg.raft_store.pd_store_heartbeat_tick_interval = ReadableDuration::millis(50);\n    cluster.cfg.raft_store.raft_log_gc_threshold = 10;\n    cluster.add_label(1, \"zone\", \"ES\");\n    cluster.add_label(2, \"zone\", \"ES\");\n    cluster.add_label(3, \"zone\", \"WS\");\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n    cluster.must_put(b\"k1\", b\"v0\");\n    // Non exists label key can't tolerate any node unavailable.\n    cluster.pd_client.configure_dr_auto_sync(\"host\");\n    thread::sleep(Duration::from_millis(100));\n    let region = cluster.get_region(b\"k1\");\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_put_cf_cmd(\"default\", b\"k2\", b\"v2\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(1, 1));\n    let (cb, mut rx) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(1, request, cb)\n        .unwrap();\n    assert_eq!(\n        rx.recv_timeout(Duration::from_millis(100)),\n        Err(future::RecvTimeoutError::Timeout)\n    );\n    must_get_none(&cluster.get_engine(1), b\"k2\");\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 1);\n    assert_eq!(state.state, RegionReplicationState::SimpleMajority);\n\n    // Correct label key should resume committing log\n    cluster.pd_client.configure_dr_auto_sync(\"zone\");\n    rx.recv_timeout(Duration::from_millis(100)).unwrap();\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n    thread::sleep(Duration::from_millis(100));\n    let state = cluster.pd_client.region_replication_status(region.get_id());\n    assert_eq!(state.state_id, 2);\n    assert_eq!(state.state, RegionReplicationState::IntegrityOverLabel);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/channel.rs::recv_timeout", "code": "pub fn recv_timeout<S, I>(s: &mut S, dur: std::time::Duration) -> Result<Option<I>, ()>\nwhere\n    S: Stream<Item = I> + Unpin,\n{\n    poll_timeout(&mut s.next(), dur)\n}", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_scale_pool.rs::test_increase_pool", "test": "fn test_increase_pool() {\n    let mut cluster = new_node_cluster(0, 1);\n    cluster.cfg.raft_store.store_batch_system.pool_size = 1;\n    cluster.cfg.raft_store.apply_batch_system.pool_size = 1;\n    cluster.pd_client.disable_default_operator();\n    let fp1 = \"poll\";\n\n    // Pause at the entrance of the apply-0, apply-low-0, rafstore-1-0 threads\n    fail::cfg(fp1, \"3*pause\").unwrap();\n    let _ = cluster.run_conf_change();\n\n    // Request cann't be handled as all pollers have been paused\n    put_with_timeout(&mut cluster, b\"k1\", b\"k1\", Duration::from_secs(1)).unwrap();\n    must_get_none(&cluster.get_engine(1), b\"k1\");\n\n    {\n        let sim = cluster.sim.rl();\n        let cfg_controller = sim.get_cfg_controller().unwrap();\n\n        let change = {\n            let mut change = HashMap::new();\n            change.insert(\"raftstore.store-pool-size\".to_owned(), \"2\".to_owned());\n            change.insert(\"raftstore.apply_pool_size\".to_owned(), \"2\".to_owned());\n            change\n        };\n\n        // Update config, expand from 1 to 2\n        cfg_controller.update(change).unwrap();\n        assert_eq!(\n            cfg_controller\n                .get_current()\n                .raft_store\n                .apply_batch_system\n                .pool_size,\n            2\n        );\n        assert_eq!(\n            cfg_controller\n                .get_current()\n                .raft_store\n                .store_batch_system\n                .pool_size,\n            2\n        );\n    }\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n\n    fail::remove(fp1);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/config/mod.rs::get_current", "code": "pub fn get_current(&self) -> TikvConfig {\n        self.inner.read().unwrap().current.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_scale_pool.rs::test_decrease_pool", "test": "fn test_decrease_pool() {\n    let mut cluster = new_node_cluster(0, 1);\n    cluster.pd_client.disable_default_operator();\n    cluster.cfg.raft_store.store_batch_system.pool_size = 2;\n    cluster.cfg.raft_store.apply_batch_system.pool_size = 2;\n    let _ = cluster.run_conf_change();\n\n    // Save current poller tids before shrinking\n    let original_poller_tids = get_poller_thread_ids();\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    {\n        let sim = cluster.sim.rl();\n        let cfg_controller = sim.get_cfg_controller().unwrap();\n        let change = {\n            let mut change = HashMap::new();\n            change.insert(\"raftstore.store_pool_size\".to_owned(), \"1\".to_owned());\n            change.insert(\"raftstore.apply-pool-size\".to_owned(), \"1\".to_owned());\n            change\n        };\n\n        // Update config, shrink from 2 to 1\n        cfg_controller.update(change).unwrap();\n        std::thread::sleep(std::time::Duration::from_secs(1));\n\n        assert_eq!(\n            cfg_controller\n                .get_current()\n                .raft_store\n                .apply_batch_system\n                .pool_size,\n            1\n        );\n        assert_eq!(\n            cfg_controller\n                .get_current()\n                .raft_store\n                .store_batch_system\n                .pool_size,\n            1\n        );\n    }\n\n    // Save current poller tids after scaling down\n    let current_poller_tids = get_poller_thread_ids();\n    // Compared with before shrinking, the thread num should be reduced by two\n    assert_eq!(current_poller_tids.len(), original_poller_tids.len() - 2);\n    // After shrinking, all the left tids must be there before\n    for tid in current_poller_tids {\n        assert!(original_poller_tids.contains(&tid));\n    }\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/config/mod.rs::get_current", "code": "pub fn get_current(&self) -> TikvConfig {\n        self.inner.read().unwrap().current.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_scale_pool.rs::test_increase_async_ios", "test": "fn test_increase_async_ios() {\n    let mut cluster = new_node_cluster(0, 1);\n    cluster.cfg.raft_store.store_io_pool_size = 1;\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n\n    // Save current async-io tids before shrinking\n    let org_writers_tids = get_async_writers_tids();\n    assert_eq!(1, org_writers_tids.len());\n    // Request can be handled as usual\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    // Update config, expand from 1 to 2\n    {\n        let sim = cluster.sim.rl();\n        let cfg_controller = sim.get_cfg_controller().unwrap();\n\n        let change = {\n            let mut change = HashMap::new();\n            change.insert(\"raftstore.store-io-pool-size\".to_owned(), \"2\".to_owned());\n            change\n        };\n\n        cfg_controller.update(change).unwrap();\n        assert_eq!(\n            cfg_controller.get_current().raft_store.store_io_pool_size,\n            2\n        );\n        // Wait for the completion of increasing async-ios\n        std::thread::sleep(std::time::Duration::from_secs(1));\n    }\n    // Save current async-io tids after scaling up, and compared with the\n    // orginial one before scaling up, the thread num should be added up to TWO.\n    let cur_writers_tids = get_async_writers_tids();\n    assert_eq!(cur_writers_tids.len() - 1, org_writers_tids.len());\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/table.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cols.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_scale_pool.rs::test_decrease_async_ios", "test": "fn test_decrease_async_ios() {\n    let mut cluster = new_node_cluster(0, 1);\n    cluster.cfg.raft_store.store_io_pool_size = 4;\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n\n    // Save current async-io tids before shrinking\n    let org_writers_tids = get_async_writers_tids();\n    assert_eq!(4, org_writers_tids.len());\n    // Request can be handled as usual\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    // Update config, shrink from 4 to 1\n    {\n        let sim = cluster.sim.rl();\n        let cfg_controller = sim.get_cfg_controller().unwrap();\n        let change = {\n            let mut change = HashMap::new();\n            change.insert(\"raftstore.store-io-pool-size\".to_owned(), \"1\".to_owned());\n            change\n        };\n\n        cfg_controller.update(change).unwrap();\n        assert_eq!(\n            cfg_controller.get_current().raft_store.store_io_pool_size,\n            1\n        );\n        // Wait for the completion of decreasing async-ios\n        std::thread::sleep(std::time::Duration::from_secs(1));\n    }\n\n    // Save current async-io tids after scaling down, and compared with the\n    // orginial one before shrinking. As the decreasing of async-ios won't\n    // release asynchronous writers, the thread num should not be updated.\n    let cur_writers_tids = get_async_writers_tids();\n    assert_eq!(cur_writers_tids.len(), org_writers_tids.len());\n    // After shrinking, all the left tids must be there before\n    for tid in cur_writers_tids {\n        assert!(org_writers_tids.contains(&tid));\n    }\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/table.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cols.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_scale_pool.rs::test_resize_async_ios_failed_1", "test": "fn test_resize_async_ios_failed_1() {\n    let mut cluster = new_node_cluster(0, 1);\n    cluster.cfg.raft_store.store_io_pool_size = 2;\n    cluster.pd_client.disable_default_operator();\n    cluster.run();\n\n    // Save current async-io tids before shrinking\n    let org_writers_tids = get_async_writers_tids();\n    assert_eq!(2, org_writers_tids.len());\n    // Request can be handled as usual\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    // Update config, expand from async-mode(async-ios == 2) to\n    // sync-mode(async-ios == 0).\n    {\n        let sim = cluster.sim.rl();\n        let cfg_controller = sim.get_cfg_controller().unwrap();\n\n        let change = {\n            let mut change = HashMap::new();\n            change.insert(\"raftstore.store-io-pool-size\".to_owned(), \"0\".to_owned());\n            change\n        };\n\n        assert!(cfg_controller.update(change).is_err());\n        assert_eq!(\n            cfg_controller.get_current().raft_store.store_io_pool_size,\n            2\n        );\n    }\n    // Save current async-io tids after scaling up, and compared with the\n    // orginial one before scaling up, the thread num should be added up to TWO.\n    let cur_writers_tids = get_async_writers_tids();\n    assert_eq!(cur_writers_tids.len(), org_writers_tids.len());\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/table.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cols.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_scale_pool.rs::test_resize_async_ios_failed_2", "test": "fn test_resize_async_ios_failed_2() {\n    let mut cluster = new_node_cluster(0, 1);\n    cluster.cfg.raft_store.store_io_pool_size = 0;\n    cluster.pd_client.disable_default_operator();\n    let _ = cluster.run_conf_change();\n\n    // Save current async-io tids before shrinking\n    let org_writers_tids = get_async_writers_tids();\n    assert_eq!(0, org_writers_tids.len());\n    // Request can be handled as usual\n    cluster.must_put(b\"k1\", b\"v1\");\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    // Update config, expand from sync-mode(async-ios == 0) to\n    // async-mode(async-ios == 2).\n    {\n        let sim = cluster.sim.rl();\n        let cfg_controller = sim.get_cfg_controller().unwrap();\n\n        let change = {\n            let mut change = HashMap::new();\n            change.insert(\"raftstore.store-io-pool-size\".to_owned(), \"2\".to_owned());\n            change\n        };\n\n        assert!(cfg_controller.update(change).is_err());\n        assert_eq!(\n            cfg_controller.get_current().raft_store.store_io_pool_size,\n            0\n        );\n    }\n    // Save current async-io tids after scaling up, and compared with the\n    // orginial one before scaling up, the thread num should be added up to TWO.\n    let cur_writers_tids = get_async_writers_tids();\n    assert_eq!(cur_writers_tids.len(), org_writers_tids.len());\n\n    // Request can be handled as usual\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(1), b\"k2\", b\"v2\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/table.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cols.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_snap.rs::test_inspected_snapshot", "test": "fn test_inspected_snapshot() {\n    let mut cluster = new_server_cluster(1, 3);\n    cluster.cfg.prefer_mem = false;\n    cluster.cfg.raft_store.raft_log_gc_tick_interval = ReadableDuration::millis(20);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(8);\n    cluster.cfg.raft_store.merge_max_log_gap = 3;\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_transfer_leader(1, new_peer(1, 1));\n    cluster.stop_node(3);\n    (0..10).for_each(|_| cluster.must_put(b\"k2\", b\"v2\"));\n    // Sleep for a while to ensure all logs are compacted.\n    sleep_ms(100);\n\n    let stats = cluster\n        .io_rate_limiter\n        .as_ref()\n        .unwrap()\n        .statistics()\n        .unwrap();\n    assert_eq!(stats.fetch(IoType::Replication, IoOp::Read), 0);\n    assert_eq!(stats.fetch(IoType::Replication, IoOp::Write), 0);\n    // Make sure snapshot read hits disk\n    cluster.flush_data();\n    // Let store 3 inform leader to generate a snapshot.\n    cluster.run_node(3).unwrap();\n    must_get_equal(&cluster.get_engine(3), b\"k2\", b\"v2\");\n    assert_ne!(stats.fetch(IoType::Replication, IoOp::Read), 0);\n    assert_ne!(stats.fetch(IoType::Replication, IoOp::Write), 0);\n\n    pd_client.must_remove_peer(1, new_peer(2, 2));\n    must_get_none(&cluster.get_engine(2), b\"k2\");\n    assert_eq!(stats.fetch(IoType::LoadBalance, IoOp::Read), 0);\n    assert_eq!(stats.fetch(IoType::LoadBalance, IoOp::Write), 0);\n    pd_client.must_add_peer(1, new_peer(2, 2));\n    must_get_equal(&cluster.get_engine(2), b\"k2\", b\"v2\");\n    assert_ne!(stats.fetch(IoType::LoadBalance, IoOp::Read), 0);\n    assert_ne!(stats.fetch(IoType::LoadBalance, IoOp::Write), 0);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tikv_util/src/math.rs::fetch", "code": "pub fn fetch(&self) -> u32 {\n        self.cached_avg.load(Ordering::Relaxed)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_snap_recovery.rs::test_snap_wait_apply", "test": "fn test_snap_wait_apply() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.pd_client.disable_default_operator();\n    cluster.cfg.raft_store.store_io_pool_size = 0;\n\n    cluster.run();\n\n    // write a key to let leader stuck.\n    cluster.must_put(b\"k\", b\"v\");\n    must_get_equal(&cluster.get_engine(1), b\"k\", b\"v\");\n    must_get_equal(&cluster.get_engine(2), b\"k\", b\"v\");\n    must_get_equal(&cluster.get_engine(3), b\"k\", b\"v\");\n\n    // add filter to make leader 1 cannot receive follower append response.\n    cluster.add_send_filter(CloneFilterFactory(\n        RegionPacketFilter::new(1, 1)\n            .msg_type(MessageType::MsgAppendResponse)\n            .direction(Direction::Recv),\n    ));\n\n    // make a async put request to let leader has inflight raft log.\n    cluster.async_put(b\"k2\", b\"v2\").unwrap();\n    std::thread::sleep(Duration::from_millis(800));\n\n    let router = cluster.sim.wl().get_router(1).unwrap();\n\n    let (tx, rx) = std::sync::mpsc::sync_channel(1);\n\n    router.broadcast_normal(|| {\n        PeerMsg::SignificantMsg(SignificantMsg::SnapshotRecoveryWaitApply(\n            SnapshotRecoveryWaitApplySyncer::new(1, tx.clone()),\n        ))\n    });\n\n    // we expect recv timeout because the leader peer on store 1 cannot finished the\n    // apply. so the wait apply will timeout.\n    rx.recv_timeout(Duration::from_secs(1)).unwrap_err();\n\n    // clear filter so we can make wait apply finished.\n    cluster.clear_send_filters();\n    std::thread::sleep(Duration::from_millis(800));\n\n    // after clear the filter the leader peer on store 1 can finsihed the wait\n    // apply.\n    let (tx, rx) = std::sync::mpsc::sync_channel(1);\n    router.broadcast_normal(|| {\n        PeerMsg::SignificantMsg(SignificantMsg::SnapshotRecoveryWaitApply(\n            SnapshotRecoveryWaitApplySyncer::new(1, tx.clone()),\n        ))\n    });\n\n    // we expect recv the region id from rx.\n    assert_eq!(rx.recv(), Ok(1));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/fsm/peer.rs::recv", "code": "pub fn recv(&mut self, peer_msg_buf: &mut Vec<PeerMsg>, batch_size: usize) -> usize {\n        let l = peer_msg_buf.len();\n        for i in l..batch_size {\n            match self.receiver.try_recv() {\n                Ok(msg) => peer_msg_buf.push(msg),\n                Err(e) => {\n                    if let TryRecvError::Disconnected = e {\n                        self.is_stopped = true;\n                    }\n                    return i - l;\n                }\n            }\n        }\n        batch_size - l\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_split_region.rs::test_split_region_keep_records", "test": "fn test_split_region_keep_records() {\n    let mut cluster = test_raftstore_v2::new_node_cluster(0, 3);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let r1 = cluster.run_conf_change();\n    cluster.must_put(b\"k1\", b\"v1\");\n    pd_client.must_add_peer(r1, new_peer(2, 2));\n    must_get_equal(&cluster.get_engine(2), b\"k1\", b\"v1\");\n    pd_client.must_remove_peer(r1, new_peer(2, 2));\n\n    let leader = cluster.leader_of_region(r1).unwrap();\n    cluster.add_send_filter_on_node(\n        leader.get_store_id(),\n        Box::new(DropMessageFilter::new(Arc::new(|m: &RaftMessage| {\n            // Drop all gc peer requests and responses.\n            !(m.has_extra_msg()\n                && (m.get_extra_msg().get_type() == ExtraMessageType::MsgGcPeerRequest\n                    || m.get_extra_msg().get_type() == ExtraMessageType::MsgGcPeerResponse))\n        }))),\n    );\n\n    // Make sure split has applied.\n    let region = pd_client.get_region(b\"\").unwrap();\n    cluster.must_split(&region, b\"k1\");\n    cluster.must_put(b\"k2\", b\"v2\");\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region_state = cluster.region_local_state(r1, leader.get_store_id());\n    assert!(\n        !region_state.get_removed_records().is_empty(),\n        \"{:?}\",\n        region_state\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/encryption.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.key.is_empty() && self.iv.is_empty()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_stale_peer.rs::test_stale_learner", "test": "fn test_stale_learner() {\n    let mut cluster = new_server_cluster(0, 4);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 5;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::millis(150);\n    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::millis(100);\n    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::millis(100);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let r1 = cluster.run_conf_change();\n    pd_client.must_add_peer(r1, new_peer(2, 2));\n    pd_client.must_add_peer(r1, new_learner_peer(3, 3));\n    cluster.must_put(b\"k1\", b\"v1\");\n    let engine3 = cluster.get_engine(3);\n    must_get_equal(&engine3, b\"k1\", b\"v1\");\n\n    // And then isolate peer on store 3 from leader.\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n\n    // Add a new peer to increase the conf version.\n    pd_client.must_add_peer(r1, new_peer(4, 4));\n\n    // It should not be deleted.\n    thread::sleep(Duration::from_millis(200));\n    must_get_equal(&engine3, b\"k1\", b\"v1\");\n\n    // Promote the learner\n    pd_client.must_add_peer(r1, new_peer(3, 3));\n\n    // It should not be deleted.\n    thread::sleep(Duration::from_millis(200));\n    must_get_equal(&engine3, b\"k1\", b\"v1\");\n\n    // Delete the learner\n    pd_client.must_remove_peer(r1, new_peer(3, 3));\n\n    // Check not leader should fail, all data should be removed.\n    must_get_none(&engine3, b\"k1\");\n    let state_key = keys::region_state_key(r1);\n    let state: RegionLocalState = engine3.get_msg_cf(CF_RAFT, &state_key).unwrap().unwrap();\n    assert_eq!(state.get_state(), PeerState::Tombstone);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_state", "code": "pub fn get_state(&self) -> Arc<AtomicCell<DownstreamState>> {\n        self.state.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_stale_peer.rs::test_stale_learner_with_read_index", "test": "fn test_stale_learner_with_read_index() {\n    let mut cluster = new_server_cluster(0, 4);\n    // Do not rely on pd to remove stale peer\n    cluster.cfg.raft_store.max_leader_missing_duration = ReadableDuration::hours(2);\n    cluster.cfg.raft_store.abnormal_leader_missing_duration = ReadableDuration::minutes(20);\n    cluster.cfg.raft_store.peer_stale_state_check_interval = ReadableDuration::minutes(10);\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check\n    pd_client.disable_default_operator();\n\n    let r1 = cluster.run_conf_change();\n    pd_client.must_add_peer(r1, new_peer(2, 2));\n    pd_client.must_add_peer(r1, new_learner_peer(3, 3));\n    cluster.must_put(b\"k1\", b\"v1\");\n    let engine3 = cluster.get_engine(3);\n    must_get_equal(&engine3, b\"k1\", b\"v1\");\n\n    // And then isolate peer on store 3 from leader\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n\n    // Delete the learner\n    pd_client.must_remove_peer(r1, new_learner_peer(3, 3));\n\n    cluster.clear_send_filters();\n\n    // Stale learner should exist\n    must_get_equal(&engine3, b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cf_cmd(\"default\", b\"k1\")],\n        false,\n    );\n    request.mut_header().set_peer(new_peer(3, 3));\n    request.mut_header().set_replica_read(true);\n    let (cb, _) = make_cb(&request);\n    cluster\n        .sim\n        .rl()\n        .async_command_on_node(3, request, cb)\n        .unwrap();\n\n    // Stale learner should be destroyed due to interaction between leader\n    must_get_none(&engine3, b\"k1\");\n    let state_key = keys::region_state_key(r1);\n    let state: RegionLocalState = engine3.get_msg_cf(CF_RAFT, &state_key).unwrap().unwrap();\n    assert_eq!(state.get_state(), PeerState::Tombstone);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/delegate.rs::get_state", "code": "pub fn get_state(&self) -> Arc<AtomicCell<DownstreamState>> {\n        self.state.clone()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_tombstone.rs::test_destroy_clean_up_logs_with_log_gc", "test": "fn test_destroy_clean_up_logs_with_log_gc() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(50);\n    cluster.cfg.raft_store.raft_log_gc_threshold = 50;\n    let pd_client = cluster.pd_client.clone();\n\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k2\", b\"v2\");\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    let raft_engine = cluster.engines[&3].raft.clone();\n    let mut dest = vec![];\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    assert!(!dest.is_empty());\n\n    pd_client.must_remove_peer(1, new_peer(3, 3));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    dest.clear();\n    // Normally destroy peer should cleanup all logs.\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    assert!(dest.is_empty(), \"{:?}\", dest);\n\n    pd_client.must_add_peer(1, new_peer(3, 4));\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n    must_get_equal(&cluster.get_engine(3), b\"k3\", b\"v3\");\n    dest.clear();\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    assert!(!dest.is_empty());\n\n    pd_client.must_remove_peer(1, new_peer(3, 4));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    dest.clear();\n    // Peer created by snapshot should also cleanup all logs.\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    assert!(dest.is_empty(), \"{:?}\", dest);\n\n    pd_client.must_add_peer(1, new_peer(3, 5));\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n    cluster.must_put(b\"k4\", b\"v4\");\n    must_get_equal(&cluster.get_engine(3), b\"k4\", b\"v4\");\n    dest.clear();\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    assert!(!dest.is_empty());\n\n    let state = cluster.truncated_state(1, 3);\n    for _ in 0..50 {\n        cluster.must_put(b\"k5\", b\"v5\");\n    }\n    cluster.wait_log_truncated(1, 3, state.get_index() + 1);\n\n    pd_client.must_remove_peer(1, new_peer(3, 5));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    dest.clear();\n    // Peer destroy after log gc should also cleanup all logs.\n    raft_engine.get_all_entries_to(1, &mut dest).unwrap();\n    assert!(dest.is_empty(), \"{:?}\", dest);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/txn/store.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.entries.len() == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_demote_failed_voters", "test": "fn test_unsafe_recovery_demote_failed_voters() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    let peer_on_store2 = find_peer(&region, nodes[2]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store2.clone());\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    cluster.must_enter_force_leader(region.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    let to_be_removed: Vec<metapb::Peer> = region\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut plan = pdpb::RecoveryPlan::default();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    let mut demoted = true;\n    for _ in 0..10 {\n        let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n        demoted = true;\n        for peer in region.get_peers() {\n            if peer.get_id() != nodes[0] && peer.get_role() == metapb::PeerRole::Voter {\n                demoted = false;\n            }\n        }\n        if demoted {\n            break;\n        }\n        sleep_ms(200);\n    }\n    assert!(demoted);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_demote_non_exist_voters", "test": "fn test_unsafe_recovery_demote_non_exist_voters() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    let peer_on_store2 = find_peer(&region, nodes[2]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store2.clone());\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n    cluster.must_enter_force_leader(region.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    let mut plan = pdpb::RecoveryPlan::default();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region.get_id());\n    let mut peer = metapb::Peer::default();\n    peer.set_id(12345);\n    peer.set_store_id(region.get_id());\n    peer.set_role(metapb::PeerRole::Voter);\n    demote.mut_failed_voters().push(peer);\n    plan.mut_demotes().push(demote);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    let mut store_report = None;\n    for _ in 0..20 {\n        store_report = pd_client.must_get_store_report(nodes[0]);\n        if store_report.is_some() {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_ne!(store_report, None);\n    let report = store_report.unwrap();\n    let peer_reports = report.get_peer_reports();\n    assert_eq!(peer_reports.len(), 1);\n    let reported_region = peer_reports[0].get_region_state().get_region();\n    assert_eq!(reported_region.get_id(), region.get_id());\n    assert_eq!(reported_region.get_peers().len(), 3);\n    let demoted = reported_region\n        .get_peers()\n        .iter()\n        .any(|peer| peer.get_role() != metapb::PeerRole::Voter);\n    assert_eq!(demoted, false);\n\n    let region_in_pd = block_on(pd_client.get_region_by_id(region.get_id()))\n        .unwrap()\n        .unwrap();\n    assert_eq!(region_in_pd.get_peers().len(), 3);\n    let demoted = region_in_pd\n        .get_peers()\n        .iter()\n        .any(|peer| peer.get_role() != metapb::PeerRole::Voter);\n    assert_eq!(demoted, false);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_auto_promote_learner", "test": "fn test_unsafe_recovery_auto_promote_learner() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    let peer_on_store0 = find_peer(&region, nodes[0]).unwrap();\n    let peer_on_store2 = find_peer(&region, nodes[2]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store2.clone());\n    // replace one peer with learner\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store0.clone());\n    cluster.pd_client.must_add_peer(\n        region.get_id(),\n        new_learner_peer(nodes[0], peer_on_store0.get_id()),\n    );\n    // Sleep 100 ms to wait for the new learner to be initialized.\n    sleep_ms(100);\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n    cluster.must_enter_force_leader(region.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    let to_be_removed: Vec<metapb::Peer> = region\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut plan = pdpb::RecoveryPlan::default();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    let mut demoted = true;\n    let mut promoted = false;\n    for _ in 0..10 {\n        let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n        promoted = region\n            .get_peers()\n            .iter()\n            .find(|peer| peer.get_store_id() == nodes[0])\n            .unwrap()\n            .get_role()\n            == metapb::PeerRole::Voter;\n\n        demoted = region\n            .get_peers()\n            .iter()\n            .filter(|peer| peer.get_store_id() != nodes[0])\n            .all(|peer| peer.get_role() == metapb::PeerRole::Learner);\n        if demoted && promoted {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert!(demoted);\n    assert!(promoted);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_already_in_joint_state", "test": "fn test_unsafe_recovery_already_in_joint_state() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    let peer_on_store0 = find_peer(&region, nodes[0]).unwrap();\n    let peer_on_store2 = find_peer(&region, nodes[2]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store2.clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store2.clone());\n    cluster.pd_client.must_add_peer(\n        region.get_id(),\n        new_learner_peer(nodes[2], peer_on_store2.get_id()),\n    );\n    // Wait the new learner to be initialized.\n    sleep_ms(100);\n    pd_client.must_joint_confchange(\n        region.get_id(),\n        vec![\n            (\n                ConfChangeType::AddLearnerNode,\n                new_learner_peer(nodes[0], peer_on_store0.get_id()),\n            ),\n            (\n                ConfChangeType::AddNode,\n                new_peer(nodes[2], peer_on_store2.get_id()),\n            ),\n        ],\n    );\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    cluster.must_wait_for_leader_expire(nodes[0], region.get_id());\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n    cluster.must_enter_force_leader(region.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    let to_be_removed: Vec<metapb::Peer> = region\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut plan = pdpb::RecoveryPlan::default();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    let mut demoted = true;\n    let mut promoted = false;\n    for _ in 0..10 {\n        let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n        promoted = region\n            .get_peers()\n            .iter()\n            .find(|peer| peer.get_store_id() == nodes[0])\n            .unwrap()\n            .get_role()\n            == metapb::PeerRole::Voter;\n\n        demoted = region\n            .get_peers()\n            .iter()\n            .filter(|peer| peer.get_store_id() != nodes[0])\n            .all(|peer| peer.get_role() == metapb::PeerRole::Learner);\n        if demoted && promoted {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert!(demoted);\n    assert!(promoted);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_early_return_after_exit_joint_state", "test": "fn test_unsafe_recovery_early_return_after_exit_joint_state() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // Changes the group config to\n    let peer_on_store0 = find_peer(&region, nodes[0]).unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[1]).unwrap();\n    let peer_on_store2 = find_peer(&region, nodes[2]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store2.clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store0.clone());\n    cluster.pd_client.must_add_peer(\n        region.get_id(),\n        new_learner_peer(nodes[0], peer_on_store0.get_id()),\n    );\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store2.clone());\n    cluster.pd_client.must_add_peer(\n        region.get_id(),\n        new_learner_peer(nodes[2], peer_on_store2.get_id()),\n    );\n    // Wait the new learner to be initialized.\n    sleep_ms(100);\n    pd_client.must_joint_confchange(\n        region.get_id(),\n        vec![\n            (\n                ConfChangeType::AddNode,\n                new_peer(nodes[0], peer_on_store0.get_id()),\n            ),\n            (\n                ConfChangeType::AddLearnerNode,\n                new_learner_peer(nodes[1], peer_on_store1.get_id()),\n            ),\n        ],\n    );\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    cluster.must_wait_for_leader_expire(nodes[0], region.get_id());\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n    cluster.must_enter_force_leader(region.get_id(), nodes[0], vec![nodes[1], nodes[2]]);\n\n    let to_be_removed: Vec<metapb::Peer> = region\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != nodes[0])\n        .cloned()\n        .collect();\n    let mut plan = pdpb::RecoveryPlan::default();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(region.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n\n    let mut demoted = true;\n    for _ in 0..10 {\n        let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n        demoted = region\n            .get_peers()\n            .iter()\n            .filter(|peer| peer.get_store_id() != nodes[0])\n            .all(|peer| peer.get_role() == metapb::PeerRole::Learner);\n        if demoted {\n            break;\n        }\n        sleep_ms(100);\n    }\n    assert_eq!(demoted, true);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_create_region", "test": "fn test_unsafe_recovery_create_region() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    // Disable default max peer number check.\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let store0_peer = find_peer(&region, nodes[0]).unwrap().to_owned();\n\n    // Removes the boostrap region, since it overlaps with any regions we create.\n    pd_client.must_remove_peer(region.get_id(), store0_peer);\n    cluster.must_remove_region(nodes[0], region.get_id());\n\n    cluster.stop_node(nodes[1]);\n    cluster.stop_node(nodes[2]);\n    cluster.must_wait_for_leader_expire(nodes[0], region.get_id());\n\n    let mut create = metapb::Region::default();\n    create.set_id(101);\n    create.set_start_key(b\"anykey\".to_vec());\n    let mut peer = metapb::Peer::default();\n    peer.set_id(102);\n    peer.set_store_id(nodes[0]);\n    create.mut_peers().push(peer);\n    let mut plan = pdpb::RecoveryPlan::default();\n    plan.mut_creates().push(create);\n    pd_client.must_set_unsafe_recovery_plan(nodes[0], plan);\n    cluster.must_send_store_heartbeat(nodes[0]);\n    let mut created = false;\n    for _ in 1..11 {\n        let region = pd_client.get_region(b\"anykey1\").unwrap();\n        if region.get_id() == 101 {\n            created = true;\n        }\n        sleep_ms(200);\n    }\n    assert_eq!(created, true);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_three_nodes", "test": "fn test_force_leader_three_nodes() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store3 = find_peer(&region, 3).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store3.clone());\n\n    cluster.stop_node(2);\n    cluster.stop_node(3);\n\n    // quorum is lost, can't propose command successfully.\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![2, 3]);\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 2).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    // forbid writes in force leader state\n    let put = new_put_cmd(b\"k3\", b\"v3\");\n    must_get_error_recovery_in_progress(&mut cluster, &region, put);\n    // forbid reads in force leader state\n    let get = new_get_cmd(b\"k1\");\n    must_get_error_recovery_in_progress(&mut cluster, &region, get);\n    // forbid read index in force leader state\n    let read_index = new_read_index_cmd();\n    must_get_error_recovery_in_progress(&mut cluster, &region, read_index);\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), None);\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_five_nodes", "test": "fn test_force_leader_five_nodes() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    // quorum is lost, can't propose command successfully.\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    // forbid writes in force leader state\n    let put = new_put_cmd(b\"k3\", b\"v3\");\n    must_get_error_recovery_in_progress(&mut cluster, &region, put);\n    // forbid reads in force leader state\n    let get = new_get_cmd(b\"k1\");\n    must_get_error_recovery_in_progress(&mut cluster, &region, get);\n    // forbid read index in force leader state\n    let read_index = new_read_index_cmd();\n    must_get_error_recovery_in_progress(&mut cluster, &region, read_index);\n\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), None);\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_for_learner", "test": "fn test_force_leader_for_learner() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(10);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 5;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    let peer_on_store1 = find_peer(&region, 1).unwrap();\n    // replace one peer with learner\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store1.clone());\n    cluster.pd_client.must_add_peer(\n        region.get_id(),\n        new_learner_peer(peer_on_store1.get_store_id(), peer_on_store1.get_id()),\n    );\n    // Sleep 100 ms to wait for the new learner to be initialized.\n    sleep_ms(100);\n\n    must_get_equal(&cluster.get_engine(1), b\"k1\", b\"v1\");\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    // wait election timeout\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 2,\n    ));\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // promote the learner first and remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_add_peer(region.get_id(), find_peer(&region, 1).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), None);\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n    cluster.must_transfer_leader(region.get_id(), find_peer(&region, 1).unwrap().clone());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_on_hibernated_leader", "test": "fn test_force_leader_on_hibernated_leader() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store1 = find_peer(&region, 1).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // wait a while to hibernate\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 3,\n    ));\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), None);\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_on_hibernated_follower", "test": "fn test_force_leader_on_hibernated_follower() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    // wait a while to hibernate\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 3,\n    ));\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), None);\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_with_uncommitted_conf_change", "test": "fn test_force_leader_with_uncommitted_conf_change() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(10);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 10;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(90);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store1 = find_peer(&region, 1).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    // an uncommitted conf-change\n    let cmd = new_change_peer_request(\n        ConfChangeType::RemoveNode,\n        find_peer(&region, 2).unwrap().clone(),\n    );\n    let req = new_admin_request(region.get_id(), region.get_region_epoch(), cmd);\n    cluster\n        .call_command_on_leader(req, Duration::from_millis(10))\n        .unwrap_err();\n\n    // wait election timeout\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 2,\n    ));\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // the uncommitted conf-change is committed successfully after being force\n    // leader\n    cluster\n        .pd_client\n        .must_none_peer(region.get_id(), find_peer(&region, 2).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k2\"), Some(b\"v2\".to_vec()));\n    assert_eq!(cluster.must_get(b\"k3\"), None);\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_on_healthy_region", "test": "fn test_force_leader_on_healthy_region() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(30);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 5;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    // try to enter force leader, it can't succeed due to quorum isn't lost\n    cluster.enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // make sure it leaves pre force leader state.\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 3,\n    ));\n    // put and get can propose successfully.\n    assert_eq!(cluster.must_get(b\"k1\"), Some(b\"v1\".to_vec()));\n    cluster.must_put(b\"k2\", b\"v2\");\n\n    // try to exit force leader, it will be ignored silently as it's not in the\n    // force leader state\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_twice_on_different_peers", "test": "fn test_force_leader_twice_on_different_peers() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    // restart to clean lease\n    cluster.stop_node(1);\n    cluster.run_node(1).unwrap();\n    cluster.stop_node(2);\n    cluster.run_node(2).unwrap();\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // enter force leader on a different peer\n    cluster.enter_force_leader(region.get_id(), 2, vec![3, 4, 5]);\n    assert_eq!(\n        cluster.leader_of_region(region.get_id()).unwrap(),\n        *find_peer(&region, 1).unwrap()\n    );\n\n    let conf_change = new_change_peer_request(ConfChangeType::RemoveNode, new_peer(3, 3));\n    let mut req = new_admin_request(region.get_id(), region.get_region_epoch(), conf_change);\n    req.mut_header()\n        .set_peer(find_peer(&region, 2).unwrap().clone());\n    let resp = cluster\n        .call_command(req, Duration::from_millis(10))\n        .unwrap();\n    let mut not_leader = kvproto::errorpb::NotLeader {\n        region_id: region.get_id(),\n        ..Default::default()\n    };\n    not_leader.set_leader(find_peer(&region, 1).unwrap().clone());\n    assert_eq!(resp.get_header().get_error().get_not_leader(), &not_leader,);\n\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/service.rs::get_id", "code": "pub fn get_id(&self) -> ConnId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_twice_on_same_peer", "test": "fn test_force_leader_twice_on_same_peer() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    // restart to clean lease\n    cluster.stop_node(1);\n    cluster.run_node(1).unwrap();\n    cluster.stop_node(2);\n    cluster.run_node(2).unwrap();\n\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    cluster.must_enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_force_leader_multiple_election_rounds", "test": "fn test_force_leader_multiple_election_rounds() {\n    let mut cluster = new_node_cluster(0, 5);\n    cluster.cfg.raft_store.raft_base_tick_interval = ReadableDuration::millis(30);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 5;\n    cluster.cfg.raft_store.raft_store_max_leader_lease = ReadableDuration::millis(40);\n    cluster.pd_client.disable_default_operator();\n\n    cluster.run();\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = cluster.get_region(b\"k1\");\n    cluster.must_split(&region, b\"k9\");\n    let region = cluster.get_region(b\"k2\");\n    let peer_on_store5 = find_peer(&region, 5).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store5.clone());\n\n    cluster.stop_node(3);\n    cluster.stop_node(4);\n    cluster.stop_node(5);\n\n    cluster.add_send_filter(IsolationFilterFactory::new(1));\n    cluster.add_send_filter(IsolationFilterFactory::new(2));\n\n    // wait election timeout\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 2,\n    ));\n    cluster.enter_force_leader(region.get_id(), 1, vec![3, 4, 5]);\n    // wait multiple election rounds\n    std::thread::sleep(Duration::from_millis(\n        cluster.cfg.raft_store.raft_election_timeout_ticks as u64\n            * cluster.cfg.raft_store.raft_base_tick_interval.as_millis()\n            * 6,\n    ));\n\n    cluster.clear_send_filters();\n    // remove the peers on failed nodes\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 3).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 4).unwrap().clone());\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), find_peer(&region, 5).unwrap().clone());\n    cluster.exit_force_leader(region.get_id(), 1);\n\n    // quorum is formed, can propose command successfully now\n    cluster.must_put(b\"k4\", b\"v4\");\n    assert_eq!(cluster.must_get(b\"k4\"), Some(b\"v4\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_unsafe_recovery.rs::test_unsafe_recovery_during_merge", "test": "fn test_unsafe_recovery_during_merge() {\n    let mut cluster = new_node_cluster(0, 3);\n    configure_for_merge(&mut cluster.cfg);\n\n    cluster.run();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k3\", b\"v3\");\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n    let region = pd_client.get_region(b\"k1\").unwrap();\n    cluster.must_split(&region, b\"k2\");\n\n    let left = pd_client.get_region(b\"k1\").unwrap();\n    let right = pd_client.get_region(b\"k3\").unwrap();\n\n    let left_on_store1 = find_peer(&left, 1).unwrap();\n    cluster.must_transfer_leader(left.get_id(), left_on_store1.clone());\n    let right_on_store1 = find_peer(&right, 1).unwrap();\n    cluster.must_transfer_leader(right.get_id(), right_on_store1.clone());\n\n    // Blocks the replication of prepare merge message, so that the commit merge\n    // back fills it in CatchUpLogs.\n    let append_filter = Box::new(\n        RegionPacketFilter::new(left.get_id(), 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend),\n    );\n    // Blocks the target region from receiving MsgAppendResponse, so that the commit\n    // merge message will only be replicated but not committed.\n    let commit_filter = Box::new(\n        RegionPacketFilter::new(right.get_id(), 1)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppendResponse),\n    );\n    cluster.sim.wl().add_recv_filter(1, append_filter);\n    cluster.sim.wl().add_recv_filter(1, commit_filter);\n\n    pd_client.merge_region(left.get_id(), right.get_id());\n    // Wait until the commit merge is proposed.\n    sleep_ms(300);\n\n    cluster.stop_node(1);\n    cluster.stop_node(3);\n    confirm_quorum_is_lost(&mut cluster, &region);\n\n    let report = cluster.must_enter_force_leader(right.get_id(), 2, vec![1, 3]);\n    assert_eq!(report.get_peer_reports().len(), 1);\n    let peer_report = &report.get_peer_reports()[0];\n    assert_eq!(peer_report.get_has_commit_merge(), false);\n    let region = peer_report.get_region_state().get_region();\n    assert_eq!(region.get_id(), right.get_id());\n    assert_eq!(region.get_start_key().len(), 0);\n    assert_eq!(region.get_end_key().len(), 0);\n\n    let to_be_removed: Vec<metapb::Peer> = right\n        .get_peers()\n        .iter()\n        .filter(|&peer| peer.get_store_id() != 2)\n        .cloned()\n        .collect();\n    let mut plan = pdpb::RecoveryPlan::default();\n    let mut demote = pdpb::DemoteFailedVoters::default();\n    demote.set_region_id(right.get_id());\n    demote.set_failed_voters(to_be_removed.into());\n    plan.mut_demotes().push(demote);\n    pd_client.must_set_unsafe_recovery_plan(2, plan);\n    cluster.must_send_store_heartbeat(2);\n\n    let mut demoted = true;\n    for _ in 0..10 {\n        let region = block_on(pd_client.get_region_by_id(right.get_id()))\n            .unwrap()\n            .unwrap();\n\n        demoted = true;\n        for peer in region.get_peers() {\n            if peer.get_id() != 2 && peer.get_role() == metapb::PeerRole::Voter {\n                demoted = false;\n            }\n        }\n        if demoted {\n            break;\n        }\n        sleep_ms(200);\n    }\n    assert!(demoted);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_v1_v2_mixed.rs::test_v1_simple_write", "test": "fn test_v1_simple_write() {\n    let mut cluster_v2 = test_raftstore_v2::new_node_cluster(1, 2);\n    let mut cluster_v1 = test_raftstore::new_node_cluster(1, 2);\n    cluster_v1.cfg.tikv.raft_store.enable_v2_compatible_learner = true;\n    cluster_v1.pd_client.disable_default_operator();\n    cluster_v2.pd_client.disable_default_operator();\n    let r11 = cluster_v1.run_conf_change();\n    let r21 = cluster_v2.run_conf_change();\n\n    cluster_v1.must_put(b\"k0\", b\"v0\");\n    cluster_v2.must_put(b\"k0\", b\"v0\");\n    cluster_v1\n        .pd_client\n        .must_add_peer(r11, new_learner_peer(2, 10));\n    cluster_v2\n        .pd_client\n        .must_add_peer(r21, new_learner_peer(2, 10));\n    check_key_in_engine(&cluster_v1.get_engine(2), b\"zk0\", b\"v0\");\n    check_key_in_engine(&cluster_v2.get_engine(2), b\"zk0\", b\"v0\");\n    let trans1 = Mutex::new(cluster_v1.sim.read().unwrap().get_router(2).unwrap());\n    let trans2 = Mutex::new(cluster_v2.sim.read().unwrap().get_router(1).unwrap());\n\n    let factory1 = ForwardFactory {\n        node_id: 1,\n        chain_send: Arc::new(move |m| {\n            info!(\"send to trans2\"; \"msg\" => ?m);\n            let _ = trans2.lock().unwrap().send_raft_message(Box::new(m));\n        }),\n    };\n    cluster_v1.add_send_filter(factory1);\n    let factory2 = ForwardFactory {\n        node_id: 2,\n        chain_send: Arc::new(move |m| {\n            info!(\"send to trans1\"; \"msg\" => ?m);\n            let _ = trans1.lock().unwrap().send_raft_message(m);\n        }),\n    };\n    cluster_v2.add_send_filter(factory2);\n    let filter11 = Box::new(\n        RegionPacketFilter::new(r11, 2)\n            .direction(Direction::Recv)\n            .msg_type(MessageType::MsgAppend)\n            .msg_type(MessageType::MsgAppendResponse)\n            .msg_type(MessageType::MsgSnapshot)\n            .msg_type(MessageType::MsgHeartbeat)\n            .msg_type(MessageType::MsgHeartbeatResponse),\n    );\n    cluster_v1.add_recv_filter_on_node(2, filter11);\n\n    cluster_v2.must_put(b\"k1\", b\"v1\");\n    assert_eq!(\n        cluster_v2.must_get(b\"k1\").unwrap(),\n        \"v1\".as_bytes().to_vec()\n    );\n    check_key_in_engine(&cluster_v1.get_engine(2), b\"zk1\", b\"v1\");\n\n    cluster_v1.shutdown();\n    cluster_v2.shutdown();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_split_merge", "test": "fn test_witness_split_merge() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n    let before = cluster\n        .apply_state(region.get_id(), nodes[2])\n        .get_applied_index();\n    cluster.must_put(b\"k1\", b\"v1\");\n    cluster.must_put(b\"k2\", b\"v2\");\n    cluster.must_split(&region, b\"k2\");\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    must_get_none(&cluster.get_engine(3), b\"k2\");\n    // applied index of witness is updated\n    let after = cluster\n        .apply_state(region.get_id(), nodes[2])\n        .get_applied_index();\n    assert!(after - before >= 3);\n\n    // the newly split peer should be witness as well\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k2\");\n    assert_ne!(left.get_id(), right.get_id());\n    assert!(find_peer(&left, nodes[2]).unwrap().is_witness);\n    assert!(find_peer(&right, nodes[2]).unwrap().is_witness);\n\n    // merge\n    pd_client.must_merge(left.get_id(), right.get_id());\n    let after_merge = cluster.get_region(b\"k1\");\n    assert!(find_peer(&after_merge, nodes[2]).unwrap().is_witness);\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    must_get_none(&cluster.get_engine(3), b\"k2\");\n    // epoch of witness is updated\n    assert_eq!(\n        cluster\n            .region_local_state(after_merge.get_id(), nodes[2])\n            .get_region()\n            .get_region_epoch(),\n        after_merge.get_region_epoch()\n    );\n\n    // split again\n    cluster.must_split(&after_merge, b\"k2\");\n    let left = cluster.get_region(b\"k1\");\n    let right = cluster.get_region(b\"k2\");\n    assert!(find_peer(&left, nodes[2]).unwrap().is_witness);\n    assert!(find_peer(&right, nodes[2]).unwrap().is_witness);\n\n    // can't merge with different witness location\n    let peer_on_store3 = find_peer(&left, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        left.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![false],\n    );\n    let left = cluster.get_region(b\"k1\");\n    let req = new_admin_request(\n        left.get_id(),\n        left.get_region_epoch(),\n        new_prepare_merge(right),\n    );\n    let resp = cluster\n        .call_command_on_leader(req, Duration::from_millis(100))\n        .unwrap();\n    assert!(\n        resp.get_header()\n            .get_error()\n            .get_message()\n            .contains(\"peers doesn't match\")\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_conf_change", "test": "fn test_witness_conf_change() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // can't switch witness by conf change\n    let mut peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    let mut peer = peer_on_store3.clone();\n    peer.set_is_witness(true);\n    let mut cp = ChangePeerRequest::default();\n    cp.set_change_type(ConfChangeType::AddLearnerNode);\n    cp.set_peer(peer);\n    let req = new_admin_request(\n        region.get_id(),\n        region.get_region_epoch(),\n        new_change_peer_v2_request(vec![cp]),\n    );\n    let resp = cluster\n        .call_command_on_leader(req, Duration::from_millis(100))\n        .unwrap();\n    assert!(resp.get_header().has_error());\n\n    // add a new witness peer\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store3.clone());\n    peer_on_store3.set_is_witness(true);\n    let applied_index = cluster.apply_state(1, 2).applied_index;\n    cluster\n        .pd_client\n        .must_add_peer(region.get_id(), peer_on_store3.clone());\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n    let region = cluster.get_region(b\"k1\");\n    cluster.wait_applied_index(region.get_id(), nodes[2], applied_index + 1);\n    assert_eq!(\n        cluster\n            .region_local_state(region.get_id(), nodes[2])\n            .get_region(),\n        &region\n    );\n\n    // remove a witness peer\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster\n        .pd_client\n        .must_remove_peer(region.get_id(), peer_on_store3);\n\n    std::thread::sleep(Duration::from_millis(10));\n\n    assert_eq!(\n        cluster\n            .region_local_state(region.get_id(), nodes[2])\n            .get_state(),\n        PeerState::Tombstone\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_switch_witness", "test": "fn test_witness_switch_witness() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    std::thread::sleep(Duration::from_millis(100));\n    must_get_none(&cluster.get_engine(3), b\"k1\");\n\n    // witness -> non-witness\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![false],\n    );\n\n    std::thread::sleep(Duration::from_millis(100));\n    must_get_equal(&cluster.get_engine(3), b\"k1\", b\"v1\");\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_leader", "test": "fn test_witness_leader() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // can't make leader to witness\n    cluster\n        .pd_client\n        .switch_witnesses(region.get_id(), vec![peer_on_store1.get_id()], vec![true]);\n\n    std::thread::sleep(Duration::from_millis(100));\n    assert_eq!(\n        cluster.leader_of_region(region.get_id()).unwrap().store_id,\n        1\n    );\n    // leader changes to witness failed, so still can get the value\n    must_get_equal(&cluster.get_engine(nodes[0]), b\"k1\", b\"v1\");\n\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    // can't transfer leader to witness\n    cluster.transfer_leader(region.get_id(), peer_on_store3);\n    assert_eq!(\n        cluster.leader_of_region(region.get_id()).unwrap().store_id,\n        nodes[0],\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_election_priority", "test": "fn test_witness_election_priority() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    // make sure logs are replicated to the witness\n    std::thread::sleep(Duration::from_millis(100));\n\n    for i in 1..10 {\n        let node = cluster.leader_of_region(region.get_id()).unwrap().store_id;\n        cluster.stop_node(node);\n        let (k, v) = (format!(\"k{}\", i), format!(\"v{}\", i));\n        let key = k.as_bytes();\n        let value = v.as_bytes();\n        cluster.must_put(key, value);\n        // the witness can't be elected as the leader when there is no log gap\n        assert_ne!(\n            cluster.leader_of_region(region.get_id()).unwrap().store_id,\n            nodes[2],\n        );\n        cluster.run_node(node).unwrap();\n        // make sure logs are replicated to the restarted node\n        std::thread::sleep(Duration::from_millis(100));\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_raftlog_gc_lagged_follower", "test": "fn test_witness_raftlog_gc_lagged_follower() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(100);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(200));\n    let mut before_states = HashMap::default();\n    for (&id, engines) in &cluster.engines {\n        let mut state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        before_states.insert(id, state.take_truncated_state());\n    }\n\n    // one follower is down\n    cluster.stop_node(nodes[1]);\n\n    // write some data to make log gap exceeds the gc limit\n    for i in 1..1000 {\n        let (k, v) = (format!(\"k{}\", i), format!(\"v{}\", i));\n        let key = k.as_bytes();\n        let value = v.as_bytes();\n        cluster.must_put(key, value);\n    }\n\n    // the witness truncated index is not advanced\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        if id == 2 {\n            assert_eq!(\n                state.get_truncated_state().get_index() - before_states[&id].get_index(),\n                0\n            );\n        } else {\n            assert_ne!(\n                900,\n                state.get_truncated_state().get_index() - before_states[&id].get_index()\n            );\n        }\n    }\n\n    // the follower is back online\n    cluster.run_node(nodes[1]).unwrap();\n    cluster.must_put(b\"k00\", b\"v00\");\n    must_get_equal(&cluster.get_engine(nodes[1]), b\"k00\", b\"v00\");\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(300));\n\n    // the truncated index is advanced now, as all the peers has replicated\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        assert_ne!(\n            900,\n            state.get_truncated_state().get_index() - before_states[&id].get_index()\n        );\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_raftlog_gc_lagged_witness", "test": "fn test_witness_raftlog_gc_lagged_witness() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_log_gc_count_limit = Some(100);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    // make sure raft log gc is triggered\n    std::thread::sleep(Duration::from_millis(200));\n    let mut before_states = HashMap::default();\n    for (&id, engines) in &cluster.engines {\n        let mut state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        before_states.insert(id, state.take_truncated_state());\n    }\n\n    // the witness is down\n    cluster.stop_node(nodes[2]);\n\n    // write some data to make log gap exceeds the gc limit\n    for i in 1..1000 {\n        let (k, v) = (format!(\"k{}\", i), format!(\"v{}\", i));\n        let key = k.as_bytes();\n        let value = v.as_bytes();\n        cluster.must_put(key, value);\n    }\n\n    // the witness is back online\n    cluster.run_node(nodes[2]).unwrap();\n\n    cluster.must_put(b\"k00\", b\"v00\");\n    std::thread::sleep(Duration::from_millis(200));\n\n    // the truncated index is advanced\n    for (&id, engines) in &cluster.engines {\n        let state: RaftApplyState = get_raft_msg_or_default(engines, &keys::apply_state_key(1));\n        assert_ne!(\n            900,\n            state.get_truncated_state().get_index() - before_states[&id].get_index()\n        );\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_replica_read", "test": "fn test_witness_replica_read() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    // make sure the peer_on_store3 has completed applied to witness\n    std::thread::sleep(Duration::from_millis(200));\n\n    let mut request = new_request(\n        region.get_id(),\n        region.get_region_epoch().clone(),\n        vec![new_get_cmd(b\"k0\")],\n        false,\n    );\n    request.mut_header().set_peer(peer_on_store3);\n    request.mut_header().set_replica_read(true);\n\n    let resp = cluster\n        .read(None, request, Duration::from_millis(100))\n        .unwrap();\n    assert_eq!(\n        resp.get_header().get_error().get_is_witness(),\n        &kvproto::errorpb::IsWitness {\n            region_id: region.get_id(),\n            ..Default::default()\n        }\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_leader_down", "test": "fn test_witness_leader_down() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k0\", b\"v0\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1);\n\n    let peer_on_store2 = find_peer(&region, nodes[1]).unwrap().clone();\n    // nonwitness -> witness\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store2.get_id()],\n        vec![true],\n    );\n\n    // the other follower is isolated\n    cluster.add_send_filter(IsolationFilterFactory::new(3));\n    for i in 1..10 {\n        cluster.must_put(format!(\"k{}\", i).as_bytes(), format!(\"v{}\", i).as_bytes());\n    }\n    // the leader is down\n    cluster.stop_node(1);\n\n    // witness would help to replicate the logs\n    cluster.clear_send_filters();\n\n    // forbid writes\n    let put = new_put_cmd(b\"k3\", b\"v3\");\n    must_get_error_is_witness(&mut cluster, &region, put);\n    // forbid reads\n    let get = new_get_cmd(b\"k1\");\n    must_get_error_is_witness(&mut cluster, &region, get);\n    // forbid read index\n    let read_index = new_read_index_cmd();\n    must_get_error_is_witness(&mut cluster, &region, read_index);\n\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store3);\n    cluster.must_put(b\"k1\", b\"v1\");\n    assert_eq!(\n        cluster.leader_of_region(region.get_id()).unwrap().store_id,\n        nodes[2],\n    );\n    assert_eq!(cluster.must_get(b\"k9\"), Some(b\"v9\".to_vec()));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/service.rs::get_id", "code": "pub fn get_id(&self) -> ConnId {\n        self.id\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/raftstore/test_witness.rs::test_witness_ignore_consistency_check", "test": "fn test_witness_ignore_consistency_check() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.cfg.raft_store.raft_election_timeout_ticks = 50;\n    // disable compact log to make test more stable.\n    cluster.cfg.raft_store.raft_log_gc_threshold = 1000;\n    cluster.cfg.raft_store.consistency_check_interval = ReadableDuration::secs(1);\n    cluster.run();\n\n    let nodes = Vec::from_iter(cluster.get_node_ids());\n    assert_eq!(nodes.len(), 3);\n\n    let pd_client = Arc::clone(&cluster.pd_client);\n    pd_client.disable_default_operator();\n\n    cluster.must_put(b\"k1\", b\"v1\");\n\n    let region = block_on(pd_client.get_region_by_id(1)).unwrap().unwrap();\n    let peer_on_store1 = find_peer(&region, nodes[0]).unwrap();\n    cluster.must_transfer_leader(region.get_id(), peer_on_store1.clone());\n\n    // nonwitness -> witness\n    let peer_on_store3 = find_peer(&region, nodes[2]).unwrap().clone();\n    cluster.pd_client.must_switch_witnesses(\n        region.get_id(),\n        vec![peer_on_store3.get_id()],\n        vec![true],\n    );\n\n    // make sure the peer_on_store3 has completed applied to witness\n    std::thread::sleep(Duration::from_millis(200));\n\n    for i in 0..300 {\n        cluster.must_put(\n            format!(\"k{:06}\", i).as_bytes(),\n            format!(\"k{:06}\", i).as_bytes(),\n        );\n        std::thread::sleep(Duration::from_millis(10));\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/backup-stream/src/router.rs::len", "code": "pub fn len(&self) -> usize {\n        self.events.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_cpu.rs::test_reschedule_coprocessor", "test": "pub fn test_reschedule_coprocessor() {\n    let tag = \"tag_coprocessor\";\n\n    let (test_suite, mut store, endpoint) = setup_test_suite();\n    fail::cfg(\"copr_reschedule\", \"return\").unwrap();\n    fail::cfg_callback(\"scanner_next\", || cpu_load(Duration::from_millis(100))).unwrap();\n    defer!({\n        fail::remove(\"scanner_next\");\n        fail::remove(\"copr_reschedule\");\n    });\n\n    let jh = test_suite\n        .rt\n        .spawn(require_cpu_time_not_zero(&test_suite, tag));\n\n    let table = ProductTable::new();\n    let insert = prepare_insert(&mut store, &table);\n    insert.execute();\n    store.commit();\n\n    let mut req = DagSelect::from(&table).build();\n    let mut ctx = Context::default();\n    ctx.set_resource_group_tag(tag.as_bytes().to_vec());\n    req.set_context(ctx);\n    assert!(\n        !block_on(endpoint.parse_and_handle_unary_request(req, None))\n            .consume()\n            .get_data()\n            .is_empty()\n    );\n\n    assert!(block_on(jh).unwrap());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/cdc/src/endpoint.rs::is_empty", "code": "fn is_empty(&self) -> bool {\n        self.heap.is_empty()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_cpu.rs::test_get", "test": "fn test_get() {\n    let tag = \"tag_get\";\n\n    let (test_suite, mut store, _) = setup_test_suite();\n    fail::cfg_callback(\"point_getter_get\", || cpu_load(Duration::from_millis(100))).unwrap();\n    defer!(fail::remove(\"point_getter_get\"));\n\n    let jh = test_suite\n        .rt\n        .spawn(require_cpu_time_not_zero(&test_suite, tag));\n\n    let table = ProductTable::new();\n    let insert = prepare_insert(&mut store, &table);\n    insert.execute();\n    store.commit();\n\n    let storage = store.get_storage();\n    for (k, v) in store.export() {\n        let mut ctx = Context::default();\n        ctx.set_resource_group_tag(tag.as_bytes().to_vec());\n        assert_eq!(\n            storage\n                .get(ctx, &Key::from_raw(&k), TimeStamp::max())\n                .unwrap()\n                .0\n                .unwrap()\n                .as_slice(),\n            v.as_slice()\n        );\n    }\n\n    assert!(block_on(jh).unwrap());\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/encryption/src/master_key/metadata.rs::as_slice", "code": "pub fn as_slice(self) -> &'static [u8] {\n        match self {\n            MetadataMethod::Plaintext => METADATA_METHOD_PLAINTEXT,\n            MetadataMethod::Aes256Gcm => METADATA_METHOD_AES256_GCM,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_dynamic_config.rs::test_enable", "test": "pub fn test_enable() {\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        receiver_address: \"\".to_string(),\n        report_receiver_interval: ReadableDuration::millis(2500),\n        max_resource_groups: 5000,\n        precision: ReadableDuration::secs(1),\n    });\n\n    let port = alloc_port();\n    test_suite.start_receiver_at(port);\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n\n    // | Address |\n    // |   x     |\n    sleep(Duration::from_millis(3000));\n    assert!(test_suite.nonblock_receiver_all().is_empty());\n\n    // | Address |\n    // |   o     |\n    test_suite.cfg_receiver_address(format!(\"127.0.0.1:{}\", port));\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n\n    // | Address |\n    // |   x     |\n    test_suite.cfg_receiver_address(\"\");\n    test_suite.flush_receiver();\n    sleep(Duration::from_millis(3000));\n    assert!(test_suite.nonblock_receiver_all().is_empty());\n\n    // | Address |\n    // |   o     |\n    test_suite.cfg_receiver_address(format!(\"127.0.0.1:{}\", port));\n    let res = test_suite.block_receive_one();\n    assert!(res.contains_key(\"req-1\"));\n    assert!(res.contains_key(\"req-2\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tikv_util/src/buffer_vec.rs::is_empty", "code": "pub fn is_empty(&self) -> bool {\n        self.len() == 0\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_pubsub.rs::test_basic", "test": "pub fn test_basic() {\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        report_receiver_interval: ReadableDuration::secs(3),\n        precision: ReadableDuration::secs(1),\n        ..Default::default()\n    });\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n\n    let (_client, stream) = test_suite.subscribe();\n    let tags = stream.take(4).map(|record| {\n        String::from_utf8_lossy(record.unwrap().get_record().get_resource_group_tag()).into_owned()\n    });\n    let res = block_on(tags.collect::<HashSet<_>>());\n\n    assert!(res.contains(\"req-1\"));\n    assert!(res.contains(\"req-2\"));\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/resource_metering/test_pubsub.rs::test_multiple_subscribers", "test": "pub fn test_multiple_subscribers() {\n    let mut test_suite = TestSuite::new(resource_metering::Config {\n        report_receiver_interval: ReadableDuration::secs(3),\n        precision: ReadableDuration::secs(1),\n        ..Default::default()\n    });\n\n    // Workload\n    // [req-1, req-2]\n    test_suite.setup_workload(vec![\"req-1\", \"req-2\"]);\n    let jhs: Vec<_> = (0..3)\n        .map(|_| {\n            let (client, stream) = test_suite.subscribe();\n            test_suite.rt.spawn(async move {\n                let _client = client;\n                let tags = stream.take(4).map(|record| {\n                    String::from_utf8_lossy(record.unwrap().get_record().get_resource_group_tag())\n                        .into_owned()\n                });\n                tags.collect::<HashSet<_>>().await\n            })\n        })\n        .collect();\n\n    for jh in jhs {\n        let res = test_suite.rt.block_on(jh).unwrap();\n        assert!(res.contains(\"req-1\"));\n        assert!(res.contains(\"req-2\"));\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/gc_worker.rs::test_gc_bypass_raft", "test": "fn test_gc_bypass_raft() {\n    let (cluster, leader, ctx) = must_new_cluster_mul(2);\n    cluster.pd_client.disable_default_operator();\n\n    let env = Arc::new(Environment::new(1));\n    let leader_store = leader.get_store_id();\n    let channel = ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader_store));\n    let client = TikvClient::new(channel);\n\n    let pk = b\"k1\".to_vec();\n    let value = vec![b'x'; 300];\n    let engine = cluster.engines.get(&leader_store).unwrap();\n\n    for &start_ts in &[10, 20, 30, 40] {\n        let commit_ts = start_ts + 5;\n        let muts = vec![new_mutation(Op::Put, b\"k1\", &value)];\n\n        must_kv_prewrite(&client, ctx.clone(), muts, pk.clone(), start_ts);\n        let keys = vec![pk.clone()];\n        must_kv_commit(&client, ctx.clone(), keys, start_ts, commit_ts, commit_ts);\n\n        let key = Key::from_raw(b\"k1\").append_ts(start_ts.into());\n        let key = data_key(key.as_encoded());\n        assert!(engine.kv.get_value(&key).unwrap().is_some());\n\n        let key = Key::from_raw(b\"k1\").append_ts(commit_ts.into());\n        let key = data_key(key.as_encoded());\n        assert!(engine.kv.get_value_cf(CF_WRITE, &key).unwrap().is_some());\n    }\n\n    let node_ids = cluster.get_node_ids();\n    for store_id in node_ids {\n        let gc_sched = cluster.sim.rl().get_gc_worker(store_id).scheduler();\n\n        let mut region = cluster.get_region(b\"a\");\n        region.set_start_key(b\"k1\".to_vec());\n        region.set_end_key(b\"k2\".to_vec());\n        sync_gc(&gc_sched, region, 200.into()).unwrap();\n\n        let engine = cluster.engines.get(&store_id).unwrap();\n        for &start_ts in &[10, 20, 30] {\n            let commit_ts = start_ts + 5;\n            let key = Key::from_raw(b\"k1\").append_ts(start_ts.into());\n            let key = data_key(key.as_encoded());\n            assert!(engine.kv.get_value(&key).unwrap().is_none());\n\n            let key = Key::from_raw(b\"k1\").append_ts(commit_ts.into());\n            let key = data_key(key.as_encoded());\n            assert!(engine.kv.get_value_cf(CF_WRITE, &key).unwrap().is_none());\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/ttl_properties.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        self.max_expire_ts.is_some() || self.min_expire_ts.is_some()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/kv_service.rs::test_debug_region_info", "test": "fn test_debug_region_info() {\n    let (cluster, debug_client, store_id) = must_new_cluster_and_debug_client();\n\n    let raft_engine = cluster.get_raft_engine(store_id);\n    let kv_engine = cluster.get_engine(store_id);\n\n    let region_id = 100;\n    let mut raft_state = raft_serverpb::RaftLocalState::default();\n    raft_state.set_last_index(42);\n    let mut lb = raft_engine.log_batch(0);\n    lb.put_raft_state(region_id, &raft_state).unwrap();\n    raft_engine.consume(&mut lb, false).unwrap();\n    assert_eq!(\n        raft_engine.get_raft_state(region_id).unwrap().unwrap(),\n        raft_state\n    );\n\n    let apply_state_key = keys::apply_state_key(region_id);\n    let mut apply_state = raft_serverpb::RaftApplyState::default();\n    apply_state.set_applied_index(42);\n    kv_engine\n        .put_msg_cf(CF_RAFT, &apply_state_key, &apply_state)\n        .unwrap();\n    assert_eq!(\n        kv_engine\n            .get_msg_cf::<raft_serverpb::RaftApplyState>(CF_RAFT, &apply_state_key)\n            .unwrap()\n            .unwrap(),\n        apply_state\n    );\n\n    let region_state_key = keys::region_state_key(region_id);\n    let mut region_state = raft_serverpb::RegionLocalState::default();\n    region_state.set_state(raft_serverpb::PeerState::Tombstone);\n    kv_engine\n        .put_msg_cf(CF_RAFT, &region_state_key, &region_state)\n        .unwrap();\n    assert_eq!(\n        kv_engine\n            .get_msg_cf::<raft_serverpb::RegionLocalState>(CF_RAFT, &region_state_key)\n            .unwrap()\n            .unwrap(),\n        region_state\n    );\n\n    // Debug region_info\n    let mut req = debugpb::RegionInfoRequest::default();\n    req.set_region_id(region_id);\n    let mut resp = debug_client.region_info(&req).unwrap();\n    assert_eq!(resp.take_raft_local_state(), raft_state);\n    assert_eq!(resp.take_raft_apply_state(), apply_state);\n    assert_eq!(resp.take_region_local_state(), region_state);\n\n    req.set_region_id(region_id + 1);\n    match debug_client.region_info(&req).unwrap_err() {\n        Error::RpcFailure(status) => {\n            assert_eq!(status.code(), RpcStatusCode::NOT_FOUND);\n        }\n        _ => panic!(\"expect NotFound\"),\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/kv_service.rs::test_debug_region_info_v2", "test": "fn test_debug_region_info_v2() {\n    let (cluster, debug_client, store_id) = test_raftstore_v2::must_new_cluster_and_debug_client();\n\n    let raft_engine = cluster.get_raft_engine(store_id);\n    let region_id = 100;\n    let mut raft_state = raft_serverpb::RaftLocalState::default();\n    raft_state.set_last_index(42);\n    let mut lb = raft_engine.log_batch(10);\n    lb.put_raft_state(region_id, &raft_state).unwrap();\n\n    let mut apply_state = raft_serverpb::RaftApplyState::default();\n    apply_state.set_applied_index(42);\n    lb.put_apply_state(region_id, 42, &apply_state).unwrap();\n\n    let mut region_state = raft_serverpb::RegionLocalState::default();\n    region_state.set_state(raft_serverpb::PeerState::Tombstone);\n    lb.put_region_state(region_id, 42, &region_state).unwrap();\n\n    raft_engine.consume(&mut lb, false).unwrap();\n    assert_eq!(\n        raft_engine.get_raft_state(region_id).unwrap().unwrap(),\n        raft_state\n    );\n\n    assert_eq!(\n        raft_engine\n            .get_apply_state(region_id, u64::MAX)\n            .unwrap()\n            .unwrap(),\n        apply_state\n    );\n\n    assert_eq!(\n        raft_engine\n            .get_region_state(region_id, u64::MAX)\n            .unwrap()\n            .unwrap(),\n        region_state\n    );\n\n    // Debug region_info\n    let mut req = debugpb::RegionInfoRequest::default();\n    req.set_region_id(region_id);\n    let mut resp = debug_client.region_info(&req).unwrap();\n    assert_eq!(resp.take_raft_local_state(), raft_state);\n    assert_eq!(resp.take_raft_apply_state(), apply_state);\n    assert_eq!(resp.take_region_local_state(), region_state);\n\n    req.set_region_id(region_id + 1);\n    match debug_client.region_info(&req).unwrap_err() {\n        Error::RpcFailure(status) => {\n            assert_eq!(status.code(), RpcStatusCode::NOT_FOUND);\n        }\n        _ => panic!(\"expect NotFound\"),\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/mysql/decimal.rs::unwrap", "code": "pub fn unwrap(self) -> T {\n        match self {\n            Res::Ok(t) | Res::Truncated(t) | Res::Overflow(t) => t,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/kv_service.rs::test_debug_region_size", "test": "fn test_debug_region_size() {\n    let (cluster, debug_client, store_id) = must_new_cluster_and_debug_client();\n    let engine = cluster.get_engine(store_id);\n\n    // Put some data.\n    let region_id = 100;\n    let region_state_key = keys::region_state_key(region_id);\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    region.set_start_key(b\"a\".to_vec());\n    region.set_end_key(b\"z\".to_vec());\n    let mut state = RegionLocalState::default();\n    state.set_region(region);\n    engine\n        .put_msg_cf(CF_RAFT, &region_state_key, &state)\n        .unwrap();\n\n    let cfs = vec![CF_DEFAULT, CF_LOCK, CF_WRITE];\n    // At lease 8 bytes for the WRITE cf.\n    let (k, v) = (keys::data_key(b\"kkkk_kkkk\"), b\"v\");\n    for cf in &cfs {\n        engine.put_cf(cf, k.as_slice(), v).unwrap();\n    }\n\n    let mut req = debugpb::RegionSizeRequest::default();\n    req.set_region_id(region_id);\n    req.set_cfs(cfs.iter().map(|s| s.to_string()).collect());\n    let entries: Vec<_> = debug_client\n        .region_size(&req)\n        .unwrap()\n        .take_entries()\n        .into();\n    assert_eq!(entries.len(), 3);\n    for e in entries {\n        cfs.iter().find(|&&c| c == e.cf).unwrap();\n        assert!(e.size > 0);\n    }\n\n    req.set_region_id(region_id + 1);\n    match debug_client.region_size(&req).unwrap_err() {\n        Error::RpcFailure(status) => {\n            assert_eq!(status.code(), RpcStatusCode::NOT_FOUND);\n        }\n        _ => panic!(\"expect NotFound\"),\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/server/raft_client.rs::len", "code": "fn len(&self) -> usize {\n        self.buf.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/kv_service.rs::test_debug_region_size_v2", "test": "fn test_debug_region_size_v2() {\n    let (cluster, debug_client, store_id) = test_raftstore_v2::must_new_cluster_and_debug_client();\n    let raft_engine = cluster.get_raft_engine(store_id);\n    let engine = cluster.get_engine(store_id);\n\n    let mut lb = raft_engine.log_batch(10);\n    // Put some data.\n    let region_id = 1;\n    let mut region = metapb::Region::default();\n    region.set_id(region_id);\n    region.set_start_key(b\"a\".to_vec());\n    region.set_end_key(b\"z\".to_vec());\n    let mut state = RegionLocalState::default();\n    state.set_region(region);\n    state.set_tablet_index(5);\n    lb.put_region_state(region_id, 5, &state).unwrap();\n    raft_engine.consume(&mut lb, false).unwrap();\n\n    let cfs = vec![CF_DEFAULT, CF_LOCK, CF_WRITE];\n    // At lease 8 bytes for the WRITE cf.\n    let (k, v) = (keys::data_key(b\"kkkk_kkkk\"), b\"v\");\n    for cf in &cfs {\n        engine.put_cf(cf, k.as_slice(), v).unwrap();\n    }\n\n    let mut req = debugpb::RegionSizeRequest::default();\n    req.set_region_id(region_id);\n    req.set_cfs(cfs.iter().map(|s| s.to_string()).collect());\n    let entries: Vec<_> = debug_client\n        .region_size(&req)\n        .unwrap()\n        .take_entries()\n        .into();\n    assert_eq!(entries.len(), 3);\n    for e in entries {\n        cfs.iter().find(|&&c| c == e.cf).unwrap();\n        assert!(e.size > 0);\n    }\n\n    req.set_region_id(region_id + 1);\n    match debug_client.region_size(&req).unwrap_err() {\n        Error::RpcFailure(status) => {\n            assert_eq!(status.code(), RpcStatusCode::NOT_FOUND);\n        }\n        _ => panic!(\"expect NotFound\"),\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/server/raft_client.rs::len", "code": "fn len(&self) -> usize {\n        self.buf.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/kv_service.rs::test_double_run_node", "test": "fn test_double_run_node() {\n    let count = 1;\n    let mut cluster = new_node_cluster(0, count);\n    cluster.run();\n    let id = *cluster.engines.keys().next().unwrap();\n    let engines = cluster.engines.values().next().unwrap().clone();\n    let router = cluster.sim.rl().get_router(id).unwrap();\n    let mut sim = cluster.sim.wl();\n    let node = sim.get_node(id).unwrap();\n    let pd_worker = LazyWorker::new(\"test-pd-worker\");\n    let simulate_trans = SimulateTransport::new(ChannelTransport::new());\n    let tmp = Builder::new().prefix(\"test_cluster\").tempdir().unwrap();\n    let snap_mgr = SnapManager::new(tmp.path().to_str().unwrap());\n    let coprocessor_host = CoprocessorHost::new(router, raftstore::coprocessor::Config::default());\n    let importer = {\n        let dir = Path::new(MiscExt::path(&engines.kv)).join(\"import-sst\");\n        Arc::new(SstImporter::new(&ImportConfig::default(), dir, None, ApiVersion::V1).unwrap())\n    };\n    let (split_check_scheduler, _) = dummy_scheduler();\n\n    let store_meta = Arc::new(Mutex::new(StoreMeta::new(20)));\n    let e = node\n        .start(\n            engines,\n            simulate_trans,\n            snap_mgr,\n            pd_worker,\n            store_meta,\n            coprocessor_host,\n            importer,\n            split_check_scheduler,\n            AutoSplitController::default(),\n            ConcurrencyManager::new(1.into()),\n            CollectorRegHandle::new_for_test(),\n            None,\n            Arc::new(AtomicU64::new(0)),\n        )\n        .unwrap_err();\n    assert!(format!(\"{:?}\", e).contains(\"already started\"), \"{:?}\", e);\n    drop(sim);\n    cluster.shutdown();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/file_system/src/rate_limiter.rs::contains", "code": "pub fn contains(&self, op: IoOp) -> bool {\n        match *self {\n            IoRateLimitMode::WriteOnly => op == IoOp::Write,\n            IoRateLimitMode::ReadOnly => op == IoOp::Read,\n            _ => true,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/lock_manager.rs::test_detect_deadlock_when_updating_wait_info", "test": "fn test_detect_deadlock_when_updating_wait_info() {\n    use kvproto::kvrpcpb::PessimisticLockKeyResultType::*;\n    let mut cluster = new_cluster_for_deadlock_test(3);\n\n    let key1 = b\"key1\";\n    let key2 = b\"key2\";\n    let (client, ctx) = build_leader_client(&mut cluster, key1);\n    let client = Arc::new(client);\n\n    fn async_pessimistic_lock(\n        client: Arc<TikvClient>,\n        ctx: Context,\n        key: &[u8],\n        ts: u64,\n    ) -> mpsc::Receiver<PessimisticLockResponse> {\n        let (tx, rx) = mpsc::channel();\n        let key = vec![key.to_vec()];\n        thread::spawn(move || {\n            let resp =\n                kv_pessimistic_lock_resumable(&client, ctx, key, ts, ts, Some(1000), false, false);\n            tx.send(resp).unwrap();\n        });\n        rx\n    }\n\n    // key1: txn 11 and 12 waits for 10\n    // key2: txn 11 waits for 12\n    let resp = kv_pessimistic_lock_resumable(\n        &client,\n        ctx.clone(),\n        vec![key1.to_vec()],\n        10,\n        10,\n        Some(1000),\n        false,\n        false,\n    );\n    assert!(resp.region_error.is_none());\n    assert!(resp.errors.is_empty());\n    assert_eq!(resp.results[0].get_type(), LockResultNormal);\n    let resp = kv_pessimistic_lock_resumable(\n        &client,\n        ctx.clone(),\n        vec![key2.to_vec()],\n        12,\n        12,\n        Some(1000),\n        false,\n        false,\n    );\n    assert!(resp.region_error.is_none());\n    assert!(resp.errors.is_empty());\n    assert_eq!(resp.results[0].get_type(), LockResultNormal);\n    let rx_txn11_k1 = async_pessimistic_lock(client.clone(), ctx.clone(), key1, 11);\n    let rx_txn12_k1 = async_pessimistic_lock(client.clone(), ctx.clone(), key1, 12);\n    let rx_txn11_k2 = async_pessimistic_lock(client.clone(), ctx.clone(), key2, 11);\n    // All blocked.\n    assert_eq!(\n        rx_txn11_k1\n            .recv_timeout(Duration::from_millis(50))\n            .unwrap_err(),\n        RecvTimeoutError::Timeout\n    );\n    assert_eq!(rx_txn12_k1.try_recv().unwrap_err(), TryRecvError::Empty);\n    assert_eq!(rx_txn11_k2.try_recv().unwrap_err(), TryRecvError::Empty);\n\n    // Release lock at ts=10 on key1 so that txn 11 will be granted the lock.\n    must_kv_pessimistic_rollback(&client, ctx.clone(), key1.to_vec(), 10, 10);\n    let resp = rx_txn11_k1\n        .recv_timeout(Duration::from_millis(200))\n        .unwrap();\n    assert!(resp.region_error.is_none());\n    assert!(resp.errors.is_empty());\n    assert_eq!(resp.results[0].get_type(), LockResultNormal);\n    // And then 12 waits for k1 on key1, which forms a deadlock.\n    let resp = rx_txn12_k1\n        .recv_timeout(Duration::from_millis(1000))\n        .unwrap();\n    assert!(resp.region_error.is_none());\n    assert!(resp.errors[0].has_deadlock());\n    assert_eq!(resp.results[0].get_type(), LockResultFailed);\n    // Check correctness of the wait chain.\n    let wait_chain = resp.errors[0].get_deadlock().get_wait_chain();\n    assert_eq!(wait_chain[0].get_txn(), 11);\n    assert_eq!(wait_chain[0].get_wait_for_txn(), 12);\n    assert_eq!(wait_chain[0].get_key(), key2);\n    assert_eq!(wait_chain[1].get_txn(), 12);\n    assert_eq!(wait_chain[1].get_wait_for_txn(), 11);\n    assert_eq!(wait_chain[1].get_key(), key1);\n\n    // Clean up.\n    must_kv_pessimistic_rollback(&client, ctx.clone(), key1.to_vec(), 11, 11);\n    must_kv_pessimistic_rollback(&client, ctx.clone(), key2.to_vec(), 12, 12);\n    let resp = rx_txn11_k2\n        .recv_timeout(Duration::from_millis(500))\n        .unwrap();\n    assert!(resp.region_error.is_none());\n    assert!(resp.errors.is_empty());\n    assert_eq!(resp.results[0].get_type(), LockResultNormal);\n    must_kv_pessimistic_rollback(&client, ctx, key2.to_vec(), 11, 11);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore-v2/src/router/response_channel.rs::is_none", "code": "fn is_none(&self) -> bool {\n        false\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/raft_client.rs::test_batch_raft_fallback", "test": "fn test_batch_raft_fallback() {\n    let msg_count = Arc::new(AtomicUsize::new(0));\n    let batch_msg_count = Arc::new(AtomicUsize::new(0));\n    let service = MockKvForRaft::new(Arc::clone(&msg_count), Arc::clone(&batch_msg_count), false);\n    let (mock_server, port) = create_mock_server(service, 60000, 60100).unwrap();\n\n    let mut raft_client = get_raft_client_by_port(port);\n    (0..100).for_each(|_| {\n        raft_client.send(RaftMessage::default()).unwrap();\n        thread::sleep(time::Duration::from_millis(10));\n        raft_client.flush();\n    });\n\n    assert!(msg_count.load(Ordering::SeqCst) > 0);\n    assert_eq!(batch_msg_count.load(Ordering::SeqCst), 0);\n    drop(mock_server)\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/raft_client.rs::test_batch_size_limit", "test": "fn test_batch_size_limit() {\n    let msg_count = Arc::new(AtomicUsize::new(0));\n    let batch_msg_count = Arc::new(AtomicUsize::new(0));\n    let service = MockKvForRaft::new(Arc::clone(&msg_count), Arc::clone(&batch_msg_count), true);\n    let (mock_server, port) = create_mock_server(service, 60200, 60300).unwrap();\n\n    let mut raft_client = get_raft_client_by_port(port);\n\n    // `send` should success.\n    for _ in 0..10 {\n        // 5M per RaftMessage.\n        let mut raft_m = RaftMessage::default();\n        for _ in 0..(5 * 1024) {\n            let mut e = Entry::default();\n            e.set_data(vec![b'a'; 1024].into());\n            raft_m.mut_message().mut_entries().push(e);\n        }\n        raft_client.send(raft_m).unwrap();\n    }\n    raft_client.flush();\n\n    check_msg_count(500, &msg_count, 10);\n    // The final received message count should be 10 exactly.\n    drop(raft_client);\n    drop(mock_server);\n    assert_eq!(msg_count.load(Ordering::SeqCst), 10);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/raft_client.rs::test_batch_size_edge_limit", "test": "fn test_batch_size_edge_limit() {\n    let msg_count = Arc::new(AtomicUsize::new(0));\n    let batch_msg_count = Arc::new(AtomicUsize::new(0));\n    let service = MockKvForRaft::new(Arc::clone(&msg_count), Arc::clone(&batch_msg_count), true);\n    let (mock_server, port) = create_mock_server(service, 60200, 60300).unwrap();\n\n    let mut raft_client = get_raft_client_by_port(port);\n\n    // Put them in buffer so sibling messages will be likely be batched during\n    // sending.\n    let mut msgs = Vec::with_capacity(5);\n    for _ in 0..5 {\n        let mut raft_m = RaftMessage::default();\n        // Magic number, this can make estimated size about 4940000, hence two messages\n        // will be batched together, but the total size will be way larger than\n        // 10MiB as there are many indexes and terms.\n        for _ in 0..38000 {\n            let mut e = Entry::default();\n            e.set_term(1);\n            e.set_index(256);\n            e.set_data(vec![b'a'; 130].into());\n            raft_m.mut_message().mut_entries().push(e);\n        }\n        msgs.push(raft_m);\n    }\n    for m in msgs {\n        raft_client.send(m).unwrap();\n    }\n    raft_client.flush();\n\n    check_msg_count(10000, &msg_count, 5);\n    // The final received message count should be 5 exactly.\n    drop(raft_client);\n    drop(mock_server);\n    assert_eq!(msg_count.load(Ordering::SeqCst), 5);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/engine_traits/src/tablet.rs::load", "code": "pub fn load(&self, ctx: TabletContext, create: bool) -> Result<CachedTablet<EK>>\n    where\n        EK: Clone,\n    {\n        assert!(ctx.suffix.is_some());\n        let id = ctx.id;\n        let path = self.tablet_path(id, ctx.suffix.unwrap());\n        if !create && !self.tablets.factory.exists(&path) {\n            return Err(Error::Other(box_err!(\n                \"tablet ({}, {:?}) doesn't exist\",\n                id,\n                ctx.suffix\n            )));\n        }\n        // TODO: use compaction filter to trim range.\n        let tablet = self.tablets.factory.open_tablet(ctx, &path)?;\n        let mut cached = self.get_or_default(id);\n        cached.set(tablet);\n        Ok(cached)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/raft_client.rs::test_tombstone_block_list", "test": "fn test_tombstone_block_list() {\n    let pd_server = test_pd::Server::new(1);\n    let eps = pd_server.bind_addrs();\n    let pd_client = Arc::new(test_pd::util::new_client(eps, None));\n    let bg_worker = WorkerBuilder::new(thd_name!(\"background\"))\n        .thread_count(2)\n        .create();\n    let resolver = resolve::new_resolver(pd_client, &bg_worker, FakeExtension).0;\n\n    let msg_count = Arc::new(AtomicUsize::new(0));\n    let batch_msg_count = Arc::new(AtomicUsize::new(0));\n    let service = MockKvForRaft::new(Arc::clone(&msg_count), Arc::clone(&batch_msg_count), true);\n    let (_mock_server, port) = create_mock_server(service, 60200, 60300).unwrap();\n\n    let mut raft_client = get_raft_client(FakeExtension, resolver);\n\n    let mut store1 = metapb::Store::default();\n    store1.set_id(1);\n    store1.set_address(format!(\"127.0.0.1:{}\", port));\n    pd_server.default_handler().add_store(store1.clone());\n\n    // `send` should success.\n    for _ in 0..10 {\n        // 5M per RaftMessage.\n        let mut raft_m = RaftMessage::default();\n        raft_m.mut_to_peer().set_store_id(1);\n        for _ in 0..(5 * 1024) {\n            let mut e = Entry::default();\n            e.set_data(vec![b'a'; 1024].into());\n            raft_m.mut_message().mut_entries().push(e);\n        }\n        raft_client.send(raft_m).unwrap();\n    }\n    raft_client.flush();\n\n    check_msg_count(500, &msg_count, 10);\n\n    let mut store2 = metapb::Store::default();\n    store2.set_id(2);\n    store2.set_address(store1.get_address().to_owned());\n    store2.set_state(metapb::StoreState::Tombstone);\n    pd_server.default_handler().add_store(store2);\n    let mut message = RaftMessage::default();\n    message.mut_to_peer().set_store_id(2);\n    // First message should be OK.\n    raft_client.send(message.clone()).unwrap();\n    // Wait some time for the resolve result.\n    thread::sleep(time::Duration::from_millis(50));\n    // Second message should fail as the store should be added to block list.\n    assert_eq!(\n        DiscardReason::Disconnected,\n        raft_client.send(message).unwrap_err()\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/server/raft_client.rs::send", "code": "pub fn send(&mut self, msg: RaftMessage) -> result::Result<(), DiscardReason> {\n        let store_id = msg.get_to_peer().store_id;\n        let grpc_raft_conn_num = self.builder.cfg.value().grpc_raft_conn_num as u64;\n        let conn_id = if grpc_raft_conn_num == 1 {\n            0\n        } else {\n            if self.last_hash.0 == 0 || msg.region_id != self.last_hash.0 {\n                self.last_hash = (\n                    msg.region_id,\n                    seahash::hash(&msg.region_id.to_ne_bytes()) % grpc_raft_conn_num,\n                );\n            };\n            self.last_hash.1 as usize\n        };\n\n        #[allow(unused_mut)]\n        let mut transport_on_send_store_fp = || {\n            fail_point!(\n                \"transport_on_send_snapshot\",\n                msg.get_message().get_msg_type() == raft::eraftpb::MessageType::MsgSnapshot,\n                |sid| if let Some(sid) = sid {\n                    let sid: u64 = sid.parse().unwrap();\n                    if sid == store_id {\n                        // Forbid building new connections.\n                        fail::cfg(_ON_RESOLVE_FP, &format!(\"1*return({})\", sid)).unwrap();\n                        self.cache.remove(&(store_id, conn_id));\n                        self.pool\n                            .lock()\n                            .unwrap()\n                            .connections\n                            .remove(&(store_id, conn_id));\n                    }\n                }\n            )\n        };\n        transport_on_send_store_fp();\n        loop {\n            if let Some(s) = self.cache.get_mut(&(store_id, conn_id)) {\n                match s.queue.push(msg) {\n                    Ok(_) => {\n                        if !s.dirty {\n                            s.dirty = true;\n                            self.need_flush.push((store_id, conn_id));\n                        }\n                        return Ok(());\n                    }\n                    Err(DiscardReason::Full) => {\n                        s.queue.notify();\n                        s.dirty = false;\n                        if !s.full {\n                            s.full = true;\n                            self.full_stores.push((store_id, conn_id));\n                        }\n                        return Err(DiscardReason::Full);\n                    }\n                    Err(DiscardReason::Disconnected) => break,\n                    Err(DiscardReason::Paused) => return Err(DiscardReason::Paused),\n                    Err(DiscardReason::Filtered) => return Err(DiscardReason::Filtered),\n                }\n            }\n            if !self.load_stream(store_id, conn_id) {\n                return Err(DiscardReason::Disconnected);\n            }\n        }\n        self.cache.remove(&(store_id, conn_id));\n        Err(DiscardReason::Disconnected)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/server/status_server.rs::test_region_meta_endpoint", "test": "fn test_region_meta_endpoint() {\n    let mut cluster = new_server_cluster(0, 1);\n    cluster.run();\n    let region = cluster.get_region(b\"\");\n    let region_id = region.get_id();\n    let peer = region.get_peers().get(0);\n    assert!(peer.is_some());\n    let store_id = peer.unwrap().get_store_id();\n    let router = cluster.raft_extension(store_id);\n    let mut status_server = StatusServer::new(\n        1,\n        ConfigController::default(),\n        Arc::new(SecurityConfig::default()),\n        router,\n        std::env::temp_dir(),\n        None,\n        GrpcServiceManager::dummy(),\n    )\n    .unwrap();\n    let addr = format!(\"127.0.0.1:{}\", test_util::alloc_port());\n    status_server.start(addr).unwrap();\n    let check_task = check(status_server.listening_addr(), region_id);\n    let rt = tokio::runtime::Runtime::new().unwrap();\n    if let Err(err) = rt.block_on(check_task) {\n        panic!(\"{}\", err);\n    }\n    status_server.stop();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/tidb_query_datatype/src/codec/data_type/scalar.rs::is_some", "code": "pub fn is_some(&self) -> bool {\n        match_template_evaltype! {\n            TT, match self {\n                ScalarValue::TT(v) => v.is_some(),\n            }\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raft_storage.rs::test_raft_storage", "test": "fn test_raft_storage() {\n    let (_cluster, storage, mut ctx) = new_raft_storage();\n    let key = Key::from_raw(b\"key\");\n    assert_eq!(storage.get(ctx.clone(), &key, 5).unwrap().0, None);\n    storage\n        .prewrite(\n            ctx.clone(),\n            vec![Mutation::make_put(key.clone(), b\"value\".to_vec())],\n            b\"key\".to_vec(),\n            10,\n        )\n        .unwrap();\n    storage\n        .commit(ctx.clone(), vec![key.clone()], 10, 15)\n        .unwrap();\n    assert_eq!(\n        storage.get(ctx.clone(), &key, 20).unwrap().0.unwrap(),\n        b\"value\".to_vec()\n    );\n\n    // Test wrong region id.\n    let region_id = ctx.get_region_id();\n    ctx.set_region_id(region_id + 1);\n    storage.get(ctx.clone(), &key, 20).unwrap_err();\n    storage\n        .batch_get(ctx.clone(), &[key.clone()], 20)\n        .unwrap_err();\n    storage\n        .scan(ctx.clone(), key, None, 1, false, 20)\n        .unwrap_err();\n    storage.scan_locks(ctx, 20, None, None, 100).unwrap_err();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/mod.rs::clone", "code": "fn clone(&self) -> Self {\n        let refs = self.refs.fetch_add(1, atomic::Ordering::SeqCst);\n\n        trace!(\n            \"Storage referenced\"; \"original_ref\" => refs\n        );\n\n        Self {\n            engine: self.engine.clone(),\n            sched: self.sched.clone(),\n            read_pool: self.read_pool.clone(),\n            refs: self.refs.clone(),\n            max_key_size: self.max_key_size,\n            concurrency_manager: self.concurrency_manager.clone(),\n            api_version: self.api_version,\n            causal_ts_provider: self.causal_ts_provider.clone(),\n            resource_tag_factory: self.resource_tag_factory.clone(),\n            quota_limiter: self.quota_limiter.clone(),\n            _phantom: PhantomData,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raft_storage.rs::test_raft_storage_get_after_lease", "test": "fn test_raft_storage_get_after_lease() {\n    let (cluster, storage, ctx) = new_raft_storage();\n    let key = b\"key\";\n    let value = b\"value\";\n    assert_eq!(\n        storage\n            .raw_get(ctx.clone(), \"\".to_string(), key.to_vec())\n            .unwrap(),\n        None\n    );\n    storage\n        .raw_put(ctx.clone(), \"\".to_string(), key.to_vec(), value.to_vec())\n        .unwrap();\n    assert_eq!(\n        storage\n            .raw_get(ctx.clone(), \"\".to_string(), key.to_vec())\n            .unwrap()\n            .unwrap(),\n        value.to_vec()\n    );\n\n    // Sleep until the leader lease is expired.\n    thread::sleep(cluster.cfg.raft_store.raft_store_max_leader_lease.0);\n    assert_eq!(\n        storage\n            .raw_get(ctx, \"\".to_string(), key.to_vec())\n            .unwrap()\n            .unwrap(),\n        value.to_vec()\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/store/peer.rs::to_vec", "code": "pub fn to_vec(self) -> Vec<u8> {\n        if self.is_empty() {\n            return vec![];\n        }\n        let ctx = self.bits();\n        vec![ctx]\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raft_storage.rs::test_raft_storage_store_not_match", "test": "fn test_raft_storage_store_not_match() {\n    let (_cluster, storage, mut ctx) = new_raft_storage();\n\n    let key = Key::from_raw(b\"key\");\n    assert_eq!(storage.get(ctx.clone(), &key, 5).unwrap().0, None);\n    storage\n        .prewrite(\n            ctx.clone(),\n            vec![Mutation::make_put(key.clone(), b\"value\".to_vec())],\n            b\"key\".to_vec(),\n            10,\n        )\n        .unwrap();\n    storage\n        .commit(ctx.clone(), vec![key.clone()], 10, 15)\n        .unwrap();\n    assert_eq!(\n        storage.get(ctx.clone(), &key, 20).unwrap().0.unwrap(),\n        b\"value\".to_vec()\n    );\n\n    // Test store not match.\n    let mut peer = ctx.get_peer().clone();\n    let store_id = peer.get_store_id();\n\n    peer.set_store_id(store_id + 1);\n    ctx.set_peer(peer);\n    storage.get(ctx.clone(), &key, 20).unwrap_err();\n    let res = storage.get(ctx.clone(), &key, 20);\n    if let StorageError(box StorageErrorInner::Txn(TxnError(box TxnErrorInner::Engine(KvError(\n        box KvErrorInner::Request(ref e),\n    ))))) = *res.as_ref().err().unwrap()\n    {\n        assert!(e.has_store_not_match());\n    } else {\n        panic!(\"expect store_not_match, but got {:?}\", res);\n    }\n    storage\n        .batch_get(ctx.clone(), &[key.clone()], 20)\n        .unwrap_err();\n    storage\n        .scan(ctx.clone(), key, None, 1, false, 20)\n        .unwrap_err();\n    storage.scan_locks(ctx, 20, None, None, 100).unwrap_err();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/mod.rs::clone", "code": "fn clone(&self) -> Self {\n        let refs = self.refs.fetch_add(1, atomic::Ordering::SeqCst);\n\n        trace!(\n            \"Storage referenced\"; \"original_ref\" => refs\n        );\n\n        Self {\n            engine: self.engine.clone(),\n            sched: self.sched.clone(),\n            read_pool: self.read_pool.clone(),\n            refs: self.refs.clone(),\n            max_key_size: self.max_key_size,\n            concurrency_manager: self.concurrency_manager.clone(),\n            api_version: self.api_version,\n            causal_ts_provider: self.causal_ts_provider.clone(),\n            resource_tag_factory: self.resource_tag_factory.clone(),\n            quota_limiter: self.quota_limiter.clone(),\n            _phantom: PhantomData,\n        }\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raft_storage.rs::test_auto_gc", "test": "fn test_auto_gc() {\n    let count = 3;\n    let (mut cluster, first_leader_storage, ctx) =\n        new_raft_storage_with_store_count::<ApiV1>(count, \"\");\n    let pd_client = Arc::clone(&cluster.pd_client);\n\n    // Used to wait for all storage's GC to finish\n    let (finish_signal_tx, finish_signal_rx) = channel();\n\n    // Create storage object for each store in the cluster\n    let mut storages: HashMap<_, _> = cluster\n        .sim\n        .rl()\n        .storages\n        .iter()\n        .map(|(id, engine)| {\n            let mut config = GcConfig::default();\n            // Do not skip GC\n            config.ratio_threshold = 0.9;\n            let storage = SyncTestStorageBuilderApiV1::from_engine(engine.clone())\n                .gc_config(config)\n                .build(*id)\n                .unwrap();\n\n            (*id, storage)\n        })\n        .collect();\n\n    let mut region_info_accessors = cluster.sim.rl().region_info_accessors.clone();\n\n    for (id, storage) in &mut storages {\n        let tx = finish_signal_tx.clone();\n\n        let mut cfg = AutoGcConfig::new_test_cfg(\n            Arc::clone(&pd_client),\n            region_info_accessors.remove(id).unwrap(),\n            *id,\n        );\n        cfg.post_a_round_of_gc = Some(Box::new(move || tx.send(()).unwrap()));\n\n        storage.start_auto_gc(cfg);\n    }\n\n    assert_eq!(storages.len(), count);\n\n    // test_data will be wrote with ts < 50\n    let test_data: Vec<_> = [\n        (b\"k1\", b\"v1\"),\n        (b\"k2\", b\"v2\"),\n        (b\"k3\", b\"v3\"),\n        (b\"k4\", b\"v4\"),\n        (b\"k5\", b\"v5\"),\n        (b\"k6\", b\"v6\"),\n        (b\"k7\", b\"v7\"),\n        (b\"k8\", b\"v8\"),\n        (b\"k9\", b\"v9\"),\n    ]\n    .iter()\n    .map(|(k, v)| (k.to_vec(), v.to_vec()))\n    .collect();\n\n    let test_data2: Vec<_> = test_data\n        .iter()\n        .map(|(k, v)| {\n            let mut v = v.to_vec();\n            v.push(b'1');\n            (k.to_vec(), v)\n        })\n        .collect();\n\n    let test_data3: Vec<_> = test_data\n        .iter()\n        .map(|(k, v)| {\n            let mut v = v.to_vec();\n            v.push(b'2');\n            (k.to_vec(), v)\n        })\n        .collect();\n\n    write_test_data(&first_leader_storage, &ctx, &test_data, 10);\n    write_test_data(&first_leader_storage, &ctx, &test_data2, 100);\n    write_test_data(&first_leader_storage, &ctx, &test_data3, 200);\n\n    let split_keys: &[&[u8]] = &[b\"k2\", b\"k4\", b\"k6\", b\"k8\"];\n\n    for k in split_keys {\n        let region = cluster.get_region(k);\n        cluster.must_split(&region, k);\n    }\n\n    check_data(&mut cluster, &storages, &test_data, 50, true);\n    check_data(&mut cluster, &storages, &test_data2, 150, true);\n    check_data(&mut cluster, &storages, &test_data3, 250, true);\n\n    pd_client.set_gc_safe_point(150);\n\n    for _ in 0..count {\n        finish_signal_rx.recv().unwrap();\n    }\n\n    check_data(&mut cluster, &storages, &test_data, 50, false);\n    check_data(&mut cluster, &storages, &test_data2, 150, true);\n    check_data(&mut cluster, &storages, &test_data3, 250, true);\n\n    // No more signals.\n    finish_signal_rx\n        .recv_timeout(Duration::from_millis(300))\n        .unwrap_err();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/src/storage/txn/store.rs::len", "code": "pub fn len(&self) -> usize {\n        self.entries.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_raftkv", "test": "fn test_raftkv() {\n    let count = 1;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.run();\n\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(b\"k1\"), None);\n\n    let region = cluster.get_region(b\"\");\n    let leader_id = cluster.leader_of_region(region.get_id()).unwrap();\n    let mut storage = cluster.sim.rl().storages[&leader_id.get_id()].clone();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(region.get_peers()[0].clone());\n    let snap_ctx = SnapContext {\n        pb_ctx: &ctx,\n        ..Default::default()\n    };\n\n    get_put(snap_ctx.clone(), &mut storage);\n    batch(snap_ctx.clone(), &mut storage);\n    seek(snap_ctx.clone(), &mut storage);\n    near_seek(snap_ctx.clone(), &mut storage);\n    cf(snap_ctx, &mut storage);\n    empty_write(&ctx, &storage);\n    wrong_context(&ctx, &storage);\n    // TODO: test multiple node\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_read_leader_in_lease", "test": "fn test_read_leader_in_lease() {\n    let count = 3;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.run();\n\n    let k1 = b\"k1\";\n    let (k2, v2) = (b\"k2\", b\"v2\");\n\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(k1), None);\n\n    let region = cluster.get_region(b\"\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let mut storage = cluster.sim.rl().storages[&leader.get_id()].clone();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader.clone());\n    let snap_ctx = SnapContext {\n        pb_ctx: &ctx,\n        ..Default::default()\n    };\n\n    // write some data\n    assert_none(snap_ctx.clone(), &mut storage, k2);\n    must_put(&ctx, &storage, k2, v2);\n\n    // isolate leader\n    cluster.add_send_filter(IsolationFilterFactory::new(leader.get_store_id()));\n\n    // leader still in lease, check if can read on leader\n    assert_eq!(can_read(snap_ctx, &mut storage, k2, v2), true);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_read_index_on_replica", "test": "fn test_read_index_on_replica() {\n    let count = 3;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.run();\n\n    let k1 = b\"k1\";\n    let (k2, v2) = (b\"k2\", b\"v2\");\n\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(k1), None);\n\n    let region = cluster.get_region(b\"\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let mut storage = cluster.sim.rl().storages[&leader.get_id()].clone();\n\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(leader.clone());\n    let snap_ctx = SnapContext {\n        pb_ctx: &ctx,\n        ..Default::default()\n    };\n\n    // write some data\n    let peers = region.get_peers();\n    assert_none(snap_ctx, &mut storage, k2);\n    must_put(&ctx, &storage, k2, v2);\n\n    // read on follower\n    let mut follower_peer = None;\n    for p in peers {\n        if p.get_id() != leader.get_id() {\n            follower_peer = Some(p.clone());\n            break;\n        }\n    }\n\n    assert!(follower_peer.is_some());\n    ctx.set_peer(follower_peer.as_ref().unwrap().clone());\n    let resp = read_index_on_peer(\n        &mut cluster,\n        follower_peer.unwrap(),\n        region.clone(),\n        false,\n        std::time::Duration::from_secs(5),\n    );\n    assert!(!resp.as_ref().unwrap().get_header().has_error());\n    assert_ne!(\n        resp.unwrap().get_responses()[0]\n            .get_read_index()\n            .get_read_index(),\n        0\n    );\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_read_on_replica", "test": "fn test_read_on_replica() {\n    let count = 3;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.cfg.raft_store.hibernate_regions = false;\n    cluster.run();\n\n    let k1 = b\"k1\";\n    let (k2, v2) = (b\"k2\", b\"v2\");\n    let (k3, v3) = (b\"k3\", b\"v3\");\n    let (k4, v4) = (b\"k4\", b\"v4\");\n\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(k1), None);\n\n    let region = cluster.get_region(b\"\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let mut leader_storage = cluster.sim.rl().storages[&leader.get_id()].clone();\n\n    let mut leader_ctx = Context::default();\n    leader_ctx.set_region_id(region.get_id());\n    leader_ctx.set_region_epoch(region.get_region_epoch().clone());\n    leader_ctx.set_peer(leader.clone());\n    let leader_snap_ctx = SnapContext {\n        pb_ctx: &leader_ctx,\n        ..Default::default()\n    };\n\n    // write some data\n    let peers = region.get_peers();\n    assert_none(leader_snap_ctx, &mut leader_storage, k2);\n    must_put(&leader_ctx, &leader_storage, k2, v2);\n\n    // read on follower\n    let mut follower_peer = None;\n    let mut follower_id = 0;\n    for p in peers {\n        if p.get_id() != leader.get_id() {\n            follower_id = p.get_id();\n            follower_peer = Some(p.clone());\n            break;\n        }\n    }\n\n    assert!(follower_peer.is_some());\n    let mut follower_ctx = Context::default();\n    follower_ctx.set_region_id(region.get_id());\n    follower_ctx.set_region_epoch(region.get_region_epoch().clone());\n    follower_ctx.set_peer(follower_peer.as_ref().unwrap().clone());\n    follower_ctx.set_replica_read(true);\n    let follower_snap_ctx = SnapContext {\n        pb_ctx: &follower_ctx,\n        ..Default::default()\n    };\n    let mut follower_storage = cluster.sim.rl().storages[&follower_id].clone();\n    assert_has(follower_snap_ctx.clone(), &mut follower_storage, k2, v2);\n\n    must_put(&leader_ctx, &leader_storage, k3, v3);\n    assert_has(follower_snap_ctx.clone(), &mut follower_storage, k3, v3);\n\n    cluster.stop_node(follower_id);\n    must_put(&leader_ctx, &leader_storage, k4, v4);\n    cluster.run_node(follower_id).unwrap();\n    let mut follower_storage = cluster.sim.rl().storages[&follower_id].clone();\n    // sleep to ensure the follower has received a heartbeat from the leader\n    thread::sleep(time::Duration::from_millis(300));\n    assert_has(follower_snap_ctx, &mut follower_storage, k4, v4);\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_read_on_replica_check_memory_locks", "test": "fn test_read_on_replica_check_memory_locks() {\n    let count = 3;\n    let mut cluster = new_server_cluster(0, count);\n    cluster.cfg.raft_store.hibernate_regions = false;\n    cluster.run();\n\n    let raw_key = b\"key\";\n    let encoded_key = Key::from_raw(raw_key);\n\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(raw_key), None);\n\n    let region = cluster.get_region(b\"\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let leader_cm = cluster.sim.rl().get_concurrency_manager(leader.get_id());\n\n    let lock = Lock::new(\n        LockType::Put,\n        raw_key.to_vec(),\n        10.into(),\n        20000,\n        None,\n        10.into(),\n        1,\n        20.into(),\n    );\n    let guard = block_on(leader_cm.lock_key(&encoded_key));\n    guard.with_lock(|l| *l = Some(lock.clone()));\n\n    // read on follower\n    let mut follower_peer = None;\n    let mut follower_id = 0;\n    let peers = region.get_peers();\n    for p in peers {\n        if p.get_id() != leader.get_id() {\n            follower_id = p.get_id();\n            follower_peer = Some(p.clone());\n            break;\n        }\n    }\n\n    assert!(follower_peer.is_some());\n    let mut follower_ctx = Context::default();\n    follower_ctx.set_region_id(region.get_id());\n    follower_ctx.set_region_epoch(region.get_region_epoch().clone());\n    follower_ctx.set_peer(follower_peer.as_ref().unwrap().clone());\n    follower_ctx.set_replica_read(true);\n    for use_max_ts in [false, true] {\n        let mut range = KeyRange::default();\n        range.set_start_key(encoded_key.as_encoded().to_vec());\n        let ts = if use_max_ts {\n            Some(TimeStamp::max())\n        } else {\n            Some(100.into())\n        };\n        let follower_snap_ctx = SnapContext {\n            pb_ctx: &follower_ctx,\n            start_ts: ts,\n            key_ranges: vec![range],\n            ..Default::default()\n        };\n        let mut follower_storage = cluster.sim.rl().storages[&follower_id].clone();\n        match follower_storage.snapshot(follower_snap_ctx) {\n            Err(Error(box ErrorInner::KeyIsLocked(lock_info))) => {\n                assert_eq!(lock_info, lock.clone().into_lock_info(raw_key.to_vec()))\n            }\n            other => panic!(\"unexpected result: {:?}\", other),\n        }\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_raftkv.rs::test_raftkv_precheck_write_with_ctx", "test": "fn test_raftkv_precheck_write_with_ctx() {\n    let mut cluster = new_server_cluster(0, 3);\n    cluster.run();\n\n    // make sure leader has been elected.\n    assert_eq!(cluster.must_get(b\"k1\"), None);\n\n    let region = cluster.get_region(b\"\");\n    let leader = cluster.leader_of_region(region.get_id()).unwrap();\n    let follower = region\n        .get_peers()\n        .iter()\n        .find(|p| p.get_id() != leader.get_id())\n        .unwrap();\n\n    let leader_storage = cluster.sim.rl().storages[&leader.get_id()].clone();\n    let follower_storage = cluster.sim.rl().storages[&follower.get_id()].clone();\n\n    // Assume this is a write request.\n    let mut ctx = Context::default();\n    ctx.set_region_id(region.get_id());\n    ctx.set_region_epoch(region.get_region_epoch().clone());\n    ctx.set_peer(region.get_peers()[0].clone());\n\n    // The (write) request can be sent to the leader.\n    leader_storage.precheck_write_with_ctx(&ctx).unwrap();\n    // The (write) request should not be send to a follower.\n    follower_storage.precheck_write_with_ctx(&ctx).unwrap_err();\n\n    // Leader has network partition and it must be not leader any more.\n    let filter = Box::new(RegionPacketFilter::new(\n        region.get_id(),\n        leader.get_store_id(),\n    ));\n    cluster\n        .sim\n        .wl()\n        .add_recv_filter(leader.get_store_id(), filter.clone());\n    cluster\n        .sim\n        .wl()\n        .add_send_filter(leader.get_store_id(), filter);\n    sleep_until_election_triggered(&cluster.cfg);\n    leader_storage.precheck_write_with_ctx(&ctx).unwrap_err();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_region_info_accessor.rs::test_region_collection_seek_region", "test": "fn test_region_collection_seek_region() {\n    let mut cluster = new_node_cluster(0, 3);\n\n    let (tx, rx) = channel();\n    cluster\n        .sim\n        .wl()\n        .post_create_coprocessor_host(Box::new(move |id, host| {\n            let p = RegionInfoAccessor::new(host);\n            tx.send((id, p)).unwrap()\n        }));\n\n    cluster.run();\n    let region_info_providers: HashMap<_, _> = rx.try_iter().collect();\n    assert_eq!(region_info_providers.len(), 3);\n    let regions = prepare_cluster(&mut cluster);\n\n    for node_id in cluster.get_node_ids() {\n        let engine = &region_info_providers[&node_id];\n\n        // Test traverse all regions\n        let key = b\"\".to_vec();\n        let (tx, rx) = channel();\n        let tx_ = tx.clone();\n        engine\n            .seek_region(\n                &key,\n                Box::new(move |infos| {\n                    tx_.send(infos.map(|i| i.region.clone()).collect()).unwrap();\n                }),\n            )\n            .unwrap();\n        let sought_regions: Vec<_> = rx.recv_timeout(Duration::from_secs(3)).unwrap();\n        assert_eq!(sought_regions, regions);\n\n        // Test end_key is exclusive\n        let (tx, rx) = channel();\n        let tx_ = tx.clone();\n        engine\n            .seek_region(\n                b\"k1\",\n                Box::new(move |infos| tx_.send(infos.next().unwrap().region.clone()).unwrap()),\n            )\n            .unwrap();\n        let region = rx.recv_timeout(Duration::from_secs(3)).unwrap();\n        assert_eq!(region, regions[1]);\n\n        // Test seek from non-starting key\n        let tx_ = tx.clone();\n        engine\n            .seek_region(\n                b\"k6\\xff\\xff\\xff\\xff\\xff\",\n                Box::new(move |infos| tx_.send(infos.next().unwrap().region.clone()).unwrap()),\n            )\n            .unwrap();\n        let region = rx.recv_timeout(Duration::from_secs(3)).unwrap();\n        assert_eq!(region, regions[3]);\n        let tx_ = tx.clone();\n        engine\n            .seek_region(\n                b\"\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\",\n                Box::new(move |infos| tx_.send(infos.next().unwrap().region.clone()).unwrap()),\n            )\n            .unwrap();\n        let region = rx.recv_timeout(Duration::from_secs(3)).unwrap();\n        assert_eq!(region, regions[5]);\n    }\n\n    for (_, p) in region_info_providers {\n        p.stop();\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/coprocessor/mod.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cmds.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_region_info_accessor.rs::test_region_collection_get_regions_in_range", "test": "fn test_region_collection_get_regions_in_range() {\n    let mut cluster = new_node_cluster(0, 3);\n\n    let (tx, rx) = channel();\n    cluster\n        .sim\n        .wl()\n        .post_create_coprocessor_host(Box::new(move |id, host| {\n            let p = RegionInfoAccessor::new(host);\n            tx.send((id, p)).unwrap()\n        }));\n\n    cluster.run();\n    let region_info_providers: HashMap<_, _> = rx.try_iter().collect();\n    assert_eq!(region_info_providers.len(), 3);\n    let regions = prepare_cluster(&mut cluster);\n\n    for node_id in cluster.get_node_ids() {\n        let engine = &region_info_providers[&node_id];\n\n        let result = engine.get_regions_in_range(b\"\", b\"\").unwrap();\n        assert_eq!(result, regions);\n\n        let result = engine.get_regions_in_range(b\"k1\", b\"k3\").unwrap();\n        assert_eq!(&result, &regions[1..3]);\n\n        let result = engine.get_regions_in_range(b\"k3\", b\"k8\").unwrap();\n        assert_eq!(&result, &regions[2..5]);\n\n        let result = engine.get_regions_in_range(b\"k6\", b\"k8\").unwrap();\n        assert_eq!(&result, &regions[3..5]);\n\n        let result = engine.get_regions_in_range(b\"k7\", b\"k99\").unwrap();\n        assert_eq!(&result, &regions[4..6]);\n\n        let result = engine.get_regions_in_range(b\"k99\", b\"\").unwrap();\n        assert_eq!(&result, &regions[5..6]);\n    }\n\n    for (_, p) in region_info_providers {\n        p.stop();\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/coprocessor/mod.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cmds.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_region_info_accessor.rs::test_region_collection_find_region_by_key", "test": "fn test_region_collection_find_region_by_key() {\n    let mut cluster = new_node_cluster(0, 3);\n\n    let (tx, rx) = channel();\n    cluster\n        .sim\n        .wl()\n        .post_create_coprocessor_host(Box::new(move |id, host| {\n            let p = RegionInfoAccessor::new(host);\n            tx.send((id, p)).unwrap()\n        }));\n\n    cluster.run();\n    let region_info_providers: HashMap<_, _> = rx.try_iter().collect();\n    assert_eq!(region_info_providers.len(), 3);\n    let regions = prepare_cluster(&mut cluster);\n\n    for node_id in cluster.get_node_ids() {\n        let engine = &region_info_providers[&node_id];\n\n        let region = engine.find_region_by_key(b\"\").unwrap();\n        assert_eq!(region, regions[0]);\n\n        let region = engine.find_region_by_key(b\"k2\").unwrap();\n        assert_eq!(region, regions[1]);\n\n        let region = engine.find_region_by_key(b\"k99\").unwrap();\n        assert_eq!(region, *regions.last().unwrap());\n    }\n\n    for (_, p) in region_info_providers {\n        p.stop();\n    }\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/raftstore/src/coprocessor/mod.rs::len", "code": "pub fn len(&self) -> usize {\n        self.cmds.len()\n    }", "docstring": null}
{"test_id": "tikv-tikv/tikv-tikv-8632b39/tests/integrations/storage/test_titan.rs::test_turnoff_titan", "test": "fn test_turnoff_titan() {\n    let mut cluster = new_node_cluster(0, 3);\n    cluster.cfg.rocksdb.defaultcf.disable_auto_compactions = true;\n    cluster.cfg.rocksdb.defaultcf.num_levels = 1;\n    configure_for_enable_titan(&mut cluster, ReadableSize::kb(0));\n    cluster.run();\n    assert_eq!(cluster.must_get(b\"k1\"), None);\n\n    let size = 5;\n    for i in 0..size {\n        cluster\n            .put(\n                format!(\"k{:02}0\", i).as_bytes(),\n                format!(\"v{}\", i).as_bytes(),\n            )\n            .unwrap();\n    }\n    cluster.must_flush_cf(CF_DEFAULT, true);\n    for i in 0..size {\n        cluster\n            .put(\n                format!(\"k{:02}1\", i).as_bytes(),\n                format!(\"v{}\", i).as_bytes(),\n            )\n            .unwrap();\n    }\n    cluster.must_flush_cf(CF_DEFAULT, true);\n    for i in cluster.get_node_ids().into_iter() {\n        let engine = cluster.get_engine(i);\n        let db = engine.as_inner();\n        assert_eq!(\n            db.get_property_int(\"rocksdb.num-files-at-level0\").unwrap(),\n            2\n        );\n        assert_eq!(\n            db.get_property_int(\"rocksdb.num-files-at-level1\").unwrap(),\n            0\n        );\n        assert_eq!(\n            db.get_property_int(\"rocksdb.titandb.num-live-blob-file\")\n                .unwrap(),\n            2\n        );\n        assert_eq!(\n            db.get_property_int(\"rocksdb.titandb.num-obsolete-blob-file\")\n                .unwrap(),\n            0\n        );\n    }\n    cluster.shutdown();\n\n    // try reopen db when titan isn't properly turned off.\n    configure_for_disable_titan(&mut cluster);\n    cluster.pre_start_check().unwrap_err();\n\n    configure_for_enable_titan(&mut cluster, ReadableSize::kb(0));\n    cluster.pre_start_check().unwrap();\n    cluster.start().unwrap();\n    assert_eq!(cluster.must_get(b\"k1\"), None);\n    for i in cluster.get_node_ids().into_iter() {\n        let db = cluster.get_engine(i);\n        let opt = vec![(\"blob_run_mode\", \"kFallback\")];\n        db.set_options_cf(CF_DEFAULT, &opt).unwrap();\n    }\n    cluster.compact_data();\n    let mut all_check_pass = true;\n    for _ in 0..10 {\n        // wait for gc completes.\n        sleep_ms(10);\n        all_check_pass = true;\n        for i in cluster.get_node_ids().into_iter() {\n            let engine = cluster.get_engine(i);\n            let db = engine.as_inner();\n            if db.get_property_int(\"rocksdb.num-files-at-level0\").unwrap() != 0 {\n                all_check_pass = false;\n                break;\n            }\n            if db.get_property_int(\"rocksdb.num-files-at-level1\").unwrap() != 1 {\n                all_check_pass = false;\n                break;\n            }\n            if db\n                .get_property_int(\"rocksdb.titandb.num-live-blob-file\")\n                .unwrap()\n                != 0\n            {\n                all_check_pass = false;\n                break;\n            }\n        }\n        if all_check_pass {\n            break;\n        }\n    }\n    if !all_check_pass {\n        panic!(\"unexpected titan gc results\");\n    }\n    cluster.shutdown();\n\n    configure_for_disable_titan(&mut cluster);\n    // wait till files are purged, timeout set to purge_obsolete_files_period.\n    for _ in 1..100 {\n        sleep_ms(10);\n        if cluster.pre_start_check().is_ok() {\n            return;\n        }\n    }\n    cluster.pre_start_check().unwrap();\n}", "code_id": "tikv-tikv/tikv-tikv-8632b39/components/test_raftstore-v2/src/cluster.rs::must_get", "code": "pub fn must_get(&mut self, key: &[u8]) -> Option<Vec<u8>> {\n        self.get_impl(CF_DEFAULT, key, true)\n    }", "docstring": null}
