fn encoded_len_padded() {
    assert_eq!(0, encoded_len(0, true).unwrap());
    assert_eq!(4, encoded_len(1, true).unwrap());
    assert_eq!(4, encoded_len(2, true).unwrap());
    assert_eq!(4, encoded_len(3, true).unwrap());
    assert_eq!(8, encoded_len(4, true).unwrap());
    assert_eq!(8, encoded_len(5, true).unwrap());
    assert_eq!(8, encoded_len(6, true).unwrap());
    assert_eq!(12, encoded_len(7, true).unwrap());
}
fn write_batch_write_twice_2() {
    let db = default_engine();

    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"aa").unwrap();

    wb.write().unwrap();

    db.engine.put(b"a", b"b").unwrap();
    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"b");

    wb.write().unwrap();

    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"aa");

    let db = multi_batch_write_engine();

    let mut wb = db.engine.write_batch_with_cap(1024);

    for i in 0..128_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"aa").unwrap();

    wb.write().unwrap();

    db.engine.put(b"a", b"b").unwrap();
    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"b");

    for i in 0..128_usize {
        let k = i.to_be_bytes();
        let v = (2 * i + 1).to_be_bytes();
        db.engine.put(&k, &v).unwrap();
    }
    for i in 0..128_usize {
        let k = i.to_be_bytes();
        let v = (2 * i + 1).to_be_bytes();
        assert_eq!(db.engine.get_value(&k).unwrap().unwrap(), &v);
    }

    wb.write().unwrap();

    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"aa");
    for i in 0..128_usize {
        let x = i.to_be_bytes();
        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);
    }
}
fn test_data_recovery() {
    let mut cluster = Cluster::default();
    let registry = cluster.node(0).tablet_registry();
    let tablet_2_path = registry.tablet_path(2, RAFT_INIT_LOG_INDEX);
    // The rocksdb is a bootstrapped tablet, so it will be opened and closed in
    // bootstrap, and then open again in fsm initialization.
    assert_eq!(count_info_log(&tablet_2_path), 2);
    let router = &mut cluster.routers[0];
    router.wait_applied_to_current_term(2, Duration::from_secs(3));

    // Write 100 keys to default CF and not flush.
    let header = Box::new(router.new_request_for(2).take_header());
    for i in 0..100 {
        let mut put = SimpleWriteEncoder::with_capacity(64);
        put.put(
            CF_DEFAULT,
            format!("key{}", i).as_bytes(),
            format!("value{}", i).as_bytes(),
        );
        router
            .send(2, PeerMsg::simple_write(header.clone(), put.encode()).0)
            .unwrap();
    }

    // Write 100 keys to write CF and flush half.
    let mut sub = None;
    for i in 0..50 {
        let mut put = SimpleWriteEncoder::with_capacity(64);
        put.put(
            CF_WRITE,
            format!("key{}", i).as_bytes(),
            format!("value{}", i).as_bytes(),
        );
        let (msg, s) = PeerMsg::simple_write(header.clone(), put.encode());
        router.send(2, msg).unwrap();
        sub = Some(s);
    }
    let resp = block_on(sub.take().unwrap().result()).unwrap();
    assert!(!resp.get_header().has_error(), "{:?}", resp);

    let mut cached = cluster.node(0).tablet_registry().get(2).unwrap();
    cached.latest().unwrap().flush_cf(CF_WRITE, true).unwrap();
    let router = &mut cluster.routers[0];
    for i in 50..100 {
        let mut put = SimpleWriteEncoder::with_capacity(64);
        put.put(
            CF_WRITE,
            format!("key{}", i).as_bytes(),
            format!("value{}", i).as_bytes(),
        );
        router
            .send(2, PeerMsg::simple_write(header.clone(), put.encode()).0)
            .unwrap();
    }

    // Write 100 keys to lock CF and flush all.
    for i in 0..100 {
        let mut put = SimpleWriteEncoder::with_capacity(64);
        put.put(
            CF_LOCK,
            format!("key{}", i).as_bytes(),
            format!("value{}", i).as_bytes(),
        );
        let (msg, s) = PeerMsg::simple_write(header.clone(), put.encode());
        router.send(2, msg).unwrap();
        sub = Some(s);
    }
    let resp = block_on(sub.take().unwrap().result()).unwrap();
    assert!(!resp.get_header().has_error(), "{:?}", resp);

    cached = cluster.node(0).tablet_registry().get(2).unwrap();
    cached.latest().unwrap().flush_cf(CF_LOCK, true).unwrap();

    // Make sure all keys must be written.
    let router = &mut cluster.routers[0];
    let snap = router.stale_snapshot(2);
    for cf in DATA_CFS {
        for i in 0..100 {
            let key = format!("key{}", i);
            let value = snap.get_value_cf(cf, key.as_bytes()).unwrap();
            assert_eq!(
                value.as_deref(),
                Some(format!("value{}", i).as_bytes()),
                "{} {}",
                cf,
                key
            );
        }
    }
    let registry = cluster.node(0).tablet_registry();
    cached = registry.get(2).unwrap();
    cached
        .latest()
        .unwrap()
        .set_db_options(&[("avoid_flush_during_shutdown", "true")])
        .unwrap();
    drop((snap, cached));

    cluster.restart(0);

    let registry = cluster.node(0).tablet_registry();
    cached = registry.get(2).unwrap();
    cached
        .latest()
        .unwrap()
        .set_db_options(&[("avoid_flush_during_shutdown", "true")])
        .unwrap();
    let router = &mut cluster.routers[0];

    // Write another key to ensure all data are recovered.
    let mut put = SimpleWriteEncoder::with_capacity(64);
    put.put(CF_DEFAULT, b"key101", b"value101");
    let resp = router.simple_write(2, header, put).unwrap();
    assert!(!resp.get_header().has_error(), "{:?}", resp);

    // After being restarted, all unflushed logs should be applied again. So there
    // should be no missing data.
    let snap = router.stale_snapshot(2);
    for cf in DATA_CFS {
        for i in 0..100 {
            let key = format!("key{}", i);
            let value = snap.get_value_cf(cf, key.as_bytes()).unwrap();
            assert_eq!(
                value.as_deref(),
                Some(format!("value{}", i).as_bytes()),
                "{} {}",
                cf,
                key
            );
        }
    }

    // There is a restart, so LOG file should be rotate.
    assert_eq!(count_info_log(&tablet_2_path), 3);
    // We only trigger Flush twice, so there should be only 2 files. And because WAL
    // is disabled, so when rocksdb is restarted, there should be no WAL to recover,
    // so no additional flush will be triggered.
    assert_eq!(count_sst(&tablet_2_path), 2);

    cached = cluster.node(0).tablet_registry().get(2).unwrap();
    cached.latest().unwrap().flush_cfs(DATA_CFS, true).unwrap();

    // Although all CFs are triggered again, but recovery should only write:
    // 1. [0, 101) to CF_DEFAULT
    // 2. [50, 100) to CF_WRITE
    //
    // So there will be only 2 memtables to be flushed.
    assert_eq!(count_sst(&tablet_2_path), 4);

    drop((snap, cached));

    cluster.restart(0);

    let router = &mut cluster.routers[0];

    assert_eq!(count_info_log(&tablet_2_path), 4);
    // Because data is flushed before restarted, so all data can be read
    // immediately.
    let snap = router.stale_snapshot(2);
    for cf in DATA_CFS {
        for i in 0..100 {
            let key = format!("key{}", i);
            let value = snap.get_value_cf(cf, key.as_bytes()).unwrap();
            assert_eq!(
                value.as_deref(),
                Some(format!("value{}", i).as_bytes()),
                "{} {}",
                cf,
                key
            );
        }
    }
    // Trigger flush again.
    cached = cluster.node(0).tablet_registry().get(2).unwrap();
    cached.latest().unwrap().flush_cfs(DATA_CFS, true).unwrap();

    // There is no recovery, so there should be nothing to flush.
    assert_eq!(count_sst(&tablet_2_path), 4);
}
fn test_cdc_rawkv_basic() {
    let mut suite = TestSuite::new(1, ApiVersion::V2);

    // rawkv
    let mut req = suite.new_changedata_request(1);
    req.set_kv_api(ChangeDataRequestKvApi::RawKv);
    let (mut req_tx, _event_feed_wrap, receive_event) =
        new_event_feed(suite.get_region_cdc_client(1));
    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();

    let event = receive_event(false);
    event.events.into_iter().for_each(|e| {
        match e.event.unwrap() {
            // Even if there is no write,
            // it should always outputs an Initialized event.
            Event_oneof_event::Entries(es) => {
                assert!(es.entries.len() == 1, "{:?}", es);
                let e = &es.entries[0];
                assert_eq!(e.get_type(), EventLogType::Initialized, "{:?}", es);
            }
            other => panic!("unknown event {:?}", other),
        }
    });
    // Sleep a while to make sure the stream is registered.
    sleep_ms(1000);
    // There must be a delegate.
    let scheduler = suite.endpoints.values().next().unwrap().scheduler();
    scheduler
        .schedule(Task::Validate(Validate::Region(
            1,
            Box::new(|delegate| {
                let d = delegate.unwrap();
                assert_eq!(d.downstreams().len(), 1);
            }),
        )))
        .unwrap();

    // If tikv enable ApiV2, raw key needs to start with 'r';
    let (k, v) = (b"rkey1".to_vec(), b"value".to_vec());
    suite.must_kv_put(1, k, v);
    let mut events = receive_event(false).events.to_vec();
    assert_eq!(events.len(), 1, "{:?}", events);

    match events.pop().unwrap().event.unwrap() {
        Event_oneof_event::Entries(entries) => {
            assert_eq!(entries.entries.len(), 1);
            assert_eq!(entries.entries[0].get_type(), EventLogType::Committed);
        }
        other => panic!("unknown event {:?}", other),
    }

    // boundary case
    let (k, v) = (b"r\0".to_vec(), b"value".to_vec());
    suite.must_kv_put(1, k, v);
    let mut events = receive_event(false).events.to_vec();
    assert_eq!(events.len(), 1, "{:?}", events);

    match events.pop().unwrap().event.unwrap() {
        Event_oneof_event::Entries(entries) => {
            assert_eq!(entries.entries.len(), 1);
            assert_eq!(entries.entries[0].get_type(), EventLogType::Committed);
        }
        other => panic!("unknown event {:?}", other),
    }
}
fn test_parse_request_failed_2() {
    // It should not even take any snapshots when parse failed.
    let product = ProductTable::new();
    let (_, endpoint) = init_with_data(&product, &[]);
    let req = DagSelect::from(&product).build();

    fail::cfg("rockskv_async_snapshot", "panic").unwrap();
    fail::cfg("coprocessor_parse_request", "return()").unwrap();
    let resp = handle_request(&endpoint, req);

    assert!(resp.get_other_error().contains("unsupported tp"));
}
fn test_server_close() {
    do_test(
        3012,
        |mut cli_sock| {
            cli_sock.send(Message::Text("Hello WebSocket".into())).unwrap();

            let message = cli_sock.read().unwrap(); // receive close from server
            assert!(message.is_close());

            let err = cli_sock.read().unwrap_err(); // now we should get ConnectionClosed
            match err {
                Error::ConnectionClosed => {}
                _ => panic!("unexpected error: {:?}", err),
            }
        },
        |mut srv_sock| {
            let message = srv_sock.read().unwrap();
            assert_eq!(message.into_data(), b"Hello WebSocket");

            srv_sock.close(None).unwrap(); // send close to client

            let message = srv_sock.read().unwrap(); // receive acknowledgement
            assert!(message.is_close());

            let err = srv_sock.read().unwrap_err(); // now we should get ConnectionClosed
            match err {
                Error::ConnectionClosed => {}
                _ => panic!("unexpected error: {:?}", err),
            }
        },
    );
}
fn save_point_rollback_one() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    wb.set_save_point();
    wb.put(b"a", b"").unwrap();

    wb.rollback_to_save_point().unwrap();

    let err = wb.rollback_to_save_point();
    assert_engine_error(err);
    let err = wb.pop_save_point();
    assert_engine_error(err);
    wb.write().unwrap();
    let val = db.engine.get_value(b"a").unwrap();
    assert!(val.is_none());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);

    wb.set_save_point();
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();

    wb.rollback_to_save_point().unwrap();

    let err = wb.rollback_to_save_point();
    assert_engine_error(err);
    let err = wb.pop_save_point();
    assert_engine_error(err);
    wb.write().unwrap();
    for i in 0..256_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
    let val = db.engine.get_value(b"a").unwrap();
    assert!(val.is_none());
}
fn should_write_to_engine_but_whatever() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();
    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;

    let mut key = vec![];
    loop {
        key.push(b'a');
        wb.put(&key, b"").unwrap();
        if key.len() <= max_keys {
            assert!(!wb.should_write_to_engine());
        }
        if key.len() > max_keys {
            assert!(wb.should_write_to_engine());
        }
        if key.len() == max_keys * 2 {
            assert!(wb.should_write_to_engine());
            wb.write().unwrap();
            break;
        }
    }

    let mut key = vec![];
    loop {
        key.push(b'a');
        assert!(db.engine.get_value(&key).unwrap().is_some());
        if key.len() == max_keys * 2 {
            break;
        }
    }

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;

    let mut key = vec![];

    loop {
        key.push(b'a');
        wb.put(&key, b"").unwrap();
        if key.len() <= max_keys {
            assert!(!wb.should_write_to_engine());
        }
        if key.len() > max_keys {
            assert!(wb.should_write_to_engine());
        }
        if key.len() == max_keys * 2 {
            assert!(wb.should_write_to_engine());
            wb.write().unwrap();
            break;
        }
    }

    let mut key = vec![];
    loop {
        key.push(b'a');
        assert!(db.engine.get_value(&key).unwrap().is_some());
        if key.len() == max_keys * 2 {
            break;
        }
    }
}
fn default_names() {
    let db = default_engine();
    let names = db.engine.cf_names();
    assert_eq!(names.len(), 1);
    assert_eq!(names[0], CF_DEFAULT);
}
fn insert_overwrite() {
    let tmpfile = create_tempfile();
    let db = Database::create(tmpfile.path()).unwrap();
    let write_txn = db.begin_write().unwrap();
    {
        let mut table = write_txn.open_table(STR_TABLE).unwrap();
        assert!(table.insert("hello", "world").unwrap().is_none());
    }
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    let table = read_txn.open_table(STR_TABLE).unwrap();
    assert_eq!("world", table.get("hello").unwrap().unwrap().value());

    let write_txn = db.begin_write().unwrap();
    {
        let mut table = write_txn.open_table(STR_TABLE).unwrap();
        let old_value = table.insert("hello", "replaced").unwrap();
        assert_eq!(old_value.unwrap().value(), "world");
    }
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    let table = read_txn.open_table(STR_TABLE).unwrap();
    assert_eq!("replaced", table.get("hello").unwrap().unwrap().value());
}
fn write_batch_delete_range_basic() {
    let db = default_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();

    let mut wb = db.engine.write_batch();

    wb.delete_range(b"b", b"e").unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());

    let db = multi_batch_write_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();

    let mut wb = db.engine.write_batch_with_cap(1024);
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }

    wb.delete_range(b"b", b"e").unwrap();
    wb.delete_range(&32_usize.to_be_bytes(), &128_usize.to_be_bytes())
        .unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());
    for i in 0..32_usize {
        let x = i.to_be_bytes();
        assert!(db.engine.get_value(&x).unwrap().is_some());
    }
    for i in 32..128_usize {
        let x = i.to_be_bytes();
        assert!(db.engine.get_value(&x).unwrap().is_none());
    }
    for i in 128..256_usize {
        let x = i.to_be_bytes();
        assert!(db.engine.get_value(&x).unwrap().is_some());
    }
}
fn test_cdc_congest() {
    let mut cluster = new_server_cluster(1, 1);
    // Increase the Raft tick interval to make this test case running reliably.
    configure_for_lease_read(&mut cluster.cfg, Some(100), None);
    let memory_quota = 1024; // 1KB
    let mut suite = TestSuiteBuilder::new()
        .cluster(cluster)
        .memory_quota(memory_quota)
        .build();

    let req = suite.new_changedata_request(1);
    let (mut req_tx, _event_feed_wrap, receive_event) =
        new_event_feed(suite.get_region_cdc_client(1));
    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();
    let event = receive_event(false);
    event.events.into_iter().for_each(|e| {
        match e.event.unwrap() {
            // Even if there is no write,
            // it should always outputs an Initialized event.
            Event_oneof_event::Entries(es) => {
                assert!(es.entries.len() == 1, "{:?}", es);
                let e = &es.entries[0];
                assert_eq!(e.get_type(), EventLogType::Initialized, "{:?}", es);
            }
            other => panic!("unknown event {:?}", other),
        }
    });

    // Client must receive messages when there is no congest error.
    let value_size = memory_quota / 2;
    let (k, v) = ("key1".to_owned(), vec![5; value_size]);
    // Prewrite
    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
    let mut mutation = Mutation::default();
    mutation.set_op(Op::Put);
    mutation.key = k.clone().into_bytes();
    mutation.value = v;
    suite.must_kv_prewrite(1, vec![mutation], k.into_bytes(), start_ts);
    let mut events = receive_event(false).events.to_vec();
    assert_eq!(events.len(), 1, "{:?}", events);
    match events.pop().unwrap().event.unwrap() {
        Event_oneof_event::Entries(entries) => {
            assert_eq!(entries.entries.len(), 1);
            assert_eq!(entries.entries[0].get_type(), EventLogType::Prewrite);
        }
        other => panic!("unknown event {:?}", other),
    }

    // Trigger congest error.
    let value_size = memory_quota * 2;
    let (k, v) = ("key2".to_owned(), vec![5; value_size]);
    // Prewrite
    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
    let mut mutation = Mutation::default();
    mutation.set_op(Op::Put);
    mutation.key = k.clone().into_bytes();
    mutation.value = v;
    suite.must_kv_prewrite(1, vec![mutation], k.into_bytes(), start_ts);
    let mut events = receive_event(false).events.to_vec();
    assert_eq!(events.len(), 1, "{:?}", events);
    match events.pop().unwrap().event.unwrap() {
        Event_oneof_event::Error(e) => {
            // Unknown errors are translated into region_not_found.
            assert!(e.has_region_not_found(), "{:?}", e);
        }
        other => panic!("unknown event {:?}", other),
    }

    // The delegate must be removed.
    let scheduler = suite.endpoints.values().next().unwrap().scheduler();
    let (tx, rx) = mpsc::channel();
    scheduler
        .schedule(Task::Validate(Validate::Region(
            1,
            Box::new(move |delegate| {
                tx.send(delegate.is_none()).unwrap();
            }),
        )))
        .unwrap();

    assert!(
        rx.recv_timeout(Duration::from_millis(1000)).unwrap(),
        "find unexpected delegate"
    );
    suite.stop();
}
fn test_client_close() {
    do_test(
        3014,
        |mut cli_sock| {
            cli_sock.send(Message::Text("Hello WebSocket".into())).unwrap();

            let message = cli_sock.read().unwrap(); // receive answer from server
            assert_eq!(message.into_data(), b"From Server");

            cli_sock.close(None).unwrap(); // send close to server

            let message = cli_sock.read().unwrap(); // receive acknowledgement from server
            assert!(message.is_close());

            let err = cli_sock.read().unwrap_err(); // now we should get ConnectionClosed
            match err {
                Error::ConnectionClosed => {}
                _ => panic!("unexpected error: {:?}", err),
            }
        },
        |mut srv_sock| {
            let message = srv_sock.read().unwrap();
            assert_eq!(message.into_data(), b"Hello WebSocket");

            srv_sock.send(Message::Text("From Server".into())).unwrap();

            let message = srv_sock.read().unwrap(); // receive close from client
            assert!(message.is_close());

            let err = srv_sock.read().unwrap_err(); // now we should get ConnectionClosed
            match err {
                Error::ConnectionClosed => {}
                _ => panic!("unexpected error: {:?}", err),
            }
        },
    );
}
fn test_storage_error() {
    let data = vec![(1, Some("name:0"), 2), (2, Some("name:4"), 3)];

    let product = ProductTable::new();
    let (_, endpoint) = init_with_data(&product, &data);
    let req = DagSelect::from(&product).build();

    fail::cfg("kv_cursor_seek", "return()").unwrap();
    let resp = handle_request(&endpoint, req);

    assert!(resp.get_other_error().contains("kv cursor seek error"));
}
fn test_cdc_rawkv_resolved_ts() {
    let mut suite = TestSuite::new(1, ApiVersion::V2);
    let cluster = &suite.cluster;

    let region = cluster.get_region(b"");
    let region_id = region.get_id();
    let leader = region.get_peers()[0].clone();
    let node_id = leader.get_id();
    let ts_provider = cluster.sim.rl().get_causal_ts_provider(node_id).unwrap();

    let env = Arc::new(Environment::new(1));
    let channel =
        ChannelBuilder::new(env).connect(&cluster.sim.rl().get_addr(leader.get_store_id()));
    let client = TikvClient::new(channel);

    let mut req = suite.new_changedata_request(region_id);
    req.set_kv_api(ChangeDataRequestKvApi::RawKv);
    let (mut req_tx, _event_feed_wrap, receive_event) =
        new_event_feed(suite.get_region_cdc_client(region_id));
    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();

    let event = receive_event(false);
    event
        .events
        .into_iter()
        .for_each(|e| match e.event.unwrap() {
            Event_oneof_event::Entries(es) => {
                assert!(es.entries.len() == 1, "{:?}", es);
                let e = &es.entries[0];
                assert_eq!(e.get_type(), EventLogType::Initialized, "{:?}", es);
            }
            other => panic!("unknown event {:?}", other),
        });
    // Sleep a while to make sure the stream is registered.
    sleep_ms(1000);

    let mut ctx = Context::default();
    ctx.set_region_id(region.get_id());
    ctx.set_region_epoch(region.get_region_epoch().clone());
    ctx.set_peer(leader);
    ctx.set_api_version(ApiVersion::V2);
    let mut put_req = RawPutRequest::default();
    put_req.set_context(ctx);
    put_req.key = b"rk3".to_vec();
    put_req.value = b"v3".to_vec();

    let pause_write_fp = "raftkv_async_write";
    fail::cfg(pause_write_fp, "pause").unwrap();
    let ts = block_on(ts_provider.async_get_ts()).unwrap();
    let handle = thread::spawn(move || {
        let _ = client.raw_put(&put_req).unwrap();
    });

    sleep_ms(100);

    let event = receive_event(true).resolved_ts.unwrap();
    assert!(
        ts.next() >= TimeStamp::from(event.ts),
        "{} {}",
        ts,
        TimeStamp::from(event.ts)
    );
    // Receive again to make sure resolved ts <= ongoing request's ts.
    let event = receive_event(true).resolved_ts.unwrap();
    assert!(
        ts.next() >= TimeStamp::from(event.ts),
        "{} {}",
        ts,
        TimeStamp::from(event.ts)
    );

    fail::remove(pause_write_fp);
    handle.join().unwrap();
}
fn custom_ordering() {
    #[derive(Debug)]
    struct ReverseKey(Vec<u8>);

    impl RedbValue for ReverseKey {
        type SelfType<'a> = ReverseKey
        where
        Self: 'a;
        type AsBytes<'a> = &'a [u8]
        where
        Self: 'a;

        fn fixed_width() -> Option<usize> {
            None
        }

        fn from_bytes<'a>(data: &'a [u8]) -> ReverseKey
        where
            Self: 'a,
        {
            ReverseKey(data.to_vec())
        }

        fn as_bytes<'a, 'b: 'a>(value: &'a Self::SelfType<'b>) -> &'a [u8]
        where
            Self: 'a,
            Self: 'b,
        {
            &value.0
        }

        fn type_name() -> TypeName {
            TypeName::new("test::ReverseKey")
        }
    }

    impl RedbKey for ReverseKey {
        fn compare(data1: &[u8], data2: &[u8]) -> Ordering {
            data2.cmp(data1)
        }
    }

    let definition: TableDefinition<ReverseKey, &str> = TableDefinition::new("x");

    let tmpfile = create_tempfile();
    let db = Database::create(tmpfile.path()).unwrap();
    let write_txn = db.begin_write().unwrap();
    {
        let mut table = write_txn.open_table(definition).unwrap();
        for i in 0..10u8 {
            let key = vec![i];
            table.insert(&ReverseKey(key), "value").unwrap();
        }
    }
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    let table = read_txn.open_table(definition).unwrap();
    let start = ReverseKey(vec![7u8]); // ReverseKey is used, so 7 < 3
    let end = ReverseKey(vec![3u8]);
    let mut iter = table.range(start..=end).unwrap();
    for i in (3..=7u8).rev() {
        let (key, value) = iter.next().unwrap().unwrap();
        assert_eq!(&[i], key.value().0.as_slice());
        assert_eq!("value", value.value());
    }
    assert!(iter.next().is_none());
}
fn test_region_heartbeat() {
    let region_id = 2;
    let cluster = Cluster::with_node_count(1, None);
    let router = &cluster.routers[0];

    // When there is only one peer, it should campaign immediately.
    let mut req = RaftCmdRequest::default();
    req.mut_header().set_peer(new_peer(1, 3));
    req.mut_status_request()
        .set_cmd_type(StatusCmdType::RegionLeader);
    let res = router.query(region_id, req.clone()).unwrap();
    let status_resp = res.response().unwrap().get_status_response();
    assert_eq!(
        *status_resp.get_region_leader().get_leader(),
        new_peer(1, 3)
    );

    for _ in 0..5 {
        let resp = block_on(
            cluster
                .node(0)
                .pd_client()
                .get_region_leader_by_id(region_id),
        )
        .unwrap();
        if let Some((region, peer)) = resp {
            assert_eq!(region.get_id(), region_id);
            assert_eq!(peer.get_id(), 3);
            assert_eq!(peer.get_store_id(), 1);
            return;
        }
        std::thread::sleep(std::time::Duration::from_millis(50));
    }
    panic!("failed to get region leader");
}
fn test_router_trace() {
    let (control_tx, control_fsm) = Runner::new(10);
    let (router, mut system) =
        batch_system::create_system(&Config::default(), control_tx, control_fsm, None);
    let builder = Builder::new();
    system.spawn("test".to_owned(), builder);

    let register_runner = |addr| {
        let (sender, runner) = Runner::new(10);
        let mailbox = BasicMailbox::new(sender, runner, router.state_cnt().clone());
        router.register(addr, mailbox);
    };
    let close_runner = |addr| {
        router.close(addr);
    };

    let mut mailboxes = vec![];
    for i in 0..10 {
        register_runner(i);
        mailboxes.push(router.mailbox(i).unwrap());
    }
    assert_eq!(router.alive_cnt(), 10);
    assert_eq!(router.state_cnt().load(Ordering::Relaxed), 11);
    for i in 0..10 {
        close_runner(i);
    }
    assert_eq!(router.alive_cnt(), 0);
    assert_eq!(router.state_cnt().load(Ordering::Relaxed), 11);
    drop(mailboxes);
    assert_eq!(router.alive_cnt(), 0);
    assert_eq!(router.state_cnt().load(Ordering::Relaxed), 1);
}
fn cap_zero() {
    let db = default_engine();
    let mut wb = db.engine.write_batch_with_cap(0);
    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.put(b"c", b"").unwrap();
    wb.put(b"d", b"").unwrap();
    wb.put(b"e", b"").unwrap();
    wb.put(b"f", b"").unwrap();
    wb.write().unwrap();
    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"f").unwrap().is_some());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(0);
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.put(b"c", b"").unwrap();
    wb.put(b"d", b"").unwrap();
    wb.put(b"e", b"").unwrap();
    wb.put(b"f", b"").unwrap();
    wb.write().unwrap();
    assert!(
        db.engine
            .get_value(&0_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(
        db.engine
            .get_value(&123_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(
        db.engine
            .get_value(&255_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"f").unwrap().is_some());
}
fn encode_engine_slice_error_when_buffer_too_small() {
    for num_triples in 1..100 {
        let input = "AAA".repeat(num_triples);
        let mut vec = vec![0; (num_triples - 1) * 4];
        assert_eq!(
            EncodeSliceError::OutputSliceTooSmall,
            STANDARD.encode_slice(&input, &mut vec).unwrap_err()
        );
        vec.push(0);
        assert_eq!(
            EncodeSliceError::OutputSliceTooSmall,
            STANDARD.encode_slice(&input, &mut vec).unwrap_err()
        );
        vec.push(0);
        assert_eq!(
            EncodeSliceError::OutputSliceTooSmall,
            STANDARD.encode_slice(&input, &mut vec).unwrap_err()
        );
        vec.push(0);
        assert_eq!(
            EncodeSliceError::OutputSliceTooSmall,
            STANDARD.encode_slice(&input, &mut vec).unwrap_err()
        );
        vec.push(0);
        assert_eq!(
            num_triples * 4,
            STANDARD.encode_slice(&input, &mut vec).unwrap()
        );
    }
}
fn test_resource_group() {
    let (control_tx, control_fsm) = Runner::new(10);
    let resource_manager = ResourceGroupManager::default();

    let get_group = |name: &str, read_tokens: u64, write_tokens: u64| -> ResourceGroup {
        let mut group = ResourceGroup::new();
        group.set_name(name.to_string());
        group.set_mode(GroupMode::RawMode);
        let mut resource_setting = GroupRawResourceSettings::new();
        resource_setting
            .mut_cpu()
            .mut_settings()
            .set_fill_rate(read_tokens);
        resource_setting
            .mut_io_write()
            .mut_settings()
            .set_fill_rate(write_tokens);
        group.set_raw_resource_settings(resource_setting);
        group
    };

    resource_manager.add_resource_group(get_group("group1", 10, 10));
    resource_manager.add_resource_group(get_group("group2", 100, 100));

    let mut cfg = Config::default();
    cfg.pool_size = 1;
    let (router, mut system) = batch_system::create_system(
        &cfg,
        control_tx,
        control_fsm,
        Some(resource_manager.derive_controller("test".to_string(), false)),
    );
    let builder = Builder::new();
    system.spawn("test".to_owned(), builder);
    let (tx, rx) = mpsc::unbounded();
    let tx_ = tx.clone();
    let r = router.clone();
    let state_cnt = Arc::new(AtomicUsize::new(0));
    router
        .send_control(Message::Callback(Box::new(
            move |_: &Handler, _: &mut Runner| {
                let (tx, runner) = Runner::new(10);
                r.register(1, BasicMailbox::new(tx, runner, state_cnt.clone()));
                let (tx2, runner2) = Runner::new(10);
                r.register(2, BasicMailbox::new(tx2, runner2, state_cnt));
                tx_.send(0).unwrap();
            },
        )))
        .unwrap();
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(0));

    let tx_ = tx.clone();
    let (tx1, rx1) = std::sync::mpsc::sync_channel(0);
    // block the thread
    router
        .send_control(Message::Callback(Box::new(
            move |_: &Handler, _: &mut Runner| {
                tx_.send(0).unwrap();
                tx1.send(0).unwrap();
            },
        )))
        .unwrap();
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(0));

    router
        .send(1, Message::Resource("group1".to_string(), 1))
        .unwrap();
    let tx_ = tx.clone();
    router
        .send(
            1,
            Message::Callback(Box::new(move |_: &Handler, _: &mut Runner| {
                tx_.send(1).unwrap();
            })),
        )
        .unwrap();

    router
        .send(2, Message::Resource("group2".to_string(), 1))
        .unwrap();
    router
        .send(
            2,
            Message::Callback(Box::new(move |_: &Handler, _: &mut Runner| {
                tx.send(2).unwrap();
            })),
        )
        .unwrap();

    // pause the blocking thread
    assert_eq!(rx1.recv_timeout(Duration::from_secs(3)), Ok(0));

    // should recv from group2 first, because group2 has more tokens and it would be
    // handled with higher priority.
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(2));
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(1));
}
fn json_with_comments() {
    let f = super::fixture_root().join("parcel/tsconfig/trailing-comma");

    let resolver = Resolver::new(ResolveOptions {
        tsconfig: Some(TsconfigOptions {
            config_file: f.join("tsconfig.json"),
            references: TsconfigReferences::Auto,
        }),
        ..ResolveOptions::default()
    });

    let resolved_path = resolver.resolve(&f, "foo").map(|f| f.full_path());
    assert_eq!(resolved_path, Ok(f.join("bar.js")));
}
fn no_dirty_reads() {
    let tmpfile = create_tempfile();
    let db = Database::create(tmpfile.path()).unwrap();
    let write_txn = db.begin_write().unwrap();
    {
        let mut table = write_txn.open_table(STR_TABLE).unwrap();
        table.insert("hello", "world").unwrap();
    }

    let read_txn = db.begin_read().unwrap();
    let table = read_txn.open_table(STR_TABLE);
    assert!(table.is_err());
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    let table = read_txn.open_table(STR_TABLE).unwrap();
    assert_eq!("world", table.get("hello").unwrap().unwrap().value());
}
fn new_engine_opt_renamed_dir() {
    use std::sync::Arc;
    let dir = tempdir();
    let root_path = dir.path();

    let encryption_cfg = test_util::new_file_security_config(root_path);
    let key_manager = Arc::new(
        data_key_manager_from_config(&encryption_cfg, root_path.to_str().unwrap())
            .unwrap()
            .unwrap(),
    );

    let mut db_opts = DbOptions::default();
    db_opts.set_key_manager(Some(key_manager.clone()));
    let cf_opts: Vec<_> = ALL_CFS.iter().map(|cf| (*cf, CfOptions::new())).collect();

    let path = root_path.join("missing").to_str().unwrap().to_owned();
    {
        let db = KvTestEngine::new_kv_engine_opt(&path, db_opts.clone(), cf_opts.clone()).unwrap();
        db.put(b"foo", b"bar").unwrap();
        db.sync().unwrap();
    }
    let new_path = root_path.join("new").to_str().unwrap().to_owned();
    key_manager.link_file(&path, &new_path).unwrap();
    fs::rename(&path, &new_path).unwrap();
    key_manager.delete_file(&path).unwrap();
    {
        let db =
            KvTestEngine::new_kv_engine_opt(&new_path, db_opts.clone(), cf_opts.clone()).unwrap();
        assert_eq!(
            db.get_value_cf(CF_DEFAULT, b"foo").unwrap().unwrap(),
            b"bar"
        );
    }
}
fn write_flush_behaviour() {
    const SEND_ME_LEN: usize = 10;
    const BATCH_ME_LEN: usize = 11;
    const WRITE_BUFFER_SIZE: usize = 600;

    let mut ws = WebSocket::from_raw_socket(
        MockWrite::default(),
        tungstenite::protocol::Role::Server,
        Some(WebSocketConfig { write_buffer_size: WRITE_BUFFER_SIZE, ..<_>::default() }),
    );

    assert_eq!(ws.get_ref().written_bytes, 0);
    assert_eq!(ws.get_ref().write_count, 0);
    assert_eq!(ws.get_ref().flush_count, 0);

    // `send` writes & flushes immediately
    ws.send(Message::Text("Send me!".into())).unwrap();
    assert_eq!(ws.get_ref().written_bytes, SEND_ME_LEN);
    assert_eq!(ws.get_ref().write_count, 1);
    assert_eq!(ws.get_ref().flush_count, 1);

    // send a batch of messages
    for msg in (0..100).map(|_| Message::Text("Batch me!".into())) {
        ws.write(msg).unwrap();
    }
    // after 55 writes the out_buffer will exceed write_buffer_size=600
    // and so do a single underlying write (not flushing).
    assert_eq!(ws.get_ref().written_bytes, 55 * BATCH_ME_LEN + SEND_ME_LEN);
    assert_eq!(ws.get_ref().write_count, 2);
    assert_eq!(ws.get_ref().flush_count, 1);

    // flushing will perform a single write for the remaining out_buffer & flush.
    ws.flush().unwrap();
    assert_eq!(ws.get_ref().written_bytes, 100 * BATCH_ME_LEN + SEND_ME_LEN);
    assert_eq!(ws.get_ref().write_count, 3);
    assert_eq!(ws.get_ref().flush_count, 2);
}
fn test_destroy_by_larger_id_while_applying() {
    let fp = "APPLY_COMMITTED_ENTRIES";
    let mut cluster = Cluster::default();
    let router = &cluster.routers[0];
    router.wait_applied_to_current_term(2, Duration::from_secs(3));

    fail::cfg(fp, "pause").unwrap();

    let header = Box::new(router.new_request_for(2).take_header());
    let mut put = SimpleWriteEncoder::with_capacity(64);
    put.put(CF_DEFAULT, b"key", b"value");
    let (msg, mut sub) = PeerMsg::simple_write(header.clone(), put.clone().encode());
    router.send(2, msg).unwrap();
    assert!(block_on(sub.wait_committed()));

    let mut larger_id_msg = Box::<RaftMessage>::default();
    larger_id_msg.set_region_id(2);
    let mut target_peer = header.get_peer().clone();
    target_peer.set_id(target_peer.get_id() + 1);
    larger_id_msg.set_to_peer(target_peer.clone());
    larger_id_msg.set_region_epoch(header.get_region_epoch().clone());
    larger_id_msg
        .mut_region_epoch()
        .set_conf_ver(header.get_region_epoch().get_conf_ver() + 1);
    larger_id_msg.set_from_peer(new_peer(2, 8));
    let raft_message = larger_id_msg.mut_message();
    raft_message.set_msg_type(MessageType::MsgHeartbeat);
    raft_message.set_from(8);
    raft_message.set_to(target_peer.get_id());
    raft_message.set_term(10);

    // Larger ID should trigger destroy.
    router.send_raft_message(larger_id_msg).unwrap();
    fail::remove(fp);
    assert_peer_not_exist(2, header.get_peer().get_id(), router);
    let meta = router
        .must_query_debug_info(2, Duration::from_secs(3))
        .unwrap();
    assert_eq!(meta.raft_status.id, target_peer.get_id());
    assert_eq!(meta.raft_status.hard_state.term, 10);

    std::thread::sleep(Duration::from_millis(10));

    // New peer should survive restart.
    cluster.restart(0);
    let router = &cluster.routers[0];
    let meta = router
        .must_query_debug_info(2, Duration::from_secs(3))
        .unwrap();
    assert_eq!(meta.raft_status.id, target_peer.get_id());
    assert_eq!(meta.raft_status.hard_state.term, 10);
}
fn test_evil_server_close() {
    do_test(
        3013,
        |mut cli_sock| {
            cli_sock.send(Message::Text("Hello WebSocket".into())).unwrap();

            sleep(Duration::from_secs(1));

            let message = cli_sock.read().unwrap(); // receive close from server
            assert!(message.is_close());

            let err = cli_sock.read().unwrap_err(); // now we should get ConnectionClosed
            match err {
                Error::ConnectionClosed => {}
                _ => panic!("unexpected error: {:?}", err),
            }
        },
        |mut srv_sock| {
            let message = srv_sock.read().unwrap();
            assert_eq!(message.into_data(), b"Hello WebSocket");

            srv_sock.close(None).unwrap(); // send close to client

            let message = srv_sock.read().unwrap(); // receive acknowledgement
            assert!(message.is_close());
            // and now just drop the connection without waiting for `ConnectionClosed`
            srv_sock.get_mut().set_linger(Some(Duration::from_secs(0))).unwrap();
            drop(srv_sock);
        },
    );
}
fn should_write_to_engine() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();
    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;

    let mut key = vec![];
    loop {
        key.push(b'a');
        wb.put(&key, b"").unwrap();
        if key.len() <= max_keys {
            assert!(!wb.should_write_to_engine());
        }
        if key.len() == max_keys + 1 {
            assert!(wb.should_write_to_engine());
            wb.write().unwrap();
            break;
        }
    }

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = KvTestEngine::WRITE_BATCH_MAX_KEYS;

    let mut key = vec![];
    loop {
        key.push(b'a');
        wb.put(&key, b"").unwrap();
        if key.len() <= max_keys {
            assert!(!wb.should_write_to_engine());
        }
        if key.len() == max_keys + 1 {
            assert!(wb.should_write_to_engine());
            wb.write().unwrap();
            break;
        }
    }
}
fn cf_names() {
    let db = engine_cfs(ALL_CFS);
    let names = db.engine.cf_names();
    assert_eq!(names.len(), ALL_CFS.len());
    for cf in ALL_CFS {
        assert!(names.contains(cf));
    }
}
fn test_deadline() {
    let product = ProductTable::new();
    let (_, endpoint) = init_with_data(&product, &[]);
    let req = DagSelect::from(&product).build();

    fail::cfg("deadline_check_fail", "return()").unwrap();
    let resp = handle_request(&endpoint, req);

    assert!(resp.get_other_error().contains("exceeding the deadline"));
}
fn write_batch_delete_range_empty_range() {
    let db = default_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();

    let mut wb = db.engine.write_batch();

    wb.delete_range(b"b", b"b").unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_some());
    assert!(db.engine.get_value(b"c").unwrap().is_some());

    let db = multi_batch_write_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        db.engine.put(&x, &x).unwrap();
    }

    let mut wb = db.engine.write_batch_with_cap(1024);

    wb.delete_range(b"b", b"b").unwrap();
    wb.delete_range(&1_usize.to_be_bytes(), &1_usize.to_be_bytes())
        .unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_some());
    assert!(db.engine.get_value(b"c").unwrap().is_some());
    for i in 0..256_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());
    }
}
fn test_txn_create_compaction_filter() {
    GC_COMPACTION_FILTER_PERFORM.reset();
    GC_COMPACTION_FILTER_SKIP.reset();

    let mut cfg = DbConfig::default();
    cfg.writecf.disable_auto_compactions = true;
    cfg.writecf.dynamic_level_bytes = false;
    let dir = tempfile::TempDir::new().unwrap();
    let builder = TestEngineBuilder::new().path(dir.path());
    let mut engine = builder.build_with_cfg(&cfg).unwrap();
    let raw_engine = engine.get_rocksdb();

    let mut gc_runner = TestGcRunner::new(0);
    let value = vec![b'v'; 512];

    must_prewrite_put(&mut engine, b"zkey", &value, b"zkey", 100);
    must_commit(&mut engine, b"zkey", 100, 110);

    gc_runner
        .safe_point(TimeStamp::new(1).into_inner())
        .gc(&raw_engine);
    assert_eq!(
        GC_COMPACTION_FILTER_PERFORM
            .with_label_values(&[STAT_TXN_KEYMODE])
            .get(),
        1
    );
    assert_eq!(
        GC_COMPACTION_FILTER_SKIP
            .with_label_values(&[STAT_TXN_KEYMODE])
            .get(),
        1
    );

    GC_COMPACTION_FILTER_PERFORM.reset();
    GC_COMPACTION_FILTER_SKIP.reset();
}
fn write_batch_delete_range_twice_1() {
    let db = default_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();

    let mut wb = db.engine.write_batch();

    wb.delete_range(b"b", b"e").unwrap();
    wb.delete_range(b"b", b"e").unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());

    let db = multi_batch_write_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        db.engine.put(&x, &x).unwrap();
    }

    let mut wb = db.engine.write_batch_with_cap(1024);

    wb.delete_range(b"b", b"e").unwrap();
    wb.delete_range(b"b", b"e").unwrap();
    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())
        .unwrap();
    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())
        .unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());
    assert!(
        db.engine
            .get_value(&0_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    for i in 1..256_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
}
fn external_sst_info_key_values_with_delete() -> Result<()> {
    let tempdir = tempdir();
    let sst_path = tempdir
        .path()
        .join("test-data.sst")
        .to_string_lossy()
        .to_string();
    let sst_builder = <KvTestEngine as SstExt>::SstWriterBuilder::new();
    let mut sst_writer = sst_builder.build(&sst_path)?;

    sst_writer.delete(b"k1")?;

    let info = sst_writer.finish()?;

    assert_eq!(b"k1", info.smallest_key());
    assert_eq!(b"k1", info.largest_key());
    assert_eq!(1, info.num_entries());

    let size = fs::metadata(&sst_path).unwrap().len();

    assert_eq!(size, info.file_size());

    Ok(())
}
fn write_batch_delete_range_inexact() {
    let db = default_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();
    db.engine.put(b"g", b"").unwrap();

    let mut wb = db.engine.write_batch();

    wb.delete_range(b"b", b"f").unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_none());
    assert!(db.engine.get_value(b"f").unwrap().is_none());
    assert!(db.engine.get_value(b"g").unwrap().is_some());

    let db = multi_batch_write_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();
    db.engine.put(b"g", b"").unwrap();

    let mut wb = db.engine.write_batch_with_cap(1024);
    for i in (0..256_usize).step_by(2) {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }

    wb.delete_range(b"b", b"f").unwrap();
    wb.delete_range(&0_usize.to_be_bytes(), &252_usize.to_be_bytes())
        .unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_none());
    assert!(db.engine.get_value(b"f").unwrap().is_none());
    assert!(db.engine.get_value(b"g").unwrap().is_some());
    for i in 0..252_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
    assert!(
        db.engine
            .get_value(&252_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(
        db.engine
            .get_value(&253_usize.to_be_bytes())
            .unwrap()
            .is_none()
    );
    assert!(
        db.engine
            .get_value(&254_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
}
fn save_point_rollback_two() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    wb.set_save_point();
    wb.put(b"a", b"").unwrap();
    wb.set_save_point();
    wb.put(b"b", b"").unwrap();

    wb.rollback_to_save_point().unwrap();
    wb.rollback_to_save_point().unwrap();

    let err = wb.rollback_to_save_point();
    assert_engine_error(err);
    let err = wb.pop_save_point();
    assert_engine_error(err);
    wb.write().unwrap();
    let a = db.engine.get_value(b"a").unwrap();
    assert!(a.is_none());
    let b = db.engine.get_value(b"b").unwrap();
    assert!(b.is_none());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = 256_usize;

    wb.set_save_point();
    for i in 0..max_keys {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();
    wb.set_save_point();
    for i in max_keys..2 * max_keys {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"b", b"").unwrap();

    wb.rollback_to_save_point().unwrap();
    wb.rollback_to_save_point().unwrap();

    let err = wb.rollback_to_save_point();
    assert_engine_error(err);
    let err = wb.pop_save_point();
    assert_engine_error(err);
    wb.write().unwrap();
    let a = db.engine.get_value(b"a").unwrap();
    assert!(a.is_none());
    let b = db.engine.get_value(b"b").unwrap();
    assert!(b.is_none());
    for i in 0..2 * max_keys {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
}
fn write_batch_write_twice_1() {
    let db = default_engine();

    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"aa").unwrap();

    wb.write().unwrap();
    wb.write().unwrap();

    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"aa");

    let db = multi_batch_write_engine();

    let mut wb = db.engine.write_batch_with_cap(1024);

    for i in 0..123_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"aa").unwrap();

    wb.write().unwrap();
    wb.write().unwrap();

    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"aa");
    for i in 0..123_usize {
        let x = i.to_be_bytes();
        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);
    }
}
fn save_point_rollback_partial() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"").unwrap();
    wb.set_save_point();
    wb.put(b"b", b"").unwrap();

    wb.rollback_to_save_point().unwrap();
    wb.write().unwrap();
    let a = db.engine.get_value(b"a").unwrap();
    assert!(a.is_some());
    let b = db.engine.get_value(b"b").unwrap();
    assert!(b.is_none());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = 256_usize;

    for i in 0..max_keys {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();
    wb.set_save_point();
    wb.put(b"b", b"").unwrap();
    for i in max_keys..2 * max_keys {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }

    wb.rollback_to_save_point().unwrap();
    wb.write().unwrap();
    let a = db.engine.get_value(b"a").unwrap();
    assert!(a.is_some());
    for i in 0..max_keys {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());
    }
    let b = db.engine.get_value(b"b").unwrap();
    assert!(b.is_none());
    for i in max_keys..2 * max_keys {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
}
fn write_batch_clear() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.clear();
    assert!(wb.is_empty());
    assert_eq!(wb.count(), 0);
    wb.write().unwrap();
    assert!(db.engine.get_value(b"a").unwrap().is_none());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);

    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.clear();
    assert!(wb.is_empty());
    assert_eq!(wb.count(), 0);
    wb.write().unwrap();
    for i in 0..256_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
}
fn test_majority_disk_full() {
    let mut cluster = new_node_cluster(0, 3);
    // To ensure the thread has full store disk usage infomation.
    cluster.cfg.raft_store.store_batch_system.pool_size = 1;
    cluster.pd_client.disable_default_operator();
    cluster.run();

    // To ensure all replicas are not pending.
    cluster.must_put(b"k1", b"v1");
    must_get_equal(&cluster.get_engine(1), b"k1", b"v1");
    must_get_equal(&cluster.get_engine(2), b"k1", b"v1");
    must_get_equal(&cluster.get_engine(3), b"k1", b"v1");

    cluster.must_transfer_leader(1, new_peer(1, 1));
    let region = cluster.get_region(b"k1");
    let epoch = region.get_region_epoch().clone();

    // To ensure followers have reported disk usages to the leader.
    for i in 1..3 {
        fail::cfg(get_fp(DiskUsage::AlmostFull, i + 1), "return").unwrap();
        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);
    }

    // Normal proposals will be rejected because of majority peers' disk full.
    let mut ch = cluster.async_put(b"k2", b"v2").unwrap();
    let resp = ch.recv_timeout(Duration::from_secs(1)).unwrap();
    assert_eq!(disk_full_stores(&resp), vec![2, 3]);

    // Proposals with special `DiskFullOpt`s can be accepted even if all peers are
    // disk full.
    fail::cfg(get_fp(DiskUsage::AlmostFull, 1), "return").unwrap();
    let reqs = vec![new_put_cmd(b"k3", b"v3")];
    let put = new_request(1, epoch.clone(), reqs, false);
    let mut opts = RaftCmdExtraOpts::default();
    opts.disk_full_opt = DiskFullOpt::AllowedOnAlmostFull;
    let mut ch = cluster.async_request_with_opts(put, opts).unwrap();
    let resp = ch.recv_timeout(Duration::from_secs(1)).unwrap();
    assert!(!resp.get_header().has_error());

    // Reset disk full status for peer 2 and 3. 2 follower reads must success
    // because the leader will continue to append entries to followers after the
    // new disk usages are reported.
    for i in 1..3 {
        fail::remove(get_fp(DiskUsage::AlmostFull, i + 1));
        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);
        must_get_equal(&cluster.get_engine(i + 1), b"k3", b"v3");
    }

    // To ensure followers have reported disk usages to the leader.
    for i in 1..3 {
        fail::cfg(get_fp(DiskUsage::AlreadyFull, i + 1), "return").unwrap();
        ensure_disk_usage_is_reported(&mut cluster, i + 1, i + 1, &region);
    }

    // Proposals with special `DiskFullOpt`s will still be rejected if majority
    // peers are already disk full.
    let reqs = vec![new_put_cmd(b"k3", b"v3")];
    let put = new_request(1, epoch.clone(), reqs, false);
    let mut opts = RaftCmdExtraOpts::default();
    opts.disk_full_opt = DiskFullOpt::AllowedOnAlmostFull;
    let mut ch = cluster.async_request_with_opts(put, opts).unwrap();
    let resp = ch.recv_timeout(Duration::from_secs(10)).unwrap();
    assert_eq!(disk_full_stores(&resp), vec![2, 3]);

    // Peer 2 disk usage changes from already full to almost full.
    fail::remove(get_fp(DiskUsage::AlreadyFull, 2));
    fail::cfg(get_fp(DiskUsage::AlmostFull, 2), "return").unwrap();
    ensure_disk_usage_is_reported(&mut cluster, 2, 2, &region);

    // Configuration change should be alloed.
    cluster.pd_client.must_remove_peer(1, new_peer(2, 2));

    // After the last configuration change is applied, the raft group will be like
    // `[(1, DiskUsage::AlmostFull), (3, DiskUsage::AlreadyFull)]`. So no more
    // proposals should be allowed.
    let reqs = vec![new_put_cmd(b"k4", b"v4")];
    let put = new_request(1, epoch, reqs, false);
    let mut opts = RaftCmdExtraOpts::default();
    opts.disk_full_opt = DiskFullOpt::AllowedOnAlmostFull;
    let mut ch = cluster.async_request_with_opts(put, opts).unwrap();
    let resp = ch.recv_timeout(Duration::from_secs(1)).unwrap();
    assert_eq!(disk_full_stores(&resp), vec![3]);

    for i in 0..3 {
        fail::remove(get_fp(DiskUsage::AlreadyFull, i + 1));
        fail::remove(get_fp(DiskUsage::AlmostFull, i + 1));
    }
}
fn test_basic_write() {
    let cluster = Cluster::default();
    let router = &cluster.routers[0];
    let header = Box::new(router.new_request_for(2).take_header());
    let mut put = SimpleWriteEncoder::with_capacity(64);
    put.put(CF_DEFAULT, b"key", b"value");

    router.wait_applied_to_current_term(2, Duration::from_secs(3));

    // Good proposal should be committed.
    let (msg, mut sub) = PeerMsg::simple_write(header.clone(), put.clone().encode());
    router.send(2, msg).unwrap();
    assert!(block_on(sub.wait_proposed()));
    assert!(block_on(sub.wait_committed()));
    let resp = block_on(sub.result()).unwrap();
    assert!(!resp.get_header().has_error(), "{:?}", resp);

    // Store id should be checked.
    let mut invalid_header = header.clone();
    invalid_header.set_peer(new_peer(3, 3));
    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();
    assert!(
        resp.get_header().get_error().has_store_not_match(),
        "{:?}",
        resp
    );

    // Peer id should be checked.
    invalid_header = header.clone();
    invalid_header.set_peer(new_peer(1, 1));
    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();
    assert!(resp.get_header().has_error(), "{:?}", resp);

    // Epoch should be checked.
    invalid_header = header.clone();
    invalid_header
        .mut_region_epoch()
        .set_version(INIT_EPOCH_VER - 1);
    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();
    assert!(
        resp.get_header().get_error().has_epoch_not_match(),
        "{:?}",
        resp
    );

    // Term should be checked if set.
    invalid_header = header.clone();
    invalid_header.set_term(1);
    let resp = router.simple_write(2, invalid_header, put.clone()).unwrap();
    assert!(
        resp.get_header().get_error().has_stale_command(),
        "{:?}",
        resp
    );

    // Too large message can cause regression and should be rejected.
    let mut invalid_put = SimpleWriteEncoder::with_capacity(9 * 1024 * 1024);
    invalid_put.put(CF_DEFAULT, b"key", &vec![0; 8 * 1024 * 1024]);
    let resp = router.simple_write(2, header.clone(), invalid_put).unwrap();
    assert!(
        resp.get_header().get_error().has_raft_entry_too_large(),
        "{:?}",
        resp
    );

    // Make it step down and follower should reject write.
    let mut msg = Box::<RaftMessage>::default();
    msg.set_region_id(2);
    msg.set_to_peer(new_peer(1, 3));
    msg.mut_region_epoch().set_conf_ver(INIT_EPOCH_CONF_VER);
    msg.set_from_peer(new_peer(2, 4));
    let raft_message = msg.mut_message();
    raft_message.set_msg_type(raft::prelude::MessageType::MsgHeartbeat);
    raft_message.set_from(4);
    raft_message.set_term(8);
    router.send_raft_message(msg).unwrap();
    let resp = router.simple_write(2, header, put).unwrap();
    assert!(resp.get_header().get_error().has_not_leader(), "{:?}", resp);
}
fn delete() -> Result<()> {
    let tempdir = tempdir();
    let sst_path = tempdir
        .path()
        .join("test-data.sst")
        .to_string_lossy()
        .to_string();
    let sst_builder = <KvTestEngine as SstExt>::SstWriterBuilder::new();
    let mut sst_writer = sst_builder.build(&sst_path)?;

    sst_writer.delete(b"k1")?;
    sst_writer.finish()?;

    let sst_reader = <KvTestEngine as SstExt>::SstReader::open(&sst_path)?;
    let mut iter = sst_reader.iter(IterOptions::default()).unwrap();

    iter.seek_to_first()?;

    assert_eq!(iter.valid()?, false);

    iter.prev().unwrap_err();
    iter.next().unwrap_err();
    recover_safe(|| {
        iter.key();
    })
    .unwrap_err();
    recover_safe(|| {
        iter.value();
    })
    .unwrap_err();

    assert_eq!(iter.seek_to_first()?, false);
    assert_eq!(iter.seek_to_last()?, false);
    assert_eq!(iter.seek(b"foo")?, false);
    assert_eq!(iter.seek_for_prev(b"foo")?, false);

    Ok(())
}
fn write_batch_is_empty() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    assert!(wb.is_empty());
    wb.put(b"a", b"").unwrap();
    assert!(!wb.is_empty());
    wb.write().unwrap();
    assert!(!wb.is_empty());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);

    assert!(wb.is_empty());
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    assert!(!wb.is_empty());
    wb.write().unwrap();
    assert!(!wb.is_empty());
}
fn write_batch_write_twice_3() {
    let db = default_engine();

    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"aa").unwrap();

    wb.write().unwrap();
    db.engine.put(b"a", b"b").unwrap();
    wb.put(b"b", b"bb").unwrap();
    wb.write().unwrap();

    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"aa");
    assert_eq!(db.engine.get_value(b"b").unwrap().unwrap(), b"bb");

    let db = multi_batch_write_engine();

    let mut wb = db.engine.write_batch_with_cap(1024);

    for i in 0..128_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"aa").unwrap();

    wb.write().unwrap();
    for i in 0..128_usize {
        let k = i.to_be_bytes();
        let v = (2 * i + 1).to_be_bytes();
        db.engine.put(&k, &v).unwrap();
    }
    db.engine.put(b"a", b"b").unwrap();
    for i in 128..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"b", b"bb").unwrap();
    wb.write().unwrap();

    assert_eq!(db.engine.get_value(b"a").unwrap().unwrap(), b"aa");
    assert_eq!(db.engine.get_value(b"b").unwrap().unwrap(), b"bb");
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        assert_eq!(db.engine.get_value(&x).unwrap().unwrap(), &x);
    }
}
fn test_file_dict_file_record_corrupted() {
    let tempdir = tempfile::tempdir().unwrap();
    let mut file_dict_file = FileDictionaryFile::new(
        tempdir.path(),
        "test_file_dict_file_record_corrupted_1",
        true,
        10, // file_rewrite_threshold
    )
    .unwrap();
    let info1 = create_file_info(1, EncryptionMethod::Aes256Ctr);
    let info2 = create_file_info(2, EncryptionMethod::Unknown);
    // 9 represents that the first 9 bytes will be discarded.
    // Crc32 (4 bytes) + File name length (2 bytes) + FileInfo length (2 bytes) +
    // Log type (1 bytes)
    fail::cfg("file_dict_log_append_incomplete", "return(9)").unwrap();
    file_dict_file.insert("info1", &info1, true).unwrap();
    fail::remove("file_dict_log_append_incomplete");
    file_dict_file.insert("info2", &info2, true).unwrap();
    // Intermediate record damage is not allowed.
    file_dict_file.recovery().unwrap_err();

    let mut file_dict_file = FileDictionaryFile::new(
        tempdir.path(),
        "test_file_dict_file_record_corrupted_2",
        true,
        10, // file_rewrite_threshold
    )
    .unwrap();
    let info1 = create_file_info(1, EncryptionMethod::Aes256Ctr);
    let info2 = create_file_info(2, EncryptionMethod::Unknown);
    file_dict_file.insert("info1", &info1, true).unwrap();
    fail::cfg("file_dict_log_append_incomplete", "return(9)").unwrap();
    file_dict_file.insert("info2", &info2, true).unwrap();
    fail::remove("file_dict_log_append_incomplete");
    // The ending record can be discarded.
    let file_dict = file_dict_file.recovery().unwrap();
    assert_eq!(*file_dict.files.get("info1").unwrap(), info1);
    assert_eq!(file_dict.files.len(), 1);
}
fn write_batch_delete_range_backward_range() {
    let db = default_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();

    let mut wb = db.engine.write_batch();

    wb.delete_range(b"c", b"a").unwrap();
    recover_safe(|| {
        wb.write().unwrap();
    })
    .unwrap_err();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_some());
    assert!(db.engine.get_value(b"c").unwrap().is_some());

    let db = multi_batch_write_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"b", b"").unwrap();
    db.engine.put(b"c", b"").unwrap();

    for i in 0..256_usize {
        let x = i.to_be_bytes();
        db.engine.put(&x, &x).unwrap();
    }

    let mut wb = db.engine.write_batch_with_cap(1024);

    wb.delete_range(b"c", b"a").unwrap();
    wb.delete_range(&256_usize.to_be_bytes(), &0_usize.to_be_bytes())
        .unwrap();

    recover_safe(|| {
        wb.write().unwrap();
    })
    .unwrap_err();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_some());
    assert!(db.engine.get_value(b"c").unwrap().is_some());
    for i in 0..256_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());
    }
}
fn save_points_and_counts() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.put(b"a", b"").unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.rollback_to_save_point().unwrap();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.put(b"a", b"").unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.pop_save_point().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.clear();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.put(b"a", b"").unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.write().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.rollback_to_save_point().unwrap();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.put(b"a", b"").unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.write().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.pop_save_point().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), 1);

    wb.clear();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = 256_usize;

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    for i in 0..max_keys {
        wb.put(&i.to_be_bytes(), b"").unwrap();
    }

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.rollback_to_save_point().unwrap();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    for i in 0..max_keys {
        wb.put(&i.to_be_bytes(), b"").unwrap();
    }

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.pop_save_point().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.clear();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    for i in 0..max_keys {
        wb.put(&i.to_be_bytes(), b"").unwrap();
    }

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.write().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.rollback_to_save_point().unwrap();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    wb.set_save_point();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);

    for i in 0..max_keys {
        wb.put(&i.to_be_bytes(), b"").unwrap();
    }

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.write().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.pop_save_point().unwrap();

    assert_eq!(wb.is_empty(), false);
    assert_eq!(wb.count(), max_keys);

    wb.clear();

    assert_eq!(wb.is_empty(), true);
    assert_eq!(wb.count(), 0);
}
fn test_flashback() {
    let mut cluster = new_server_cluster(0, 1);
    cluster.cfg.resolved_ts.advance_ts_interval = ReadableDuration::millis(50);
    let mut suite = TestSuiteBuilder::new().cluster(cluster).build();

    let key = Key::from_raw(b"a");
    let region = suite.cluster.get_region(key.as_encoded());
    let region_id = region.get_id();
    let req = suite.new_changedata_request(region_id);
    let (mut req_tx, _, receive_event) = new_event_feed(suite.get_region_cdc_client(region_id));
    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();
    let event = receive_event(false);
    event.events.into_iter().for_each(|e| {
        match e.event.unwrap() {
            // Even if there is no write,
            // it should always outputs an Initialized event.
            Event_oneof_event::Entries(es) => {
                assert!(es.entries.len() == 1, "{:?}", es);
                let e = &es.entries[0];
                assert_eq!(e.get_type(), EventLogType::Initialized, "{:?}", es);
            }
            other => panic!("unknown event {:?}", other),
        }
    });
    // Sleep a while to make sure the stream is registered.
    sleep_ms(1000);
    let start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
    for i in 0..2 {
        let (k, v) = (
            format!("key{}", i).as_bytes().to_vec(),
            format!("value{}", i).as_bytes().to_vec(),
        );
        // Prewrite
        let start_ts1 = block_on(suite.cluster.pd_client.get_tso()).unwrap();
        let mut mutation = Mutation::default();
        mutation.set_op(Op::Put);
        mutation.key = k.clone();
        mutation.value = v;
        suite.must_kv_prewrite(1, vec![mutation], k.clone(), start_ts1);
        // Commit
        let commit_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
        suite.must_kv_commit(1, vec![k.clone()], start_ts1, commit_ts);
    }
    let (start_key, end_key) = (b"key0".to_vec(), b"key2".to_vec());
    // Prepare flashback.
    let flashback_start_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
    suite.must_kv_prepare_flashback(region_id, &start_key, &end_key, flashback_start_ts);
    // resolved ts should not be advanced anymore.
    let mut counter = 0;
    let mut last_resolved_ts = 0;
    loop {
        let event = receive_event(true);
        if let Some(resolved_ts) = event.resolved_ts.as_ref() {
            if resolved_ts.ts == last_resolved_ts {
                counter += 1;
            }
            last_resolved_ts = resolved_ts.ts;
        }
        if counter > 20 {
            break;
        }
        sleep_ms(50);
    }
    // Flashback.
    let flashback_commit_ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
    suite.must_kv_flashback(
        region_id,
        &start_key,
        &end_key,
        flashback_start_ts,
        flashback_commit_ts,
        start_ts,
    );
    // Check the flashback event.
    let mut resolved_ts = 0;
    let mut event_counter = 0;
    loop {
        let mut cde = receive_event(true);
        if cde.get_resolved_ts().get_ts() > resolved_ts {
            resolved_ts = cde.get_resolved_ts().get_ts();
        }
        let events = cde.mut_events();
        if !events.is_empty() {
            assert_eq!(events.len(), 1);
            match events.pop().unwrap().event.unwrap() {
                Event_oneof_event::Entries(entries) => {
                    assert_eq!(entries.entries.len(), 1);
                    event_counter += 1;
                    let e = &entries.entries[0];
                    assert!(e.commit_ts > resolved_ts);
                    assert_eq!(e.get_op_type(), EventRowOpType::Delete);
                    match e.get_type() {
                        EventLogType::Committed => {
                            // First entry should be a 1PC flashback.
                            assert_eq!(e.get_key(), b"key1");
                            assert_eq!(event_counter, 1);
                        }
                        EventLogType::Commit => {
                            // Second entry should be a 2PC commit.
                            assert_eq!(e.get_key(), b"key0");
                            assert_eq!(event_counter, 2);
                            break;
                        }
                        _ => panic!("unknown event type {:?}", e.get_type()),
                    }
                }
                other => panic!("unknown event {:?}", other),
            }
        }
    }
}
fn test_snapshot_failed() {
    let product = ProductTable::new();
    let (_, endpoint) = init_with_data(&product, &[]);
    let req = DagSelect::from(&product).build();

    fail::cfg("rockskv_async_snapshot", "return()").unwrap();
    let resp = handle_request(&endpoint, req);

    assert!(resp.get_other_error().contains("snapshot failed"));
}
fn write_batch_count() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    assert_eq!(wb.count(), 0);
    wb.put(b"a", b"").unwrap();
    assert_eq!(wb.count(), 1);
    wb.write().unwrap();
    assert_eq!(wb.count(), 1);

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    assert_eq!(wb.count(), 0);
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    assert_eq!(wb.count(), 256);
    wb.write().unwrap();
    assert_eq!(wb.count(), 256);
}
fn cap_two() {
    let db = default_engine();
    let mut wb = db.engine.write_batch_with_cap(2);
    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.put(b"c", b"").unwrap();
    wb.put(b"d", b"").unwrap();
    wb.put(b"e", b"").unwrap();
    wb.put(b"f", b"").unwrap();
    wb.write().unwrap();
    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"f").unwrap().is_some());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(2);

    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.put(b"c", b"").unwrap();
    wb.put(b"d", b"").unwrap();
    wb.put(b"e", b"").unwrap();
    wb.put(b"f", b"").unwrap();
    wb.write().unwrap();
    assert!(
        db.engine
            .get_value(&0_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(
        db.engine
            .get_value(&123_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(
        db.engine
            .get_value(&255_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"f").unwrap().is_some());
}
fn test_status() {
    let cluster = Cluster::default();
    let router = &cluster.routers[0];
    // When there is only one peer, it should campaign immediately.
    let mut req = RaftCmdRequest::default();
    req.mut_header().set_peer(new_peer(1, 3));
    req.mut_status_request()
        .set_cmd_type(StatusCmdType::RegionLeader);
    let res = router.query(2, req.clone()).unwrap();
    let status_resp = res.response().unwrap().get_status_response();
    assert_eq!(
        *status_resp.get_region_leader().get_leader(),
        new_peer(1, 3)
    );

    req.mut_status_request()
        .set_cmd_type(StatusCmdType::RegionDetail);
    let res = router.query(2, req.clone()).unwrap();
    let status_resp = res.response().unwrap().get_status_response();
    let detail = status_resp.get_region_detail();
    assert_eq!(*detail.get_leader(), new_peer(1, 3));
    let region = detail.get_region();
    assert_eq!(region.get_id(), 2);
    assert!(region.get_start_key().is_empty());
    assert!(region.get_end_key().is_empty());
    assert_eq!(*region.get_peers(), vec![new_peer(1, 3)]);
    assert_eq!(region.get_region_epoch().get_version(), 1);
    assert_eq!(region.get_region_epoch().get_conf_ver(), 1);

    // Invalid store id should return error.
    req.mut_header().mut_peer().set_store_id(4);
    let res = router.query(2, req).unwrap();
    let resp = res.response().unwrap();
    assert!(
        resp.get_header().get_error().has_store_not_match(),
        "{:?}",
        resp
    );

    // TODO: add a peer then check for region change and leadership change.
}
fn test_encrypted_checkpoint() {
    let dir = tempdir();
    let root_path = dir.path();

    let encryption_cfg = test_util::new_file_security_config(root_path);
    let key_manager = Arc::new(
        data_key_manager_from_config(&encryption_cfg, root_path.to_str().unwrap())
            .unwrap()
            .unwrap(),
    );

    let mut db_opts = DbOptions::default();
    db_opts.set_key_manager(Some(key_manager));
    let cf_opts: Vec<_> = ALL_CFS.iter().map(|cf| (*cf, CfOptions::new())).collect();

    let path1 = root_path.join("1").to_str().unwrap().to_owned();
    let db1 = KvTestEngine::new_kv_engine_opt(&path1, db_opts.clone(), cf_opts.clone()).unwrap();
    db1.put(b"foo", b"bar").unwrap();
    db1.sync().unwrap();

    let path2 = root_path.join("2");
    let mut checkpointer = db1.new_checkpointer().unwrap();
    checkpointer.create_at(&path2, None, 0).unwrap();
    let db2 =
        KvTestEngine::new_kv_engine_opt(path2.to_str().unwrap(), db_opts.clone(), cf_opts.clone())
            .unwrap();
    assert_eq!(
        db2.get_value_cf(CF_DEFAULT, b"foo").unwrap().unwrap(),
        b"bar"
    );
}
fn test_cdc_rawkv_scan() {
    let mut suite = TestSuite::new(3, ApiVersion::V2);

    let (k1, v1) = (b"rkey1".to_vec(), b"value1".to_vec());
    suite.must_kv_put(1, k1, v1);

    let (k2, v2) = (b"rkey2".to_vec(), b"value2".to_vec());
    suite.must_kv_put(1, k2, v2);

    let ts = block_on(suite.cluster.pd_client.get_tso()).unwrap();
    suite.flush_causal_timestamp_for_region(1);

    let (k3, v3) = (b"rkey3".to_vec(), b"value3".to_vec());
    suite.must_kv_put(1, k3.clone(), v3.clone());

    let (k4, v4) = (b"rkey4".to_vec(), b"value4".to_vec());
    suite.must_kv_put(1, k4.clone(), v4.clone());

    let mut req = suite.new_changedata_request(1);
    req.set_kv_api(ChangeDataRequestKvApi::RawKv);
    req.set_checkpoint_ts(ts.into_inner());
    let (mut req_tx, event_feed_wrap, receive_event) =
        new_event_feed(suite.get_region_cdc_client(1));
    block_on(req_tx.send((req, WriteFlags::default()))).unwrap();
    let mut events = receive_event(false).events.to_vec();
    if events.len() == 1 {
        events.extend(receive_event(false).events.into_iter());
    }
    assert_eq!(events.len(), 2, "{:?}", events);

    match events.remove(0).event.unwrap() {
        // Batch size is set to 3.
        Event_oneof_event::Entries(es) => {
            assert!(es.entries.len() == 2, "{:?}", es);
            let e = &es.entries[0];
            assert_eq!(e.get_type(), EventLogType::Committed, "{:?}", es);
            assert_eq!(e.key, k3, "{:?}", es);
            assert_eq!(e.value, v3, "{:?}", es);

            let e = &es.entries[1];
            assert_eq!(e.get_type(), EventLogType::Committed, "{:?}", es);
            assert_eq!(e.key, k4, "{:?}", es);
            assert_eq!(e.value, v4, "{:?}", es);
        }
        other => panic!("unknown event {:?}", other),
    }

    match events.pop().unwrap().event.unwrap() {
        // Then it outputs Initialized event.
        Event_oneof_event::Entries(es) => {
            assert!(es.entries.len() == 1, "{:?}", es);
            let e = &es.entries[0];
            assert_eq!(e.get_type(), EventLogType::Initialized, "{:?}", es);
        }
        other => panic!("unknown event {:?}", other),
    }

    event_feed_wrap.replace(None);
    suite.stop();
}
fn test() -> io::Result<()> {
    let root = super::fixture_root().join("enhanced_resolve");
    let dirname = root.join("test");
    let temp_path = dirname.join("temp");
    if !temp_path.exists() {
        let is_admin = init(&dirname, &temp_path).is_ok();
        if !is_admin {
            return Ok(());
        }
        if let Err(err) = create_symlinks(&dirname, &temp_path) {
            cleanup_symlinks(&temp_path);
            return Err(err);
        }
    }

    let resolver_without_symlinks =
        Resolver::new(ResolveOptions { symlinks: false, ..ResolveOptions::default() });
    let resolver_with_symlinks = Resolver::default();

    #[rustfmt::skip]
    let pass = [
        ("with a symlink to a file", temp_path.clone(), "./index.js"),
        ("with a relative symlink to a file", temp_path.clone(), "./node.relative.js"),
        ("with a relative symlink to a symlink to a file", temp_path.clone(), "./node.relative.sym.js"),
        ("with a symlink to a directory 1", temp_path.clone(), "./lib/index.js"),
        ("with a symlink to a directory 2", temp_path.clone(), "./this/lib/index.js"),
        ("with multiple symlinks in the path 1", temp_path.clone(), "./this/test/temp/index.js"),
        ("with multiple symlinks in the path 2", temp_path.clone(), "./this/test/temp/lib/index.js"),
        ("with multiple symlinks in the path 3", temp_path.clone(), "./this/test/temp/this/lib/index.js"),
        ("with a symlink to a directory 2 (chained)", temp_path.clone(), "./that/lib/index.js"),
        ("with multiple symlinks in the path 1 (chained)", temp_path.clone(), "./that/test/temp/index.js"),
        ("with multiple symlinks in the path 2 (chained)", temp_path.clone(), "./that/test/temp/lib/index.js"),
        ("with multiple symlinks in the path 3 (chained)", temp_path.clone(), "./that/test/temp/that/lib/index.js"),
        ("with symlinked directory as context 1", temp_path.join( "lib"), "./index.js"),
        ("with symlinked directory as context 2", temp_path.join( "this"), "./lib/index.js"),
        ("with symlinked directory as context and in path", temp_path.join( "this"), "./test/temp/lib/index.js"),
        ("with symlinked directory in context path", temp_path.join( "this/lib"), "./index.js"),
        ("with symlinked directory in context path and symlinked file", temp_path.join( "this/test"), "./temp/index.js"),
        ("with symlinked directory in context path and symlinked directory", temp_path.join( "this/test"), "./temp/lib/index.js"),
        ("with symlinked directory as context 2 (chained)", temp_path.join( "that"), "./lib/index.js"),
        ("with symlinked directory as context and in path (chained)", temp_path.join( "that"), "./test/temp/lib/index.js"),
        ("with symlinked directory in context path (chained)", temp_path.join( "that/lib"), "./index.js"),
        ("with symlinked directory in context path and symlinked file (chained)", temp_path.join( "that/test"), "./temp/index.js"),
        ("with symlinked directory in context path and symlinked directory (chained)", temp_path.join( "that/test"), "./temp/lib/index.js")
    ];

    for (comment, path, request) in pass {
        let filename = resolver_with_symlinks.resolve(&path, request).map(|r| r.full_path());
        assert_eq!(filename, Ok(root.join("lib/index.js")), "{comment:?}");

        let resolved_path =
            resolver_without_symlinks.resolve(&path, request).map(|r| r.full_path());
        assert_eq!(resolved_path, Ok(path.join(request)));
    }

    Ok(())
}
fn write_batch_delete_range_none() {
    let db = default_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();

    let mut wb = db.engine.write_batch();

    wb.delete_range(b"b", b"e").unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());

    let db = multi_batch_write_engine();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"e", b"").unwrap();
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        db.engine.put(&x, &x).unwrap();
    }

    let mut wb = db.engine.write_batch_with_cap(1024);

    wb.delete_range(b"b", b"e").unwrap();
    wb.delete_range(&1_usize.to_be_bytes(), &256_usize.to_be_bytes())
        .unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());
    assert!(
        db.engine
            .get_value(&0_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    for i in 1..256_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
}
fn database_lock() {
    let tmpfile = create_tempfile();
    let result = Database::create(tmpfile.path());
    assert!(result.is_ok());
    let result2 = Database::open(tmpfile.path());
    assert!(
        matches!(result2, Err(DatabaseError::DatabaseAlreadyOpen)),
        "{result2:?}",
    );
    drop(result);
    let result = Database::open(tmpfile.path());
    assert!(result.is_ok());
}
fn tsconfig() {
    let f = super::fixture_root().join("parcel");

    #[rustfmt::skip]
    let pass = [
        (f.clone(), "ts-path", f.join("foo.js")),
        (f.join("nested"), "ts-path", f.join("nested/test.js")),
        (f.join("tsconfig/index"), "foo", f.join("node_modules/tsconfig-index/foo.js")),
        // This requires reading package.json.tsconfig field
        // (f.join("tsconfig/field"), "foo", f.join("node_modules/tsconfig-field/foo.js"))
        (f.join("tsconfig/exports"), "foo", f.join("node_modules/tsconfig-exports/foo.js")),
        (f.join("tsconfig/extends-extension"), "foo", f.join("tsconfig/extends-extension/foo.js")),
        (f.join("tsconfig/extends-extensionless"), "foo", f.join("node_modules/tsconfig-field/foo.js"))
    ];

    for (path, request, expected) in pass {
        let resolver = Resolver::new(ResolveOptions {
            tsconfig: Some(TsconfigOptions {
                config_file: path.join("tsconfig.json"),
                references: TsconfigReferences::Auto,
            }),
            ..ResolveOptions::default()
        });
        let resolved_path = resolver.resolve(&path, request).map(|f| f.full_path());
        assert_eq!(resolved_path, Ok(expected), "{request} {path:?}");
    }

    #[rustfmt::skip]
    let data = [
        (f.join("node_modules/tsconfig-not-used"), "ts-path", Ok(f.join("foo.js"))),
    ];

    let resolver = Resolver::new(ResolveOptions {
        tsconfig: Some(TsconfigOptions {
            config_file: f.join("tsconfig.json"),
            references: TsconfigReferences::Auto,
        }),
        ..ResolveOptions::default()
    });
    for (path, request, expected) in data {
        let resolution = resolver.resolve(&path, request).map(|f| f.full_path());
        assert_eq!(resolution, expected, "{path:?} {request}");
    }
}
fn test_priority() {
    let (control_tx, control_fsm) = Runner::new(10);
    let (router, mut system) =
        batch_system::create_system(&Config::default(), control_tx, control_fsm, None);
    let builder = Builder::new();
    system.spawn("test".to_owned(), builder);
    let (tx, rx) = mpsc::unbounded();
    let tx_ = tx.clone();
    let r = router.clone();
    let state_cnt = Arc::new(AtomicUsize::new(0));
    router
        .send_control(Message::Callback(Box::new(
            move |_: &Handler, _: &mut Runner| {
                let (tx, runner) = Runner::new(10);
                r.register(1, BasicMailbox::new(tx, runner, state_cnt.clone()));
                let (tx2, mut runner2) = Runner::new(10);
                runner2.set_priority(Priority::Low);
                r.register(2, BasicMailbox::new(tx2, runner2, state_cnt));
                tx_.send(1).unwrap();
            },
        )))
        .unwrap();
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(1));

    let tx_ = tx.clone();
    router
        .send(
            1,
            Message::Callback(Box::new(move |h: &Handler, r: &mut Runner| {
                assert_eq!(h.get_priority(), Priority::Normal);
                assert_eq!(h.get_priority(), r.get_priority());
                tx_.send(2).unwrap();
            })),
        )
        .unwrap();
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(2));

    router
        .send(
            2,
            Message::Callback(Box::new(move |h: &Handler, r: &mut Runner| {
                assert_eq!(h.get_priority(), Priority::Low);
                assert_eq!(h.get_priority(), r.get_priority());
                tx.send(3).unwrap();
            })),
        )
        .unwrap();
    assert_eq!(rx.recv_timeout(Duration::from_secs(3)), Ok(3));
}
fn test_reject_proposal_during_leader_transfer() {
    let mut cluster = new_node_cluster(0, 2);
    let pd_client = cluster.pd_client.clone();
    pd_client.disable_default_operator();
    let r = cluster.run_conf_change();
    pd_client.must_add_peer(r, new_peer(2, 2));

    // Don't allow leader transfer succeed if it is actually triggered.
    cluster.add_send_filter(CloneFilterFactory(
        RegionPacketFilter::new(r, 2)
            .msg_type(MessageType::MsgTimeoutNow)
            .direction(Direction::Recv),
    ));

    cluster.must_put(b"k", b"v");
    cluster.transfer_leader(r, new_peer(2, 2));
    // The leader can't change to transferring state immediately due to
    // pre-transfer-leader feature, so wait for a while.
    sleep_ms(100);
    assert_ne!(cluster.leader_of_region(r).unwrap(), new_peer(2, 2));

    let force_delay_propose_batch_raft_command_fp = "force_delay_propose_batch_raft_command";
    for i in 0..2 {
        if i == 1 {
            // Test another path of calling proposed callback.
            fail::cfg(force_delay_propose_batch_raft_command_fp, "2*return").unwrap();
        }
        let write_req = make_write_req(&mut cluster, b"k");
        let (cb, mut cb_receivers) = make_cb(&write_req);
        cluster
            .sim
            .rl()
            .async_command_on_node(1, write_req, cb)
            .unwrap();
        cb_receivers.assert_err();
    }

    cluster.clear_send_filters();
}
fn test_deadline_3() {
    let data = vec![
        (1, Some("name:0"), 2),
        (2, Some("name:4"), 3),
        (4, Some("name:3"), 1),
        (5, Some("name:1"), 4),
    ];

    let product = ProductTable::new();
    let (_, endpoint, _) = {
        let engine = tikv::storage::TestEngineBuilder::new().build().unwrap();
        let cfg = tikv::server::Config {
            end_point_request_max_handle_duration: tikv_util::config::ReadableDuration::secs(1),
            ..Default::default()
        };
        init_data_with_details(Context::default(), engine, &product, &data, true, &cfg)
    };
    let req = DagSelect::from(&product).build();

    fail::cfg("kv_cursor_seek", "sleep(2000)").unwrap();
    fail::cfg("copr_batch_initial_size", "return(1)").unwrap();
    let cop_resp = handle_request(&endpoint, req);
    let mut resp = SelectResponse::default();
    resp.merge_from_bytes(cop_resp.get_data()).unwrap();

    assert!(
        cop_resp.other_error.contains("exceeding the deadline")
            || resp
                .get_error()
                .get_msg()
                .contains("exceeding the deadline")
    );
}
fn save_point_all_commands() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    db.engine.put(b"a", b"").unwrap();
    db.engine.put(b"d", b"").unwrap();

    wb.set_save_point();
    wb.delete(b"a").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.delete_range(b"c", b"e").unwrap();

    wb.rollback_to_save_point().unwrap();
    wb.write().unwrap();

    let a = db.engine.get_value(b"a").unwrap();
    let b = db.engine.get_value(b"b").unwrap();
    let d = db.engine.get_value(b"d").unwrap();
    assert!(a.is_some());
    assert!(b.is_none());
    assert!(d.is_some());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = 256_usize;

    for i in 0..max_keys / 2 {
        db.engine.put(&i.to_be_bytes(), b"").unwrap();
    }
    db.engine.put(b"a", b"").unwrap();
    for i in max_keys / 2..max_keys {
        db.engine.put(&i.to_be_bytes(), b"").unwrap();
    }
    db.engine.put(b"d", b"").unwrap();

    wb.set_save_point();
    for i in 0..max_keys / 2 {
        wb.delete(&i.to_be_bytes()).unwrap();
    }
    wb.delete(b"a").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.delete_range(b"c", b"e").unwrap();
    wb.delete_range(&(max_keys / 3).to_be_bytes(), &(2 * max_keys).to_be_bytes())
        .unwrap();

    wb.rollback_to_save_point().unwrap();
    wb.write().unwrap();

    let a = db.engine.get_value(b"a").unwrap();
    let b = db.engine.get_value(b"b").unwrap();
    let d = db.engine.get_value(b"d").unwrap();
    for i in 0..max_keys {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());
    }
    assert!(a.is_some());
    assert!(b.is_none());
    assert!(d.is_some());
}
fn test_summary() {
    let cfg = Config {
        report_receiver_interval: ReadableDuration::millis(REPORT_INTERVAL_MS),
        precision: ReadableDuration::millis(PRECISION_MS),
        ..Default::default()
    };

    let (_, collector_reg_handle, resource_tag_factory, mut recorder_worker) =
        init_recorder(cfg.precision.as_millis());
    let (_, data_sink_reg_handle, mut reporter_worker) = init_reporter(cfg, collector_reg_handle);

    let data_sink = MockDataSink::default();

    // At this point we are ready for everything except turning on the switch.

    // expect no data
    {
        let tf = resource_tag_factory.clone();
        let data_sink = data_sink.clone();
        thread::spawn(move || {
            {
                let mut ctx = Context::default();
                ctx.set_resource_group_tag(b"TAG-1".to_vec());
                let tag = tf.new_tag(&ctx);
                let _g = tag.attach();
                resource_metering::record_read_keys(123);
                resource_metering::record_write_keys(456);
            }
            thread::sleep(Duration::from_millis(REPORT_INTERVAL_MS + 500)); // wait report
            assert!(data_sink.get(b"TAG-1").is_none());
            data_sink.clear();
        })
        .join()
        .unwrap();
    }

    // turn on
    let reg_guard = data_sink_reg_handle.register(Box::new(data_sink.clone()));

    // expect can get data
    {
        let tf = resource_tag_factory.clone();
        let data_sink = data_sink.clone();
        thread::spawn(move || {
            {
                let mut ctx = Context::default();
                ctx.set_resource_group_tag(b"TAG-1".to_vec());
                let tag = tf.new_tag(&ctx);
                let _g = tag.attach();
                thread::sleep(Duration::from_millis(PRECISION_MS * 2)); // wait config apply
                resource_metering::record_read_keys(123);
                resource_metering::record_write_keys(456);
            }
            thread::sleep(Duration::from_millis(REPORT_INTERVAL_MS + 500)); // wait report

            let r = data_sink.get(b"TAG-1").unwrap();
            assert_eq!(
                r.get_record()
                    .get_items()
                    .iter()
                    .map(|item| item.read_keys)
                    .sum::<u32>(),
                123
            );
            assert_eq!(
                r.get_record()
                    .get_items()
                    .iter()
                    .map(|item| item.write_keys)
                    .sum::<u32>(),
                456
            );
            data_sink.clear();
        })
        .join()
        .unwrap();
    }

    // turn off
    drop(reg_guard);

    // expect no data
    thread::spawn(move || {
        {
            let mut ctx = Context::default();
            ctx.set_resource_group_tag(b"TAG-1".to_vec());
            let tag = resource_tag_factory.new_tag(&ctx);
            let _g = tag.attach();
            thread::sleep(Duration::from_millis(PRECISION_MS * 2)); // wait config apply
            resource_metering::record_read_keys(123);
            resource_metering::record_write_keys(456);
        }
        thread::sleep(Duration::from_millis(REPORT_INTERVAL_MS + 500)); // wait report
        assert!(data_sink.get(b"TAG-1").is_none());
        data_sink.clear();
    })
    .join()
    .unwrap();

    // stop worker
    recorder_worker.stop();
    reporter_worker.stop();
}
fn test_report_buckets() {
    let region_id = 2;
    let mut cop_cfg = CopConfig::default();
    cop_cfg.enable_region_bucket = Some(true);
    cop_cfg.region_bucket_size = ReadableSize::kb(1);
    let mut config = v2_default_config();
    config.region_split_check_diff = Some(ReadableSize::kb(1));
    let cluster = Cluster::with_cop_cfg(Some(config), cop_cfg);
    let store_id = cluster.node(0).id();
    let router = &cluster.routers[0];

    // When there is only one peer, it should campaign immediately.
    let mut req = RaftCmdRequest::default();
    req.mut_header().set_peer(new_peer(store_id, 3));
    req.mut_status_request()
        .set_cmd_type(StatusCmdType::RegionLeader);
    let res = router.query(region_id, req.clone()).unwrap();
    let status_resp = res.response().unwrap().get_status_response();
    assert_eq!(
        *status_resp.get_region_leader().get_leader(),
        new_peer(store_id, 3)
    );
    router.wait_applied_to_current_term(region_id, Duration::from_secs(3));

    // load data to split bucket.
    let mut suffix = String::from("");
    for _ in 0..200 {
        suffix.push_str("fake ");
    }

    let repeat: u64 = 10;
    let bytes = write_keys(&cluster, region_id, &suffix, repeat.try_into().unwrap());
    // To find the split keys, it should flush memtable manually.
    let mut cached = cluster.node(0).tablet_registry().get(region_id).unwrap();
    cached.latest().unwrap().flush_cf(CF_DEFAULT, true).unwrap();
    // send split region check to split bucket.
    router
        .send(region_id, PeerMsg::Tick(PeerTick::SplitRegionCheck))
        .unwrap();
    std::thread::sleep(std::time::Duration::from_millis(50));
    // report buckets to pd.
    router
        .send(region_id, PeerMsg::Tick(PeerTick::ReportBuckets))
        .unwrap();
    std::thread::sleep(std::time::Duration::from_millis(50));

    let resp = block_on(cluster.node(0).pd_client().get_buckets_by_id(region_id)).unwrap();
    let mut buckets_tmp = vec![];
    let mut bucket_ranges = vec![];
    if let Some(buckets) = resp {
        assert!(buckets.get_keys().len() > 2);
        assert_eq!(buckets.get_region_id(), region_id);
        let write_bytes = buckets.get_stats().get_write_bytes();
        let write_keys = buckets.get_stats().get_write_keys();
        for i in 0..buckets.keys.len() - 1 {
            assert!(write_bytes[i] >= bytes);
            assert!(write_keys[i] >= repeat);
        }
        for i in 0..buckets.keys.len() - 1 {
            buckets_tmp.push(raftstore::store::Bucket::default());
            let bucket_range =
                raftstore::store::BucketRange(buckets.keys[i].clone(), buckets.keys[i + 1].clone());
            bucket_ranges.push(bucket_range);
        }
    }

    // report buckets to pd again, the write bytes and keys should be zero.
    router
        .send(region_id, PeerMsg::Tick(PeerTick::ReportBuckets))
        .unwrap();
    std::thread::sleep(std::time::Duration::from_millis(50));

    let resp = block_on(cluster.node(0).pd_client().get_buckets_by_id(region_id)).unwrap();
    if let Some(buckets) = resp {
        assert_eq!(buckets.get_region_id(), region_id);
        let write_bytes = buckets.get_stats().get_write_bytes();
        let write_keys = buckets.get_stats().get_write_keys();
        for i in 0..buckets.keys.len() - 1 {
            assert!(write_bytes[i] == 0);
            assert!(write_keys[i] == 0);
        }
    }

    // send the same region buckets to refresh which needs to merge the last.
    let resp = block_on(cluster.node(0).pd_client().get_region_by_id(region_id)).unwrap();
    if let Some(region) = resp {
        let region_epoch = region.get_region_epoch().clone();
        for _ in 0..2 {
            let msg = PeerMsg::RefreshRegionBuckets {
                region_epoch: region_epoch.clone(),
                buckets: buckets_tmp.clone(),
                bucket_ranges: Some(bucket_ranges.clone()),
            };
            router.send(region_id, msg).unwrap();
            std::thread::sleep(std::time::Duration::from_millis(50));
        }
    }
    // report buckets to pd again, the write bytes and keys should be zero.
    router
        .send(region_id, PeerMsg::Tick(PeerTick::ReportBuckets))
        .unwrap();
    std::thread::sleep(std::time::Duration::from_millis(50));

    let resp = block_on(cluster.node(0).pd_client().get_buckets_by_id(region_id)).unwrap();
    if let Some(buckets) = resp {
        assert_eq!(buckets.get_region_id(), region_id);
        let write_bytes = buckets.get_stats().get_write_bytes();
        let write_keys = buckets.get_stats().get_write_keys();
        assert_eq!(write_bytes.len(), 1);
        assert_eq!(write_keys.len(), 1);
    }

    fn write_keys(cluster: &Cluster, region_id: u64, suffix: &str, repeat: usize) -> u64 {
        let router = &cluster.routers[0];
        let header = Box::new(router.new_request_for(region_id).take_header());
        for i in 0..repeat {
            let mut put = SimpleWriteEncoder::with_capacity(64);
            let mut key = format!("key-{}", i);
            key.push_str(suffix);
            put.put(CF_DEFAULT, key.as_bytes(), b"value");
            let (msg, sub) = PeerMsg::simple_write(header.clone(), put.clone().encode());
            router.send(region_id, msg).unwrap();
            let _resp = block_on(sub.result()).unwrap();
        }
        ((suffix.as_bytes().len() + 10) * repeat)
            .try_into()
            .unwrap()
    }
}
fn delete_all_tables() {
    let tmpfile = create_tempfile();
    let db = Database::create(tmpfile.path()).unwrap();

    let x_def: TableDefinition<&str, &str> = TableDefinition::new("x");
    let y_def: TableDefinition<&str, &str> = TableDefinition::new("y");

    let write_txn = db.begin_write().unwrap();
    {
        let mut table = write_txn.open_table(x_def).unwrap();
        table.insert("hello", "world").unwrap();
        let mut table = write_txn.open_table(y_def).unwrap();
        table.insert("hello", "world").unwrap();
    }
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    assert_eq!(2, read_txn.list_tables().unwrap().count());

    let write_txn = db.begin_write().unwrap();
    for table in write_txn.list_tables().unwrap() {
        write_txn.delete_table(table).unwrap();
    }
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    assert_eq!(0, read_txn.list_tables().unwrap().count());
}
fn write_batch_count_2() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    assert_eq!(wb.count(), 0);
    wb.put(b"a", b"").unwrap();
    assert_eq!(wb.count(), 1);
    wb.delete(b"a").unwrap();
    assert_eq!(wb.count(), 2);
    wb.delete_range(b"a", b"b").unwrap();
    assert_eq!(wb.count(), 3);
    wb.write().unwrap();
    assert_eq!(wb.count(), 3);

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);

    assert_eq!(wb.count(), 0);
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();
    assert_eq!(wb.count(), 257);
    wb.delete(b"a").unwrap();
    assert_eq!(wb.count(), 258);
    wb.delete_range(b"a", b"b").unwrap();
    assert_eq!(wb.count(), 259);
    wb.write().unwrap();
    assert_eq!(wb.count(), 259);
}
fn save_point_same_rollback_one() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"").unwrap();

    wb.set_save_point();
    wb.set_save_point();
    wb.set_save_point();

    wb.put(b"b", b"").unwrap();

    wb.rollback_to_save_point().unwrap();

    wb.write().unwrap();

    let a = db.engine.get_value(b"a").unwrap();
    let b = db.engine.get_value(b"b").unwrap();

    assert!(a.is_some());
    assert!(b.is_none());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    let max_keys = 256_usize;

    for i in 0..max_keys {
        wb.put(&i.to_be_bytes(), b"").unwrap();
    }
    wb.put(b"a", b"").unwrap();

    wb.set_save_point();
    wb.set_save_point();
    wb.set_save_point();

    wb.put(b"b", b"").unwrap();
    for i in max_keys..2 * max_keys {
        wb.put(&i.to_be_bytes(), b"").unwrap();
    }

    wb.rollback_to_save_point().unwrap();

    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    for i in 0..max_keys {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_some());
    }

    assert!(db.engine.get_value(b"b").unwrap().is_none());
    for i in max_keys..2 * max_keys {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
}
fn first_last() {
    let tmpfile = create_tempfile();
    let db = Database::create(tmpfile.path()).unwrap();
    let write_txn = db.begin_write().unwrap();
    {
        let mut table = write_txn.open_table(STR_TABLE).unwrap();
        assert!(table.first().unwrap().is_none());
        assert!(table.last().unwrap().is_none());
        table.insert("a", "world1").unwrap();
        assert_eq!(table.first().unwrap().unwrap().0.value(), "a");
        assert_eq!(table.last().unwrap().unwrap().0.value(), "a");
        table.insert("b", "world2").unwrap();
        table.insert("c", "world3").unwrap();
    }
    write_txn.commit().unwrap();

    let read_txn = db.begin_read().unwrap();
    let table = read_txn.open_table(STR_TABLE).unwrap();
    assert_eq!(table.first().unwrap().unwrap().0.value(), "a");
    assert_eq!(table.last().unwrap().unwrap().0.value(), "c");
}
fn test_parse_request_failed() {
    let product = ProductTable::new();
    let (_, endpoint) = init_with_data(&product, &[]);
    let req = DagSelect::from(&product).build();

    fail::cfg("coprocessor_parse_request", "return()").unwrap();
    let resp = handle_request(&endpoint, req);

    assert!(resp.get_other_error().contains("unsupported tp"));
}
fn write_batch_delete_range_after_put() {
    let db = default_engine();
    let mut wb = db.engine.write_batch();

    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.put(b"c", b"").unwrap();
    wb.put(b"d", b"").unwrap();
    wb.put(b"e", b"").unwrap();
    wb.delete_range(b"b", b"e").unwrap();
    wb.write().unwrap();

    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());

    let db = multi_batch_write_engine();
    let mut wb = db.engine.write_batch_with_cap(1024);
    for i in 0..256_usize {
        let x = i.to_be_bytes();
        wb.put(&x, &x).unwrap();
    }
    wb.put(b"a", b"").unwrap();
    wb.put(b"b", b"").unwrap();
    wb.put(b"c", b"").unwrap();
    wb.put(b"d", b"").unwrap();
    wb.put(b"e", b"").unwrap();
    wb.delete_range(&1_usize.to_be_bytes(), &255_usize.to_be_bytes())
        .unwrap();
    wb.delete_range(b"b", b"e").unwrap();
    wb.write().unwrap();

    assert!(
        db.engine
            .get_value(&0_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    for i in 1..255_usize {
        assert!(db.engine.get_value(&i.to_be_bytes()).unwrap().is_none());
    }
    assert!(
        db.engine
            .get_value(&255_usize.to_be_bytes())
            .unwrap()
            .is_some()
    );
    assert!(db.engine.get_value(b"a").unwrap().is_some());
    assert!(db.engine.get_value(b"b").unwrap().is_none());
    assert!(db.engine.get_value(b"c").unwrap().is_none());
    assert!(db.engine.get_value(b"d").unwrap().is_none());
    assert!(db.engine.get_value(b"e").unwrap().is_some());
}
